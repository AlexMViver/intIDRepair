2024-04-24T08:45:21,025  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/hcatalog/core/target/testconf/hive-site.xml
2024-04-24T08:45:21,471  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T08:45:21,555  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:45:21,555  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:45:21,556  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:45:21,557  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:45:21,557  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:45:21,557  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:45:21,557  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:45:21,557  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:45:21,558  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:45:21,558  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:45:21,558  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:45:21,969  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:22,195  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:45:22,245  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:22,256  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T08:45:22,256  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T08:45:22,302  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:45:22,309  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T08:45:23,375  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:45:23,382  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T08:45:24,381  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T08:45:24,382  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: null will be shutdown
2024-04-24T08:45:24,431  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 created in the thread with id: 1
2024-04-24T08:45:28,746  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T08:45:28,747  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T08:45:28,747  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9 from thread id: 1
2024-04-24T08:45:29,200  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T08:45:29,243  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T08:45:29,294  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T08:45:29,298  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T08:45:29,484  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T08:45:29,493  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T08:45:29,494  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T08:45:29,496  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T08:45:29,529  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:45:29,535  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T08:45:29,537  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:45:29,537  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T08:45:29,540  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:45:29,544  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T08:45:29,545  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:45:29,546  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T08:45:29,552  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T08:45:29,553  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T08:45:29,556  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T08:45:29,560  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = ca00f21b-0b02-4e5f-a45d-1bfc9016072d
2024-04-24T08:45:29,775  INFO [main] SessionState: Hive Session ID = ca00f21b-0b02-4e5f-a45d-1bfc9016072d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:45:29,792  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:45:29,902  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/ca00f21b-0b02-4e5f-a45d-1bfc9016072d
2024-04-24T08:45:29,907  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/ca00f21b-0b02-4e5f-a45d-1bfc9016072d
2024-04-24T08:45:29,912  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/ca00f21b-0b02-4e5f-a45d-1bfc9016072d/_tmp_space.db
2024-04-24T08:45:29,915  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data/intString.seq
2024-04-24T08:45:29,980  INFO [main] compress.CodecPool: Got brand-new compressor [.deflate]
2024-04-24T08:45:30,083  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): drop table if exists test_bad_records
2024-04-24T08:45:31,913  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,918  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,931  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,937  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,937  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,937  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:31,937  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:45:32,050  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:32,050  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:32,051  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 will be shutdown
2024-04-24T08:45:32,052  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 created in the thread with id: 1
2024-04-24T08:45:32,075  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:32,076  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:32,160  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T08:45:32,824  INFO [main] reflections.Reflections: Reflections took 479 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:45:33,154  INFO [main] reflections.Reflections: Reflections took 233 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:45:33,382  INFO [main] reflections.Reflections: Reflections took 213 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:45:33,400  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:33,663  INFO [main] reflections.Reflections: Reflections took 210 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:45:33,776  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:33,778  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:33,784  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:33,784  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getAllFunctions_()=101, flushCache_()=1}
2024-04-24T08:45:33,786  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 3.705 seconds
2024-04-24T08:45:33,787  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:33,788  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:33,794  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): drop table if exists test_bad_records
2024-04-24T08:45:33,801  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:45:33,803  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:33,826  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:33,826  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T08:45:33,826  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 0.032 seconds
2024-04-24T08:45:33,828  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:45:33,942  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968
2024-04-24T08:45:33,945  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:45:33,966  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ca00f21b-0b02-4e5f-a45d-1bfc9016072d, clientType=HIVECLI]
2024-04-24T08:45:33,969  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T08:45:33,971  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:45:33,971  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 will be shutdown
2024-04-24T08:45:33,971  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:33,972  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T08:45:33,974  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:33,975  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:45:33,975  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:33,977  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed, with PersistenceManager: null will be shutdown
2024-04-24T08:45:33,977  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@41f785e3 created in the thread with id: 1
2024-04-24T08:45:33,990  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed from thread id: 1
2024-04-24T08:45:33,990  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:33,991  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:34,000  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T08:45:34,011  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:34,011  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:34,012  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@41f785e3 will be shutdown
2024-04-24T08:45:34,013  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65036e8d created in the thread with id: 1
2024-04-24T08:45:34,024  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:34,025  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:34,030  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:45:34,049  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968
2024-04-24T08:45:34,049  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:34,049  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:34,050  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:34,050  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:45:34,052  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 0.222 seconds
2024-04-24T08:45:34,053  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:34,053  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:34,053  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:45:34,053  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:45:34,054  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T08:45:34,054  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:45:34,054  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72b2c5ed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65036e8d will be shutdown
2024-04-24T08:45:34,054  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:34,055  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-24T08:45:34,208  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:34,210  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:45:34,210  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:34,211  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: null will be shutdown
2024-04-24T08:45:34,211  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@615e83ac created in the thread with id: 1
2024-04-24T08:45:34,218  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca from thread id: 1
2024-04-24T08:45:34,219  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:34,219  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:34,220  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713973534, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:myint, type:int, comment:from deserializer), FieldSchema(name:underscore_int, type:string, comment:from deserializer), FieldSchema(name:mystring, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.format=org.apache.thrift.protocol.TBinaryProtocol, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, numFilesErasureCoded=0, bucketing_version=2, totalSize=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:45:34,236  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/warehouse/test_bad_records
2024-04-24T08:45:34,474  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:34,475  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=255}
2024-04-24T08:45:34,475  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 0.421 seconds
2024-04-24T08:45:34,476  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data' into table test_bad_records
2024-04-24T08:45:34,480  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968
2024-04-24T08:45:34,490  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:34,623  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:34,686  INFO [main] compress.CodecPool: Got brand-new decompressor [.deflate]
2024-04-24T08:45:34,702  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968
2024-04-24T08:45:34,702  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:34,703  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:34,703  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:34,703  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=136}
2024-04-24T08:45:34,703  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 0.227 seconds
2024-04-24T08:45:34,704  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:34,704  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:34,704  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data' into table test_bad_records
2024-04-24T08:45:34,704  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T08:45:34,707  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data
2024-04-24T08:45:34,707  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:34,737  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:34,743  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:34,772  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:34,800  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:45:34,801  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:45:34,903  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:34,903  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T08:45:34,933  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T08:45:34,934  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:34,958  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:34,959  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T08:45:34,960  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:34,991  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:34,997  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:45:34,997  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:45:35,114  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T08:45:35,114  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:35,114  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=117, alter_table_(String, String, String, Table, EnvironmentContext, String)=244}
2024-04-24T08:45:35,115  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084521_b56c9933-4b91-4d11-be7b-4c9a33490968); Time taken: 0.41 seconds
2024-04-24T08:45:35,216  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:45:35,217  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:45:35,217  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:45:35,218  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:45:35,218  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:45:35,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:45:35,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:45:35,219  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:45:35,219  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:45:35,219  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:45:35,219  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:45:35,220  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T08:45:35,221  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T08:45:35,246  INFO [main] common.HiveClientCache: Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
2024-04-24T08:45:35,280  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:35,280  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:35,281  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@615e83ac will be shutdown
2024-04-24T08:45:35,284  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@261275ae created in the thread with id: 1
2024-04-24T08:45:35,323  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:35,324  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:35,392  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T08:45:35,414  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:35,445  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:35,729  INFO [main] beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2024-04-24T08:45:35,748  WARN [main] impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
2024-04-24T08:45:35,778  INFO [main] impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-04-24T08:45:35,778  INFO [main] impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-24T08:45:35,846  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T08:45:35,872  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T08:45:35,898  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T08:45:35,919  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T08:45:35,995  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T08:45:36,060  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1577817403_0001
2024-04-24T08:45:36,060  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T08:45:36,245  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T08:45:36,246  INFO [main] mapreduce.Job: Running job: job_local1577817403_0001
2024-04-24T08:45:36,248  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T08:45:36,260  INFO [Thread-52] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:45:36,260  INFO [Thread-52] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:45:36,260  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T08:45:36,285  INFO [Thread-52] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T08:45:36,286  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1577817403_0001_m_000000_0
2024-04-24T08:45:36,323  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:45:36,323  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:45:36,346  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T08:45:36,349  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@74daf95c
2024-04-24T08:45:36,404  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=myint,underscore_int,mystring, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713973535}
2024-04-24T08:45:36,410  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T08:45:36,410  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T08:45:36,411  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T08:45:36,411  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T08:45:36,411  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T08:45:36,411  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T08:45:36,412  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T08:45:36,412  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T08:45:36,412  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T08:45:36,414  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,414  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T08:45:36,415  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T08:45:36,415  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T08:45:36,415  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T08:45:36,416  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T08:45:36,416  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T08:45:36,417  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T08:45:36,417  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T08:45:36,417  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T08:45:36,417  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,418  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 21	21	21	
2024-04-24T08:45:36,418  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 22	22	22	
2024-04-24T08:45:36,419  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 23	23	23	
2024-04-24T08:45:36,419  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 24	24	24	
2024-04-24T08:45:36,420  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 25	25	25	
2024-04-24T08:45:36,420  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 26	26	26	
2024-04-24T08:45:36,420  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 27	27	27	
2024-04-24T08:45:36,420  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 28	28	28	
2024-04-24T08:45:36,421  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 29	29	29	
2024-04-24T08:45:36,421  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (3 out of 30 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,421  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 31	31	31	
2024-04-24T08:45:36,422  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 32	32	32	
2024-04-24T08:45:36,422  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 33	33	33	
2024-04-24T08:45:36,422  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 34	34	34	
2024-04-24T08:45:36,423  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 35	35	35	
2024-04-24T08:45:36,423  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 36	36	36	
2024-04-24T08:45:36,424  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 37	37	37	
2024-04-24T08:45:36,424  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 38	38	38	
2024-04-24T08:45:36,424  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 39	39	39	
2024-04-24T08:45:36,425  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (4 out of 40 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,425  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 41	41	41	
2024-04-24T08:45:36,425  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 42	42	42	
2024-04-24T08:45:36,426  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 43	43	43	
2024-04-24T08:45:36,426  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 44	44	44	
2024-04-24T08:45:36,426  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 45	45	45	
2024-04-24T08:45:36,426  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 46	46	46	
2024-04-24T08:45:36,427  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 47	47	47	
2024-04-24T08:45:36,427  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 48	48	48	
2024-04-24T08:45:36,427  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 49	49	49	
2024-04-24T08:45:36,427  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (5 out of 50 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,428  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 51	51	51	
2024-04-24T08:45:36,428  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 52	52	52	
2024-04-24T08:45:36,429  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 53	53	53	
2024-04-24T08:45:36,429  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 54	54	54	
2024-04-24T08:45:36,429  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 55	55	55	
2024-04-24T08:45:36,430  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 56	56	56	
2024-04-24T08:45:36,430  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 57	57	57	
2024-04-24T08:45:36,431  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 58	58	58	
2024-04-24T08:45:36,431  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 59	59	59	
2024-04-24T08:45:36,431  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (6 out of 60 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,432  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 61	61	61	
2024-04-24T08:45:36,432  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 62	62	62	
2024-04-24T08:45:36,433  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 63	63	63	
2024-04-24T08:45:36,433  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 64	64	64	
2024-04-24T08:45:36,433  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 65	65	65	
2024-04-24T08:45:36,434  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 66	66	66	
2024-04-24T08:45:36,434  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 67	67	67	
2024-04-24T08:45:36,434  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 68	68	68	
2024-04-24T08:45:36,434  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 69	69	69	
2024-04-24T08:45:36,435  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (7 out of 70 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,435  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 71	71	71	
2024-04-24T08:45:36,435  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 72	72	72	
2024-04-24T08:45:36,436  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 73	73	73	
2024-04-24T08:45:36,436  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 74	74	74	
2024-04-24T08:45:36,436  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 75	75	75	
2024-04-24T08:45:36,436  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 76	76	76	
2024-04-24T08:45:36,437  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 77	77	77	
2024-04-24T08:45:36,437  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 78	78	78	
2024-04-24T08:45:36,437  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 79	79	79	
2024-04-24T08:45:36,437  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (8 out of 80 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,438  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 81	81	81	
2024-04-24T08:45:36,438  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 82	82	82	
2024-04-24T08:45:36,438  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 83	83	83	
2024-04-24T08:45:36,438  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 84	84	84	
2024-04-24T08:45:36,439  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 85	85	85	
2024-04-24T08:45:36,439  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 86	86	86	
2024-04-24T08:45:36,439  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 87	87	87	
2024-04-24T08:45:36,439  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 88	88	88	
2024-04-24T08:45:36,440  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 89	89	89	
2024-04-24T08:45:36,440  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (9 out of 90 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,440  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 91	91	91	
2024-04-24T08:45:36,440  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 92	92	92	
2024-04-24T08:45:36,441  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 93	93	93	
2024-04-24T08:45:36,441  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 94	94	94	
2024-04-24T08:45:36,441  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 95	95	95	
2024-04-24T08:45:36,442  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 96	96	96	
2024-04-24T08:45:36,442  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 97	97	97	
2024-04-24T08:45:36,442  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 98	98	98	
2024-04-24T08:45:36,442  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 99	99	99	
2024-04-24T08:45:36,442  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (10 out of 100 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:36,447  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T08:45:36,458  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1577817403_0001_m_000000_0 is done. And is in the process of committing
2024-04-24T08:45:36,459  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T08:45:36,459  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1577817403_0001_m_000000_0 is allowed to commit now
2024-04-24T08:45:36,462  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1577817403_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/test_bad_record_handling_output
2024-04-24T08:45:36,463  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/warehouse/test_bad_records/intString.seq:0+4027
2024-04-24T08:45:36,463  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1577817403_0001_m_000000_0' done.
2024-04-24T08:45:36,467  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1577817403_0001_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=523779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=729284608
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T08:45:36,467  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1577817403_0001_m_000000_0
2024-04-24T08:45:36,468  INFO [Thread-52] mapred.LocalJobRunner: map task executor complete.
2024-04-24T08:45:37,256  INFO [main] mapreduce.Job: Job job_local1577817403_0001 running in uber mode : false
2024-04-24T08:45:37,257  INFO [main] mapreduce.Job:  map 100% reduce 0%
2024-04-24T08:45:37,261  INFO [main] mapreduce.Job: Job job_local1577817403_0001 completed successfully
2024-04-24T08:45:37,269  INFO [main] mapreduce.Job: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=523779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=729284608
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T08:45:37,367  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:45:37,367  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:45:37,368  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:45:37,368  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:45:37,368  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:45:37,368  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:45:37,368  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:45:37,369  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:45:37,369  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:45:37,369  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:45:37,370  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:45:37,375  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:37,375  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:37,376  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@261275ae will be shutdown
2024-04-24T08:45:37,377  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5909ae90 created in the thread with id: 1
2024-04-24T08:45:37,388  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = bf67b0d8-1347-47ec-bc0d-2d0bc3112124
2024-04-24T08:45:37,389  INFO [main] SessionState: Hive Session ID = bf67b0d8-1347-47ec-bc0d-2d0bc3112124
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:45:37,391  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:45:37,399  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/bf67b0d8-1347-47ec-bc0d-2d0bc3112124
2024-04-24T08:45:37,405  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/bf67b0d8-1347-47ec-bc0d-2d0bc3112124
2024-04-24T08:45:37,409  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/bf67b0d8-1347-47ec-bc0d-2d0bc3112124/_tmp_space.db
2024-04-24T08:45:37,409  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data/intString.seq
2024-04-24T08:45:37,422  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): drop table if exists test_bad_records
2024-04-24T08:45:37,428  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:37,456  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:37,457  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:37,457  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:37,457  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:37,457  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=1, getTable_(GetTableRequest)=28}
2024-04-24T08:45:37,457  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.035 seconds
2024-04-24T08:45:37,457  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:37,457  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:37,457  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): drop table if exists test_bad_records
2024-04-24T08:45:37,458  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:45:37,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:37,481  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:37,483  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:37,512  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:37,512  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,095  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:38,095  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=24, dropTable_(String, String, boolean, boolean, boolean)=611, isCompatibleWith_(Configuration)=0}
2024-04-24T08:45:38,096  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.638 seconds
2024-04-24T08:45:38,097  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:45:38,101  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0
2024-04-24T08:45:38,101  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:45:38,101  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=bf67b0d8-1347-47ec-bc0d-2d0bc3112124, clientType=HIVECLI]
2024-04-24T08:45:38,101  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T08:45:38,102  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:45:38,102  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7404ddca, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5909ae90 will be shutdown
2024-04-24T08:45:38,103  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:38,103  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-24T08:45:38,104  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:38,106  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:45:38,106  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:38,107  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9, with PersistenceManager: null will be shutdown
2024-04-24T08:45:38,109  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d664323 created in the thread with id: 1
2024-04-24T08:45:38,124  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9 from thread id: 1
2024-04-24T08:45:38,125  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:38,125  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:38,125  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T08:45:38,127  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:38,127  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:38,128  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d664323 will be shutdown
2024-04-24T08:45:38,129  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3eff6846 created in the thread with id: 1
2024-04-24T08:45:38,135  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:38,136  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:38,136  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:45:38,145  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0
2024-04-24T08:45:38,145  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:38,145  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:38,145  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:38,145  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:45:38,145  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.049 seconds
2024-04-24T08:45:38,146  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:38,146  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:38,146  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:45:38,147  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:45:38,147  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T08:45:38,148  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:45:38,149  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ceb11f9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3eff6846 will be shutdown
2024-04-24T08:45:38,149  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:38,149  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-24T08:45:38,154  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:45:38,156  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:45:38,156  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:45:38,156  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@acd3460, with PersistenceManager: null will be shutdown
2024-04-24T08:45:38,156  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@acd3460, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3ea9a091 created in the thread with id: 1
2024-04-24T08:45:38,161  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@acd3460 from thread id: 1
2024-04-24T08:45:38,161  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:45:38,161  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:45:38,162  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713973538, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:myint, type:int, comment:from deserializer), FieldSchema(name:underscore_int, type:string, comment:from deserializer), FieldSchema(name:mystring, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.format=org.apache.thrift.protocol.TBinaryProtocol, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, totalSize=0, bucketing_version=2, numRows=0, numFiles=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:45:38,177  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/warehouse/test_bad_records
2024-04-24T08:45:38,234  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:38,234  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=72}
2024-04-24T08:45:38,235  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.088 seconds
2024-04-24T08:45:38,235  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data' into table test_bad_records
2024-04-24T08:45:38,236  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0
2024-04-24T08:45:38,236  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,302  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,306  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0
2024-04-24T08:45:38,306  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:45:38,306  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:45:38,306  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:45:38,306  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=66}
2024-04-24T08:45:38,306  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.071 seconds
2024-04-24T08:45:38,307  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:45:38,307  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:45:38,307  INFO [main] ql.Driver: Executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data' into table test_bad_records
2024-04-24T08:45:38,307  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T08:45:38,307  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713973520848/data
2024-04-24T08:45:38,307  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,322  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,324  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,346  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,356  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:45:38,356  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:45:38,394  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T08:45:38,394  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,410  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,410  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T08:45:38,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,424  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,425  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:45:38,426  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:45:38,479  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T08:45:38,480  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:45:38,480  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, alter_table_(String, String, String, Table, EnvironmentContext, String)=91, getTable_(GetTableRequest)=68}
2024-04-24T08:45:38,480  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424084537_c945227f-8484-4e75-92ac-72f429fa8ac0); Time taken: 0.172 seconds
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:45:38,534  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:45:38,535  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:45:38,535  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:45:38,535  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:45:38,535  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:45:38,536  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:45:38,536  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T08:45:38,537  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T08:45:38,541  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T08:45:38,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:45:38,566  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:45:38,578  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T08:45:38,586  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T08:45:38,591  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T08:45:38,596  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T08:45:38,602  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T08:45:38,624  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T08:45:38,640  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1111985827_0002
2024-04-24T08:45:38,640  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T08:45:38,706  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T08:45:38,706  INFO [main] mapreduce.Job: Running job: job_local1111985827_0002
2024-04-24T08:45:38,707  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T08:45:38,707  INFO [Thread-97] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:45:38,707  INFO [Thread-97] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:45:38,708  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T08:45:38,717  INFO [Thread-97] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T08:45:38,717  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1111985827_0002_m_000000_0
2024-04-24T08:45:38,721  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:45:38,721  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:45:38,722  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T08:45:38,723  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@3bcb77c7
2024-04-24T08:45:38,745  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=myint,underscore_int,mystring, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713973538}
2024-04-24T08:45:38,746  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T08:45:38,747  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T08:45:38,747  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T08:45:38,747  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T08:45:38,747  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T08:45:38,747  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T08:45:38,748  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T08:45:38,748  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T08:45:38,748  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T08:45:38,748  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:38,749  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T08:45:38,749  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T08:45:38,750  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T08:45:38,750  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T08:45:38,750  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T08:45:38,750  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T08:45:38,751  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T08:45:38,751  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T08:45:38,751  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T08:45:38,752  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:45:38,752 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: 2 out of 20 crosses configured threshold (0.009999999776482582)
2024-04-24T08:45:38,752  INFO [Thread-97] mapred.LocalJobRunner: map task executor complete.
2024-04-24T08:45:38,753  WARN [Thread-97] mapred.LocalJobRunner: job_local1111985827_0002
java.lang.Exception: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.1.0.jar:?]
Caused by: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader$InputErrorTracker.incErrors(HCatRecordReader.java:282) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:196) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
2024-04-24T08:45:39,707  INFO [main] mapreduce.Job: Job job_local1111985827_0002 running in uber mode : false
2024-04-24T08:45:39,707  INFO [main] mapreduce.Job:  map 0% reduce 0%
2024-04-24T08:45:39,708  INFO [main] mapreduce.Job: Job job_local1111985827_0002 failed with state FAILED due to: NA
2024-04-24T08:45:39,708  INFO [main] mapreduce.Job: Counters: 0
2024-04-24T08:45:39,731  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:45:39,731  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
