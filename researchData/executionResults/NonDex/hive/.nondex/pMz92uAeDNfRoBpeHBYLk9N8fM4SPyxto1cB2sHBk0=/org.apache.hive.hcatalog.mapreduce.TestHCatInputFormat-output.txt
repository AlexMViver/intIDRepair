2024-04-24T09:28:24,972  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/hcatalog/core/target/testconf/hive-site.xml
2024-04-24T09:28:25,293  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T09:28:25,351  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:28:25,352  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:28:25,352  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:28:25,352  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:28:25,352  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:28:25,352  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:28:25,353  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:28:25,353  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:28:25,353  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:28:25,354  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:28:25,354  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:28:25,612  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:25,752  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:28:25,784  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:25,791  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T09:28:25,792  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T09:28:25,815  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:28:25,820  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T09:28:26,508  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:28:26,513  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T09:28:27,143  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T09:28:27,144  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: null will be shutdown
2024-04-24T09:28:27,166  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 created in the thread with id: 1
2024-04-24T09:28:29,461  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T09:28:29,462  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T09:28:29,462  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9 from thread id: 1
2024-04-24T09:28:29,704  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T09:28:29,735  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T09:28:29,768  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T09:28:29,770  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T09:28:29,886  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T09:28:29,913  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:28:29,916  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T09:28:29,918  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:28:29,918  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T09:28:29,920  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:28:29,923  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T09:28:29,925  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:28:29,925  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T09:28:29,930  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T09:28:29,931  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T09:28:29,932  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T09:28:29,935  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T09:28:29,936  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T09:28:29,937  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T09:28:29,939  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 917239f0-0a88-4936-b2c2-22825dce35af
2024-04-24T09:28:30,088  INFO [main] SessionState: Hive Session ID = 917239f0-0a88-4936-b2c2-22825dce35af
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:28:30,101  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:28:30,176  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/917239f0-0a88-4936-b2c2-22825dce35af
2024-04-24T09:28:30,180  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/917239f0-0a88-4936-b2c2-22825dce35af
2024-04-24T09:28:30,184  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/917239f0-0a88-4936-b2c2-22825dce35af/_tmp_space.db
2024-04-24T09:28:30,185  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data/intString.seq
2024-04-24T09:28:30,226  INFO [main] compress.CodecPool: Got brand-new compressor [.deflate]
2024-04-24T09:28:30,292  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): drop table if exists test_bad_records
2024-04-24T09:28:31,536  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,538  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,539  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,540  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,541  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,543  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,547  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:28:31,623  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:31,623  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:31,624  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 will be shutdown
2024-04-24T09:28:31,625  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 created in the thread with id: 1
2024-04-24T09:28:31,640  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:31,641  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:31,677  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T09:28:31,942  INFO [main] reflections.Reflections: Reflections took 180 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:28:32,122  INFO [main] reflections.Reflections: Reflections took 134 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:28:32,243  INFO [main] reflections.Reflections: Reflections took 115 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:28:32,253  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:32,393  INFO [main] reflections.Reflections: Reflections took 115 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:28:32,454  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:32,456  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:32,459  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:32,460  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getAllFunctions_()=49, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T09:28:32,461  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 2.169 seconds
2024-04-24T09:28:32,461  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:32,462  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:32,465  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): drop table if exists test_bad_records
2024-04-24T09:28:32,469  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:28:32,470  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:32,486  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:32,486  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-24T09:28:32,486  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 0.022 seconds
2024-04-24T09:28:32,487  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:28:32,562  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665
2024-04-24T09:28:32,564  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T09:28:32,578  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=917239f0-0a88-4936-b2c2-22825dce35af, clientType=HIVECLI]
2024-04-24T09:28:32,580  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T09:28:32,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:28:32,582  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 will be shutdown
2024-04-24T09:28:32,582  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:32,582  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T09:28:32,583  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:32,585  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:28:32,585  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:32,586  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad, with PersistenceManager: null will be shutdown
2024-04-24T09:28:32,586  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1600a8a2 created in the thread with id: 1
2024-04-24T09:28:32,595  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad from thread id: 1
2024-04-24T09:28:32,595  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:32,595  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:32,601  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T09:28:32,607  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:32,607  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:32,608  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1600a8a2 will be shutdown
2024-04-24T09:28:32,608  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@685d7ba5 created in the thread with id: 1
2024-04-24T09:28:32,612  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:32,612  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:32,614  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T09:28:32,626  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665
2024-04-24T09:28:32,627  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:32,627  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:32,627  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:32,627  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T09:28:32,628  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 0.14 seconds
2024-04-24T09:28:32,628  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:32,629  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:32,629  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:28:32,629  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:28:32,629  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T09:28:32,630  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:28:32,630  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@727986ad, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@685d7ba5 will be shutdown
2024-04-24T09:28:32,630  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:32,630  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-24T09:28:32,731  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:32,732  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:28:32,732  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:32,733  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: null will be shutdown
2024-04-24T09:28:32,733  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c68d0db created in the thread with id: 1
2024-04-24T09:28:32,737  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56 from thread id: 1
2024-04-24T09:28:32,737  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:32,738  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:32,738  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713976112, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:underscore_int, type:int, comment:from deserializer), FieldSchema(name:mystring, type:string, comment:from deserializer), FieldSchema(name:myint, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, rawDataSize=0, numFiles=0, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, bucketing_version=2, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T09:28:32,748  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/warehouse/test_bad_records
2024-04-24T09:28:32,886  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:32,887  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=148}
2024-04-24T09:28:32,887  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 0.257 seconds
2024-04-24T09:28:32,888  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data' into table test_bad_records
2024-04-24T09:28:32,890  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665
2024-04-24T09:28:32,896  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:32,982  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,019  INFO [main] compress.CodecPool: Got brand-new decompressor [.deflate]
2024-04-24T09:28:33,028  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665
2024-04-24T09:28:33,028  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:33,028  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:33,028  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:33,028  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=88, isCompatibleWith_(Configuration)=0}
2024-04-24T09:28:33,029  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 0.14 seconds
2024-04-24T09:28:33,029  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:33,029  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:33,029  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data' into table test_bad_records
2024-04-24T09:28:33,029  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T09:28:33,031  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data
2024-04-24T09:28:33,031  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:33,047  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,051  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:33,066  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,084  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:28:33,084  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:28:33,153  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T09:28:33,153  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:33,169  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,169  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T09:28:33,170  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:33,186  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,191  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:28:33,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:28:33,243  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T09:28:33,243  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:33,243  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {alter_table_(String, String, String, Table, EnvironmentContext, String)=117, isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=67}
2024-04-24T09:28:33,243  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092825_52e5669c-6d7a-4078-8fc0-1e1294806665); Time taken: 0.214 seconds
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:28:33,291  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:28:33,292  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:28:33,292  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:28:33,292  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:28:33,292  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:28:33,292  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T09:28:33,293  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T09:28:33,306  INFO [main] common.HiveClientCache: Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
2024-04-24T09:28:33,316  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:33,317  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:33,317  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c68d0db will be shutdown
2024-04-24T09:28:33,318  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@10bdfbcc created in the thread with id: 1
2024-04-24T09:28:33,331  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:33,332  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:33,367  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T09:28:33,378  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:33,394  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:33,544  INFO [main] beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2024-04-24T09:28:33,558  WARN [main] impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
2024-04-24T09:28:33,573  INFO [main] impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-04-24T09:28:33,573  INFO [main] impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-24T09:28:33,625  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T09:28:33,633  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T09:28:33,647  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T09:28:33,660  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T09:28:33,698  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:33,698  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T09:28:33,722  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T09:28:33,759  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1027222678_0001
2024-04-24T09:28:33,759  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T09:28:33,868  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T09:28:33,869  INFO [main] mapreduce.Job: Running job: job_local1027222678_0001
2024-04-24T09:28:33,870  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T09:28:33,879  INFO [Thread-52] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:28:33,879  INFO [Thread-52] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:28:33,879  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T09:28:33,898  INFO [Thread-52] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T09:28:33,899  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1027222678_0001_m_000000_0
2024-04-24T09:28:33,924  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:28:33,924  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:28:33,939  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T09:28:33,942  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@7b270bcb
2024-04-24T09:28:33,989  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=underscore_int,mystring,myint, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713976113}
2024-04-24T09:28:33,993  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T09:28:33,993  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T09:28:33,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T09:28:33,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T09:28:33,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T09:28:33,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T09:28:33,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T09:28:33,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T09:28:33,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T09:28:33,996  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:33,997  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T09:28:33,997  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T09:28:33,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T09:28:33,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T09:28:33,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T09:28:33,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T09:28:33,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T09:28:34,000  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T09:28:34,000  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T09:28:34,000  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,000  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 21	21	21	
2024-04-24T09:28:34,001  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 22	22	22	
2024-04-24T09:28:34,001  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 23	23	23	
2024-04-24T09:28:34,002  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 24	24	24	
2024-04-24T09:28:34,002  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 25	25	25	
2024-04-24T09:28:34,002  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 26	26	26	
2024-04-24T09:28:34,002  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 27	27	27	
2024-04-24T09:28:34,002  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 28	28	28	
2024-04-24T09:28:34,003  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 29	29	29	
2024-04-24T09:28:34,003  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (3 out of 30 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,003  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 31	31	31	
2024-04-24T09:28:34,004  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 32	32	32	
2024-04-24T09:28:34,004  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 33	33	33	
2024-04-24T09:28:34,004  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 34	34	34	
2024-04-24T09:28:34,004  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 35	35	35	
2024-04-24T09:28:34,005  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 36	36	36	
2024-04-24T09:28:34,005  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 37	37	37	
2024-04-24T09:28:34,005  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 38	38	38	
2024-04-24T09:28:34,005  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 39	39	39	
2024-04-24T09:28:34,006  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (4 out of 40 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,006  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 41	41	41	
2024-04-24T09:28:34,006  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 42	42	42	
2024-04-24T09:28:34,007  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 43	43	43	
2024-04-24T09:28:34,007  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 44	44	44	
2024-04-24T09:28:34,007  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 45	45	45	
2024-04-24T09:28:34,008  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 46	46	46	
2024-04-24T09:28:34,008  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 47	47	47	
2024-04-24T09:28:34,008  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 48	48	48	
2024-04-24T09:28:34,008  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 49	49	49	
2024-04-24T09:28:34,009  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (5 out of 50 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,009  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 51	51	51	
2024-04-24T09:28:34,009  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 52	52	52	
2024-04-24T09:28:34,009  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 53	53	53	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 54	54	54	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 55	55	55	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 56	56	56	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 57	57	57	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 58	58	58	
2024-04-24T09:28:34,010  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 59	59	59	
2024-04-24T09:28:34,011  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (6 out of 60 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,011  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 61	61	61	
2024-04-24T09:28:34,011  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 62	62	62	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 63	63	63	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 64	64	64	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 65	65	65	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 66	66	66	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 67	67	67	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 68	68	68	
2024-04-24T09:28:34,012  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 69	69	69	
2024-04-24T09:28:34,013  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (7 out of 70 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,013  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 71	71	71	
2024-04-24T09:28:34,013  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 72	72	72	
2024-04-24T09:28:34,013  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 73	73	73	
2024-04-24T09:28:34,013  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 74	74	74	
2024-04-24T09:28:34,014  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 75	75	75	
2024-04-24T09:28:34,014  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 76	76	76	
2024-04-24T09:28:34,014  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 77	77	77	
2024-04-24T09:28:34,014  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 78	78	78	
2024-04-24T09:28:34,014  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 79	79	79	
2024-04-24T09:28:34,014  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (8 out of 80 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,015  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 81	81	81	
2024-04-24T09:28:34,015  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 82	82	82	
2024-04-24T09:28:34,015  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 83	83	83	
2024-04-24T09:28:34,015  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 84	84	84	
2024-04-24T09:28:34,016  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 85	85	85	
2024-04-24T09:28:34,016  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 86	86	86	
2024-04-24T09:28:34,016  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 87	87	87	
2024-04-24T09:28:34,016  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 88	88	88	
2024-04-24T09:28:34,016  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 89	89	89	
2024-04-24T09:28:34,016  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (9 out of 90 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 91	91	91	
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 92	92	92	
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 93	93	93	
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 94	94	94	
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 95	95	95	
2024-04-24T09:28:34,017  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 96	96	96	
2024-04-24T09:28:34,018  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 97	97	97	
2024-04-24T09:28:34,018  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 98	98	98	
2024-04-24T09:28:34,018  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 99	99	99	
2024-04-24T09:28:34,018  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (10 out of 100 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:34,021  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T09:28:34,027  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1027222678_0001_m_000000_0 is done. And is in the process of committing
2024-04-24T09:28:34,028  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T09:28:34,028  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1027222678_0001_m_000000_0 is allowed to commit now
2024-04-24T09:28:34,030  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1027222678_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/test_bad_record_handling_output
2024-04-24T09:28:34,031  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/warehouse/test_bad_records/intString.seq:0+4027
2024-04-24T09:28:34,031  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1027222678_0001_m_000000_0' done.
2024-04-24T09:28:34,033  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1027222678_0001_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=523779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=757596160
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T09:28:34,033  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1027222678_0001_m_000000_0
2024-04-24T09:28:34,034  INFO [Thread-52] mapred.LocalJobRunner: map task executor complete.
2024-04-24T09:28:34,875  INFO [main] mapreduce.Job: Job job_local1027222678_0001 running in uber mode : false
2024-04-24T09:28:34,877  INFO [main] mapreduce.Job:  map 100% reduce 0%
2024-04-24T09:28:34,881  INFO [main] mapreduce.Job: Job job_local1027222678_0001 completed successfully
2024-04-24T09:28:34,886  INFO [main] mapreduce.Job: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=523779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=757596160
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T09:28:34,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:28:34,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:28:34,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:28:34,926  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:28:34,927  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:28:34,930  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:34,930  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:34,931  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@10bdfbcc will be shutdown
2024-04-24T09:28:34,931  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6fd9dbe6 created in the thread with id: 1
2024-04-24T09:28:34,936  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 9f14fcd3-d767-41d7-8d18-5d9d3924f0c3
2024-04-24T09:28:34,936  INFO [main] SessionState: Hive Session ID = 9f14fcd3-d767-41d7-8d18-5d9d3924f0c3
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:28:34,937  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:28:34,943  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/9f14fcd3-d767-41d7-8d18-5d9d3924f0c3
2024-04-24T09:28:34,946  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/9f14fcd3-d767-41d7-8d18-5d9d3924f0c3
2024-04-24T09:28:34,948  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/9f14fcd3-d767-41d7-8d18-5d9d3924f0c3/_tmp_space.db
2024-04-24T09:28:34,949  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data/intString.seq
2024-04-24T09:28:34,957  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): drop table if exists test_bad_records
2024-04-24T09:28:34,960  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:34,972  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:34,973  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:34,973  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:34,973  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:34,973  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=13, flushCache_()=0, isCompatibleWith_(Configuration)=1}
2024-04-24T09:28:34,973  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.016 seconds
2024-04-24T09:28:34,973  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:34,973  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:34,973  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): drop table if exists test_bad_records
2024-04-24T09:28:34,974  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:28:34,974  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:34,986  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:34,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:34,999  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,000  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,386  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:35,386  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=398, getTable_(GetTableRequest)=12}
2024-04-24T09:28:35,386  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.413 seconds
2024-04-24T09:28:35,387  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:28:35,389  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c
2024-04-24T09:28:35,389  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T09:28:35,389  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9f14fcd3-d767-41d7-8d18-5d9d3924f0c3, clientType=HIVECLI]
2024-04-24T09:28:35,389  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T09:28:35,389  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:28:35,389  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e50ae56, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6fd9dbe6 will be shutdown
2024-04-24T09:28:35,390  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:35,390  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-24T09:28:35,391  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:35,392  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:28:35,392  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:35,392  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a, with PersistenceManager: null will be shutdown
2024-04-24T09:28:35,393  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12266084 created in the thread with id: 1
2024-04-24T09:28:35,400  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a from thread id: 1
2024-04-24T09:28:35,400  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:35,400  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:35,400  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T09:28:35,402  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:35,402  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:35,402  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12266084 will be shutdown
2024-04-24T09:28:35,403  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1a67f8b4 created in the thread with id: 1
2024-04-24T09:28:35,406  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:35,406  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:35,407  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T09:28:35,411  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c
2024-04-24T09:28:35,411  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:35,411  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:35,412  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:35,412  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T09:28:35,412  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.025 seconds
2024-04-24T09:28:35,412  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:35,412  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:35,412  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:28:35,412  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:28:35,412  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T09:28:35,413  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:28:35,413  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2676d96a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1a67f8b4 will be shutdown
2024-04-24T09:28:35,413  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:35,413  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-24T09:28:35,416  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:28:35,417  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:28:35,417  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:28:35,417  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26495639, with PersistenceManager: null will be shutdown
2024-04-24T09:28:35,417  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26495639, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@45c408a4 created in the thread with id: 1
2024-04-24T09:28:35,420  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26495639 from thread id: 1
2024-04-24T09:28:35,420  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:28:35,421  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:28:35,421  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713976115, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:underscore_int, type:int, comment:from deserializer), FieldSchema(name:mystring, type:string, comment:from deserializer), FieldSchema(name:myint, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, rawDataSize=0, totalSize=0, numRows=0, numFilesErasureCoded=0, numFiles=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T09:28:35,430  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/warehouse/test_bad_records
2024-04-24T09:28:35,472  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:35,472  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=50}
2024-04-24T09:28:35,472  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.059 seconds
2024-04-24T09:28:35,472  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data' into table test_bad_records
2024-04-24T09:28:35,473  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c
2024-04-24T09:28:35,474  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,520  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,522  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c
2024-04-24T09:28:35,522  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:28:35,522  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:28:35,522  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:28:35,523  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=47}
2024-04-24T09:28:35,523  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.05 seconds
2024-04-24T09:28:35,523  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:28:35,523  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:28:35,523  INFO [main] ql.Driver: Executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data' into table test_bad_records
2024-04-24T09:28:35,523  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T09:28:35,524  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713976104839/data
2024-04-24T09:28:35,524  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,539  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,541  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,554  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,562  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:28:35,562  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:28:35,595  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T09:28:35,596  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,607  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,608  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T09:28:35,608  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,620  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,622  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:28:35,622  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:28:35,660  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T09:28:35,660  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:28:35,660  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=53, alter_table_(String, String, String, Table, EnvironmentContext, String)=70, isCompatibleWith_(Configuration)=1}
2024-04-24T09:28:35,660  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424092834_ae0ec74e-f271-49e5-934f-cf5c085f614c); Time taken: 0.137 seconds
2024-04-24T09:28:35,699  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:28:35,699  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:28:35,699  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:28:35,699  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:28:35,699  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:28:35,700  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:28:35,701  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T09:28:35,702  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T09:28:35,704  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T09:28:35,709  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:28:35,721  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:28:35,729  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T09:28:35,736  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T09:28:35,741  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T09:28:35,745  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T09:28:35,750  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T09:28:35,770  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T09:28:35,785  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1874818472_0002
2024-04-24T09:28:35,785  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T09:28:35,840  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T09:28:35,840  INFO [main] mapreduce.Job: Running job: job_local1874818472_0002
2024-04-24T09:28:35,841  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T09:28:35,841  INFO [Thread-97] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:28:35,841  INFO [Thread-97] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:28:35,842  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T09:28:35,850  INFO [Thread-97] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T09:28:35,851  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1874818472_0002_m_000000_0
2024-04-24T09:28:35,855  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:28:35,855  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:28:35,855  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T09:28:35,857  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@1ae782df
2024-04-24T09:28:35,880  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=underscore_int,mystring,myint, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713976115}
2024-04-24T09:28:35,881  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T09:28:35,881  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T09:28:35,882  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T09:28:35,882  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T09:28:35,882  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T09:28:35,883  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T09:28:35,883  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T09:28:35,883  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T09:28:35,883  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T09:28:35,883  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:35,884  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T09:28:35,884  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T09:28:35,884  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T09:28:35,884  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T09:28:35,885  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T09:28:35,885  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T09:28:35,885  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T09:28:35,885  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T09:28:35,885  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T09:28:35,886  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:28:35,886 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: 2 out of 20 crosses configured threshold (0.009999999776482582)
2024-04-24T09:28:35,886  INFO [Thread-97] mapred.LocalJobRunner: map task executor complete.
2024-04-24T09:28:35,887  WARN [Thread-97] mapred.LocalJobRunner: job_local1874818472_0002
java.lang.Exception: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.1.0.jar:?]
Caused by: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader$InputErrorTracker.incErrors(HCatRecordReader.java:282) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:196) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
2024-04-24T09:28:36,841  INFO [main] mapreduce.Job: Job job_local1874818472_0002 running in uber mode : false
2024-04-24T09:28:36,841  INFO [main] mapreduce.Job:  map 0% reduce 0%
2024-04-24T09:28:36,841  INFO [main] mapreduce.Job: Job job_local1874818472_0002 failed with state FAILED due to: NA
2024-04-24T09:28:36,842  INFO [main] mapreduce.Job: Counters: 0
2024-04-24T09:28:36,861  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:28:36,862  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
