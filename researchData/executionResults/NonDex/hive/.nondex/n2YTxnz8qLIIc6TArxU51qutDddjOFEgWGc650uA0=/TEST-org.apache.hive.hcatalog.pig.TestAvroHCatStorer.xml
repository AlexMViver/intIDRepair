<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="46.904" tests="29" errors="4" skipped="5" failures="2">
  <properties>
    <property name="build.test.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="test.tmp.dir.uri" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="qfile" value=""/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="test.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/warehouse"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="run_disabled" value=""/>
    <property name="nondexPrintstack" value="false"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="user.country.format" value="PT"/>
    <property name="test.tmp.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter3280213720423179652.jar /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire 2024-04-23T21-59-46_173-jvmRun1 surefire3118704299472895778tmp surefire_38074086333583575727339tmp"/>
    <property name="nondexExecid" value="n2YTxnz8qLIIc6TArxU51qutDddjOFEgWGc650uA0="/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/java-8-openjdk-amd64/jre"/>
    <property name="basedir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="file.separator" value="/"/>
    <property name="nondexEnd" value="9223372036854775807"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="derby.stream.error.file" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/derby.log"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="log4j.debug" value="true"/>
    <property name="sun.boot.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex/nondex-instr.jar:/home/alex/.m2/repository/edu/illinois/nondex-common/2.1.7/nondex-common-2.1.7.jar:/home/alex/.m2/repository/edu/illinois/nondex-annotations/2.1.7/nondex-annotations-2.1.7.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="spark.home" value=""/>
    <property name="java.runtime.version" value="1.8.0_402-8u402-ga-2ubuntu1~22.04-b06"/>
    <property name="user.name" value="alex"/>
    <property name="test.src.tables" value=""/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="qfile_regex" value=""/>
    <property name="hadoop.bin.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../testutils/hadoop"/>
    <property name="localRepository" value="/home/alex/.m2/repository"/>
    <property name="build.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target"/>
    <property name="nondexDir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex"/>
    <property name="mapred.job.tracker" value="local"/>
    <property name="nondexJarDir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex"/>
    <property name="java.io.tmpdir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="com.zaxxer.hikari.pool_number" value="3"/>
    <property name="java.version" value="1.8.0_402"/>
    <property name="hive.version" value="4.0.0-SNAPSHOT"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="test.data.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib"/>
    <property name="java.vendor" value="Private Build"/>
    <property name="java.specification.maintenance.version" value="5"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="initScript" value=""/>
    <property name="sun.desktop" value="gnome"/>
    <property name="calcite.enable.rexnode.digest.normalize" value="false"/>
    <property name="nondexStart" value="0"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="log4j.configurationFile" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties"/>
    <property name="java.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="java.vm.vendor" value="Private Build"/>
    <property name="hadoop.version" value="3.1.0"/>
    <property name="user.timezone" value="US/Pacific"/>
    <property name="test.local.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/localfs/warehouse"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="os.name" value="Linux"/>
    <property name="test.build.data" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="clustermode" value=""/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="user.country" value="US"/>
    <property name="test.data.files" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="nondexLogging" value="CONFIG"/>
    <property name="surefire.test.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/alex"/>
    <property name="user.language" value="en"/>
    <property name="java.security.krb5.conf" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/krb5.conf"/>
    <property name="test.dfs.mkdir" value=""/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="nondexFilter" value=".*"/>
    <property name="nondexSeed" value="974622"/>
    <property name="surefire.real.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter3280213720423179652.jar"/>
    <property name="hadoop.log.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="6.5.0-28-generic"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="hive.test.console.log.level" value="INFO"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="maven.local.repository" value="/home/alex/.m2/repository"/>
    <property name="user.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="os.arch" value="amd64"/>
    <property name="nondexMode" value="FULL"/>
    <property name="user.language.format" value="pt"/>
    <property name="antlr.version" value="3.5.2"/>
    <property name="derby.version" value="10.14.1.0"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.402-b06"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="java.class.version" value="52.0"/>
    <property name="test.output.overwrite" value=""/>
    <property name="hive.root" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../"/>
  </properties>
  <testcase name="testWriteTinyint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
    <system-err><![CDATA[SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,098996 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@185a6e9]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@185a6e9) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@77f1baf5
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,020369 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={DRFA, console}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(PatternSelector=null, pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", charset="null", header="null", Replace=null, alwaysWriteExceptions="null", disableAnsi="null", Configuration(HiveLog4j2Test), footer="null", noConsoleNoAnsi="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(follow="null", direct="null", target="SYSTEM_ERR", bufferedIo="null", bufferSize="null", immediateFlush="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), Configuration(HiveLog4j2Test), ignoreExceptions="null", name="console", ={}, Filter=null)
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(Configuration(HiveLog4j2Test), alwaysWriteExceptions="null", Replace=null, pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, charset="null", header="null", disableAnsi="null", noConsoleNoAnsi="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(modulate="true", interval="1", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(Configuration(HiveLog4j2Test), compressionLevel="null", max="30", stopCustomActionsOnError="null", fileIndex="null", min="null", tempCompressedFilePattern="null", ={})
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(filePermissions="null", fileName="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log", fileOwner="null", advertiseURI="null", filePattern="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log.%d{yyyy-MM-dd}", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), append="null", fileGroup="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), advertise="null", immediateFlush="null", bufferSize="null", bufferedIo="null", Configuration(HiveLog4j2Test), name="DRFA", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ={}, Filter=null)
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log seek to 481628777
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T10:52:53.936-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-10:52:55.538, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-10:52:55.539, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@780cb77...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@780cb77 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@35645047
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@185a6e9
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@185a6e9) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@185a6e9] started OK.
2024-04-24T10:52:55,715  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
]]></system-err>
  </testcase>
  <testcase name="testWriteSmallint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDateCharTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteTimestamp" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteDecimalXY" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDynamicPartitioningMultiPartColsInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="13.761"/>
  <testcase name="testEmptyStore" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.266"/>
  <testcase name="testStoreFuncAllSimpleTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.606"/>
  <testcase name="testDynamicPartitioningMultiPartColsInDataPartialSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.623"/>
  <testcase name="testWriteDate2" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.226"/>
  <testcase name="testWriteDate3" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="2.428"/>
  <testcase name="testStoreInPartiitonedTbl" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.37">
    <failure message="expected:&lt;1&gt; but was:&lt;2&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testStoreInPartiitonedTbl(AbstractHCatStorerTest.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-24T10:53:21,697  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:21,697  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:21,697  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:21,698  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:21,701  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:21,701  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:21,701  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26501a93, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5dc1c1c1 will be shutdown
2024-04-24T10:53:21,701  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26501a93, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@37cc103c created in the thread with id: 1
2024-04-24T10:53:21,703  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = f9f774af-a96b-4185-8b25-33239c07a86a
2024-04-24T10:53:21,703  INFO [main] SessionState: Hive Session ID = f9f774af-a96b-4185-8b25-33239c07a86a
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:21,704  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:21,709  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/f9f774af-a96b-4185-8b25-33239c07a86a
2024-04-24T10:53:21,711  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/f9f774af-a96b-4185-8b25-33239c07a86a
2024-04-24T10:53:21,713  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/f9f774af-a96b-4185-8b25-33239c07a86a/_tmp_space.db
2024-04-24T10:53:21,714  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): drop table if exists junit_unparted
2024-04-24T10:53:21,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:21,716  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:21,716  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:21,716  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:21,717  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:53:21,717  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.002 seconds
2024-04-24T10:53:21,717  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:21,717  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:21,717  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): drop table if exists junit_unparted
2024-04-24T10:53:21,717  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:21,717  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:21,719  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:21,719  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:21,720  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.002 seconds
2024-04-24T10:53:21,720  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): use default
2024-04-24T10:53:21,721  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:21,722  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:21,722  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.002 seconds
2024-04-24T10:53:21,722  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): use default
2024-04-24T10:53:21,722  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:21,723  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:21,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:21,725  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:21,725  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:53:21,726  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.003 seconds
2024-04-24T10:53:21,726  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:21,727  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a
2024-04-24T10:53:21,727  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:21,727  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f9f774af-a96b-4185-8b25-33239c07a86a, clientType=HIVECLI]
2024-04-24T10:53:21,727  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:21,727  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:21,727  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@26501a93, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@37cc103c will be shutdown
2024-04-24T10:53:21,728  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:21,728  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-24T10:53:21,728  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:21,729  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:21,729  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:21,730  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511, with PersistenceManager: null will be shutdown
2024-04-24T10:53:21,730  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3be23894 created in the thread with id: 1
2024-04-24T10:53:21,732  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511 from thread id: 1
2024-04-24T10:53:21,732  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:21,732  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:21,732  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted position=13
2024-04-24T10:53:21,733  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:21,733  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:21,734  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3be23894 will be shutdown
2024-04-24T10:53:21,734  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@62b0059 created in the thread with id: 1
2024-04-24T10:53:21,736  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:21,736  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:21,736  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:21,737  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a
2024-04-24T10:53:21,737  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:21,737  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:21,737  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:21,737  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:21,737  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.011 seconds
2024-04-24T10:53:21,738  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:21,738  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:21,738  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:21,738  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:21,738  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:21,738  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:21,738  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f9f6511, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@62b0059 will be shutdown
2024-04-24T10:53:21,738  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:21,738  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-24T10:53:21,740  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:21,740  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:21,740  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:21,741  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e, with PersistenceManager: null will be shutdown
2024-04-24T10:53:21,741  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@598ad90e created in the thread with id: 1
2024-04-24T10:53:21,743  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e from thread id: 1
2024-04-24T10:53:21,743  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:21,743  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:21,743  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted, dbName:default, owner:alex, createTime:1713981201, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:b, type:string, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:53:21,747  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/junit_unparted
2024-04-24T10:53:21,765  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:21,765  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=22}
2024-04-24T10:53:21,765  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105321_498ada79-a4bb-4674-9594-b905934b0b2a); Time taken: 0.027 seconds
2024-04-24T10:53:21,784  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:53:21,785  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-595886ed-7a92-48c2-8ab1-ed5f04088694
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:21,815  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:21,816  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:21,817  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:53:21,817  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:21,819  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:21,829  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:21,849  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:21,849  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:21,849  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:21,849  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:21,849  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:21,850  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:21,850  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:21,851  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:21,853  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-24T10:53:21,861  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:53:21,861  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:53:21,862  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:53:21,862  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-24T10:53:21,862  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:53:21,868  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:21,869  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:53:21,869  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:53:21,870  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:53:21,870  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:53:21,870  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:53:21,871  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713981201870-0
2024-04-24T10:53:21,875  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:53:21,877  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:21,893  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:21,893  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:21,893  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:21,893  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:21,893  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:21,894  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:21,894  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:21,895  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:21,896  INFO [JobControl] metastore.HMSHandler: 26: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:21,896  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:21,896  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9775e98, with PersistenceManager: null will be shutdown
2024-04-24T10:53:21,896  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9775e98, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@53a63f created in the thread with id: 1061
2024-04-24T10:53:21,898  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9775e98 from thread id: 1061
2024-04-24T10:53:21,898  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:21,898  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:21,899  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:21,912  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:53:21,916  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:53:21,917  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:53:21,917  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:21,917  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:53:21,918  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:53:21,935  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:53:21,948  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1197125904_0018
2024-04-24T10:53:21,948  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:53:22,000  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:53:22,000  INFO [Thread-995] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:53:22,005  INFO [Thread-995] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,005  INFO [Thread-995] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,006  INFO [Thread-995] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:53:22,007  INFO [Thread-995] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,007  INFO [Thread-995] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,017  INFO [Thread-995] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:53:22,017  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1197125904_0018_m_000000_0
2024-04-24T10:53:22,023  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,023  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,025  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,025  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,025  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:53:22,026  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 23
Input split[0]:
   Length = 23
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:53:22,027  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:53:22,027  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/input.data:0+23
2024-04-24T10:53:22,030  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,030  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,033  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:53:22,033  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-24T10:53:22,033  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1] C:  R: 
2024-04-24T10:53:22,043  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:22,044  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1197125904_0018_m_000000_0 is done. And is in the process of committing
2024-04-24T10:53:22,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,046  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:22,046  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1197125904_0018_m_000000_0 is allowed to commit now
2024-04-24T10:53:22,047  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,047  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,049  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1197125904_0018_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/junit_unparted/_SCRATCH0,5756316894698642/b=1
2024-04-24T10:53:22,049  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:53:22,049  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1197125904_0018_m_000000_0' done.
2024-04-24T10:53:22,049  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1197125904_0018_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=61554
		FILE: Number of bytes written=10285359
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1030750208
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:53:22,049  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1197125904_0018_m_000000_0
2024-04-24T10:53:22,049  INFO [Thread-995] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,090  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,090  INFO [Thread-995] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:22,091  INFO [Thread-995] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:22,092  INFO [Thread-995] metastore.HMSHandler: 27: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:22,092  INFO [Thread-995] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:22,092  INFO [Thread-995] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6e334a3d, with PersistenceManager: null will be shutdown
2024-04-24T10:53:22,092  INFO [Thread-995] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6e334a3d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3d421009 created in the thread with id: 1081
2024-04-24T10:53:22,094  INFO [Thread-995] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6e334a3d from thread id: 1081
2024-04-24T10:53:22,094  INFO [Thread-995] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:22,094  INFO [Thread-995] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:22,094  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:22,128  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,128  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,128  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,128  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,129  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,129  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_unparted has new partitions [{b=1}].
2024-04-24T10:53:22,131  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_unparted[1]	
2024-04-24T10:53:22,137  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:53:22,137  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:53:22,145  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_unparted
2024-04-24T10:53:22,146  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 265
2024-04-24T10:53:22,185  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:53:22,218  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,218  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,218  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,218  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,218  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,219  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,219  INFO [Thread-995] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:22,219  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:22,221  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:53:22,377  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1197125904_0018
2024-04-24T10:53:22,377  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A
2024-04-24T10:53:22,377  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1] C:  R: 
2024-04-24T10:53:22,385  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,392  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,398  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,424  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:53:22,424  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:53:21	2024-04-24 10:53:22	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1197125904_0018	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A	MAP_ONLY	default.junit_unparted,

Input(s):
Successfully read 11 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/input.data"

Output(s):
Successfully stored 11 records in: "default.junit_unparted"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1197125904_0018


2024-04-24T10:53:22,426  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,429  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,430  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,453  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,454  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:22,456  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:22,465  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,480  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,481  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:22,483  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:22,493  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:22,495  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-24T10:53:22,502  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:53:22,503  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:53:22,503  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:53:22,504  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-24T10:53:22,504  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:53:22,509  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,511  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:53:22,511  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:22,523  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:22,524  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:22,525  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:53:22,525  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:22,528  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:22,537  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:22,538  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_by_filter : tbl=hive.default.junit_unparted	
2024-04-24T10:53:22,578  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:53:22,583  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:53:22,584  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,592  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:53:22,597  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:53:22,599  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, test.data.files=${hive.root}/data/files, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-04-24T10:53:22,603  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:22,604  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:53:22,622  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:53:22,636  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local592230226_0019
2024-04-24T10:53:22,636  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:53:22,696  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:53:22,696  INFO [Thread-1037] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:53:22,700  INFO [Thread-1037] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,700  INFO [Thread-1037] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,700  INFO [Thread-1037] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:53:22,708  INFO [Thread-1037] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:53:22,709  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local592230226_0019_m_000000_0
2024-04-24T10:53:22,712  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,712  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,714  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:53:22,714  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 265
Input split[0]:
   Length = 265
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-04-24T10:53:22,717  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@6bcec765
2024-04-24T10:53:22,718  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:22,718  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:22,730  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.avro.AvroSerDe with properties {name=default.junit_unparted, columns.types=int, serialization.format=1, columns=a, columns.comments=, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.avro.AvroSerDe, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713981201}
2024-04-24T10:53:22,730  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:53:22,730  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:53:22,730  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: B[3,4] C:  R: 
2024-04-24T10:53:22,730  WARN [LocalJobRunner Map Task Executor #0] avro.AvroDeserializer: Received different schemas.  Have to re-encode: {"type":"record","name":"junit_unparted","fields":[{"name":"a","type":["null","int"],"default":null}]}
SIZE{-4b133707:18f113d6bc7:-7fed=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@557474cc} ID -4b133707:18f113d6bc7:-7fed
2024-04-24T10:53:22,731  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:22,732  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local592230226_0019_m_000000_0 is done. And is in the process of committing
2024-04-24T10:53:22,733  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:22,733  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local592230226_0019_m_000000_0 is allowed to commit now
2024-04-24T10:53:22,735  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local592230226_0019_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713981190897/pig/temp/temp1508162590/tmp-355179338
2024-04-24T10:53:22,736  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:53:22,736  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local592230226_0019_m_000000_0' done.
2024-04-24T10:53:22,736  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local592230226_0019_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=65145
		FILE: Number of bytes written=10847807
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=2102
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1038614528
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:53:22,736  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local592230226_0019_m_000000_0
2024-04-24T10:53:22,736  INFO [Thread-1037] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:53:22,897  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local592230226_0019
2024-04-24T10:53:22,897  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases B
2024-04-24T10:53:22,897  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: B[3,4] C:  R: 
2024-04-24T10:53:22,904  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,911  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,918  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,924  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:53:22,925  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:53:22	2024-04-24 10:53:22	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local592230226_0019	1	0	n/a	n/a	n/a	n/a	0	0	0	0	B	MAP_ONLY	file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713981190897/pig/temp/temp1508162590/tmp-355179338,

Input(s):
Successfully read 11 records from: "default.junit_unparted"

Output(s):
Successfully stored 11 records in: "file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713981190897/pig/temp/temp1508162590/tmp-355179338"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local592230226_0019


2024-04-24T10:53:22,930  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,937  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:22,941  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:53:22,943  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:53:22,964  INFO [main] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:22,964  INFO [main] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:53:22,967  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:53:22,993  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testWriteChar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.157">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:23,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:23,041  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:23,041  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:23,042  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@598ad90e will be shutdown
2024-04-24T10:53:23,042  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1b1f7781 created in the thread with id: 1
2024-04-24T10:53:23,044  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = df47a7cd-dec1-4730-b509-7c6b35d91f0c
2024-04-24T10:53:23,044  INFO [main] SessionState: Hive Session ID = df47a7cd-dec1-4730-b509-7c6b35d91f0c
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:23,044  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:23,050  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/df47a7cd-dec1-4730-b509-7c6b35d91f0c
2024-04-24T10:53:23,052  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/df47a7cd-dec1-4730-b509-7c6b35d91f0c
2024-04-24T10:53:23,054  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/df47a7cd-dec1-4730-b509-7c6b35d91f0c/_tmp_space.db
2024-04-24T10:53:23,055  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): drop table if exists junitTypeTest1
2024-04-24T10:53:23,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:23,065  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:23,065  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:23,065  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:23,065  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:23,065  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:53:23,065  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.01 seconds
2024-04-24T10:53:23,066  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:23,066  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:23,066  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): drop table if exists junitTypeTest1
2024-04-24T10:53:23,066  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:23,066  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:23,076  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:23,076  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:23,087  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:23,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:23,125  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:23,125  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10, dropTable_(String, String, boolean, boolean, boolean)=49}
2024-04-24T10:53:23,125  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.059 seconds
2024-04-24T10:53:23,126  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): use default
2024-04-24T10:53:23,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:23,128  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:23,128  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:23,128  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:23,128  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:53:23,128  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.002 seconds
2024-04-24T10:53:23,128  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:23,128  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:23,128  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): use default
2024-04-24T10:53:23,129  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:23,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:23,130  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:23,132  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:23,132  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:23,132  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.003 seconds
2024-04-24T10:53:23,132  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:23,133  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606
2024-04-24T10:53:23,133  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:23,133  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=df47a7cd-dec1-4730-b509-7c6b35d91f0c, clientType=HIVECLI]
2024-04-24T10:53:23,133  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:23,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:23,134  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19ac267e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1b1f7781 will be shutdown
2024-04-24T10:53:23,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:23,134  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-24T10:53:23,135  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:23,136  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:23,136  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:23,136  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf, with PersistenceManager: null will be shutdown
2024-04-24T10:53:23,136  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4ed5c276 created in the thread with id: 1
2024-04-24T10:53:23,138  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf from thread id: 1
2024-04-24T10:53:23,138  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:23,138  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:23,138  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:53:23,139  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:23,139  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:23,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4ed5c276 will be shutdown
2024-04-24T10:53:23,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eeb14d4 created in the thread with id: 1
2024-04-24T10:53:23,142  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:23,142  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:23,142  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:23,143  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606
2024-04-24T10:53:23,143  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:23,143  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:23,143  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:23,143  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:23,143  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.011 seconds
2024-04-24T10:53:23,143  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:23,144  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:23,144  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:23,144  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:23,144  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:23,144  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:23,144  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36823faf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eeb14d4 will be shutdown
2024-04-24T10:53:23,144  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:23,145  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-24T10:53:23,155 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:23,156 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:23,158  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:53:23,158  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:53:23,158  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:23,159 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:23,159  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:23,159  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:23,159  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105323_eb5553b4-dbe2-41d4-9b34-8de530134606); Time taken: 0.015 seconds
]]></system-err>
  </testcase>
  <testcase name="testWriteDate" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="6.357"/>
  <testcase name="testDynamicPartitioningMultiPartColsNoDataInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.876"/>
  <testcase name="testWriteVarchar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.125">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:30,425  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:30,427  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:30,427  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:30,427  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f6a30d2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@664ff641 will be shutdown
2024-04-24T10:53:30,427  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f6a30d2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5fa380d9 created in the thread with id: 1
2024-04-24T10:53:30,429  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 32a93f4f-544c-42a6-8cab-0a154b8a0ac6
2024-04-24T10:53:30,429  INFO [main] SessionState: Hive Session ID = 32a93f4f-544c-42a6-8cab-0a154b8a0ac6
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:30,429  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:30,434  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/32a93f4f-544c-42a6-8cab-0a154b8a0ac6
2024-04-24T10:53:30,437  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/32a93f4f-544c-42a6-8cab-0a154b8a0ac6
2024-04-24T10:53:30,439  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/32a93f4f-544c-42a6-8cab-0a154b8a0ac6/_tmp_space.db
2024-04-24T10:53:30,439  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): drop table if exists junitTypeTest1
2024-04-24T10:53:30,440  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:30,448  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:30,448  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:30,448  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8, flushCache_()=0}
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.009 seconds
2024-04-24T10:53:30,448  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): drop table if exists junitTypeTest1
2024-04-24T10:53:30,448  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:30,449  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:30,457  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:30,457  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:30,466  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:30,466  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:30,498  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:30,499  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=41, getTable_(GetTableRequest)=8, isCompatibleWith_(Configuration)=1}
2024-04-24T10:53:30,499  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.05 seconds
2024-04-24T10:53:30,499  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): use default
2024-04-24T10:53:30,499  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:30,501  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:30,501  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:30,501  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:30,501  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:30,501  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.002 seconds
2024-04-24T10:53:30,501  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:30,501  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:30,501  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): use default
2024-04-24T10:53:30,502  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:30,502  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:30,503  INFO [communication thread] mapred.LocalJobRunner: map > map
2024-04-24T10:53:30,504  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:30,505  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:30,505  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:53:30,505  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.004 seconds
2024-04-24T10:53:30,506  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:30,507  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159
2024-04-24T10:53:30,507  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:30,507  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=32a93f4f-544c-42a6-8cab-0a154b8a0ac6, clientType=HIVECLI]
2024-04-24T10:53:30,507  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:30,507  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:30,507  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f6a30d2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5fa380d9 will be shutdown
2024-04-24T10:53:30,507  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:30,507  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-24T10:53:30,508  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:30,509  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:30,509  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:30,509  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: null will be shutdown
2024-04-24T10:53:30,510  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@17bf4515 created in the thread with id: 1
2024-04-24T10:53:30,511  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31 from thread id: 1
2024-04-24T10:53:30,511  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:30,511  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:30,511  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:53:30,513  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:30,513  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:30,513  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@17bf4515 will be shutdown
2024-04-24T10:53:30,513  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@e36e90b created in the thread with id: 1
2024-04-24T10:53:30,515  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:30,515  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:30,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:30,516  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159
2024-04-24T10:53:30,516  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:30,516  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:30,516  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:30,516  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:30,516  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.01 seconds
2024-04-24T10:53:30,516  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:30,517  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:30,517  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:30,517  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:30,517  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:30,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:30,517  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@e36e90b will be shutdown
2024-04-24T10:53:30,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:30,517  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-24T10:53:30,518 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:30,518 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:30,519  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:53:30,519  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:53:30,519  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:30,519 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:30,519  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:30,519  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:30,519  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105330_654d7327-7fb6-40bf-9ec8-cd63fff23159); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testBagNStruct" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.19"/>
  <testcase name="testStaticPartitioningMultiPartCols" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.086"/>
  <testcase name="testPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.344"/>
  <testcase name="testStoreWithNoCtorArgs" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.216"/>
  <testcase name="testWriteDecimalX" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.075">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:53:35,388  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:35,388  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:35,389  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:35,390  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:35,390  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:35,391  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6d645c87 will be shutdown
2024-04-24T10:53:35,391  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b6c92d1 created in the thread with id: 1
2024-04-24T10:53:35,392  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = de3eaf1e-5fbf-4d0f-a48e-0fac66d02633
2024-04-24T10:53:35,393  INFO [main] SessionState: Hive Session ID = de3eaf1e-5fbf-4d0f-a48e-0fac66d02633
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:35,393  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:35,398  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/de3eaf1e-5fbf-4d0f-a48e-0fac66d02633
2024-04-24T10:53:35,401  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/de3eaf1e-5fbf-4d0f-a48e-0fac66d02633
2024-04-24T10:53:35,403  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/de3eaf1e-5fbf-4d0f-a48e-0fac66d02633/_tmp_space.db
2024-04-24T10:53:35,404  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): drop table if exists junitTypeTest1
2024-04-24T10:53:35,405  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:35,407  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:35,407  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.003 seconds
2024-04-24T10:53:35,407  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): drop table if exists junitTypeTest1
2024-04-24T10:53:35,407  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:35,408  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:35,409  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:35,409  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:35,410  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.002 seconds
2024-04-24T10:53:35,410  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): use default
2024-04-24T10:53:35,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:35,411  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:35,411  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:35,411  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:35,411  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:35,411  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.001 seconds
2024-04-24T10:53:35,412  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:35,412  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:35,412  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): use default
2024-04-24T10:53:35,412  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:35,412  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:35,413  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:35,414  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:35,414  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:35,414  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.002 seconds
2024-04-24T10:53:35,414  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:35,415  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d
2024-04-24T10:53:35,415  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:35,416  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=de3eaf1e-5fbf-4d0f-a48e-0fac66d02633, clientType=HIVECLI]
2024-04-24T10:53:35,416  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:35,416  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:35,416  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b6c92d1 will be shutdown
2024-04-24T10:53:35,416  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:35,416  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -37
2024-04-24T10:53:35,418  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:35,418  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:35,419  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:35,419  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: null will be shutdown
2024-04-24T10:53:35,419  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5189077a created in the thread with id: 1
2024-04-24T10:53:35,421  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77 from thread id: 1
2024-04-24T10:53:35,421  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:35,421  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:35,421  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:53:35,423  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:35,423  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:35,423  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5189077a will be shutdown
2024-04-24T10:53:35,424  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@26fcf280 created in the thread with id: 1
2024-04-24T10:53:35,425  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:35,425  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:35,425  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:35,426  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d
2024-04-24T10:53:35,426  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:35,426  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:35,427  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:35,427  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:35,427  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.013 seconds
2024-04-24T10:53:35,427  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:35,427  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:35,427  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:35,427  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:35,427  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:35,427  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:35,427  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@26fcf280 will be shutdown
2024-04-24T10:53:35,428  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:35,428  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -38
2024-04-24T10:53:35,428 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:35,429 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:35,429  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:53:35,430  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:53:35,430  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:35,430 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:35,430  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:35,430  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:35,430  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105335_6fd50c96-fb03-4000-8663-18c779d0bd6d); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testPartitionPublish" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.606"/>
  <testcase name="testWriteDecimal" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.07">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,067  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,068  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,068  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,068  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,068  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,069  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,069  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,069  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@217b0f0e will be shutdown
2024-04-24T10:53:36,070  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa9893 created in the thread with id: 1
2024-04-24T10:53:36,071  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 5b793c52-7432-4aa4-8d6c-30421da4ea44
2024-04-24T10:53:36,071  INFO [main] SessionState: Hive Session ID = 5b793c52-7432-4aa4-8d6c-30421da4ea44
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:36,071  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:36,077  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/5b793c52-7432-4aa4-8d6c-30421da4ea44
2024-04-24T10:53:36,079  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/5b793c52-7432-4aa4-8d6c-30421da4ea44
2024-04-24T10:53:36,081  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/5b793c52-7432-4aa4-8d6c-30421da4ea44/_tmp_space.db
2024-04-24T10:53:36,082  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): drop table if exists junitTypeTest1
2024-04-24T10:53:36,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:36,084  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,084  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,084  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,084  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,085  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.002 seconds
2024-04-24T10:53:36,085  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,085  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,085  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): drop table if exists junitTypeTest1
2024-04-24T10:53:36,085  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:53:36,087  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,087  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,087  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.002 seconds
2024-04-24T10:53:36,088  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): use default
2024-04-24T10:53:36,088  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,089  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,089  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.001 seconds
2024-04-24T10:53:36,089  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): use default
2024-04-24T10:53:36,089  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,089  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,090  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,091  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,091  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:53:36,091  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.002 seconds
2024-04-24T10:53:36,091  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:36,092  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06
2024-04-24T10:53:36,092  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:36,092  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5b793c52-7432-4aa4-8d6c-30421da4ea44, clientType=HIVECLI]
2024-04-24T10:53:36,092  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:36,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:36,092  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa9893 will be shutdown
2024-04-24T10:53:36,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,092  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -42
2024-04-24T10:53:36,093  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,094  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,094  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,094  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,095  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4440a153 created in the thread with id: 1
2024-04-24T10:53:36,097  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790 from thread id: 1
2024-04-24T10:53:36,097  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,097  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,097  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:53:36,098  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,098  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,099  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4440a153 will be shutdown
2024-04-24T10:53:36,099  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f415ae0 created in the thread with id: 1
2024-04-24T10:53:36,101  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,101  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,101  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,103  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,103  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,103  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.012 seconds
2024-04-24T10:53:36,103  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:36,103  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,104  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:36,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:36,104  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f415ae0 will be shutdown
2024-04-24T10:53:36,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,104  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -43
2024-04-24T10:53:36,104 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:36,105 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:36,106  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:53:36,106  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:53:36,106  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:36,106 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:53:36,106  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,106  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:36,106  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_71957b29-9568-487b-8fdd-aa7b91e47a06); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testMultiPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.763">
    <failure message="expected:&lt;1&gt; but was:&lt;5&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<5>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,138  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,140  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,140  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,140  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,141  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,141  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c44dccf created in the thread with id: 1
2024-04-24T10:53:36,143  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2 from thread id: 1
2024-04-24T10:53:36,143  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 0667c7d3-9bc5-4574-860a-79db4b302eea
2024-04-24T10:53:36,143  INFO [main] SessionState: Hive Session ID = 0667c7d3-9bc5-4574-860a-79db4b302eea
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:36,143  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:53:36,149  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/0667c7d3-9bc5-4574-860a-79db4b302eea
2024-04-24T10:53:36,151  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/0667c7d3-9bc5-4574-860a-79db4b302eea
2024-04-24T10:53:36,153  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/0667c7d3-9bc5-4574-860a-79db4b302eea/_tmp_space.db
2024-04-24T10:53:36,154  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): drop table if exists employee
2024-04-24T10:53:36,155  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,155  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,156  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c44dccf will be shutdown
2024-04-24T10:53:36,156  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3644e451 created in the thread with id: 1
2024-04-24T10:53:36,158  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,158  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,158  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,160  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,160  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,160  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,160  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,160  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.006 seconds
2024-04-24T10:53:36,161  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,161  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,161  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): drop table if exists employee
2024-04-24T10:53:36,161  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,161  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,163  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,163  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,163  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.002 seconds
2024-04-24T10:53:36,164  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): use default
2024-04-24T10:53:36,164  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,165  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,165  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,165  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,165  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:53:36,165  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.001 seconds
2024-04-24T10:53:36,165  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,165  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,165  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): use default
2024-04-24T10:53:36,166  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,166  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,167  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,167  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,168  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:36,168  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.002 seconds
2024-04-24T10:53:36,168  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:36,169  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9
2024-04-24T10:53:36,169  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:36,169  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0667c7d3-9bc5-4574-860a-79db4b302eea, clientType=HIVECLI]
2024-04-24T10:53:36,169  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:53:36,169  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:36,169  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3644e451 will be shutdown
2024-04-24T10:53:36,169  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,169  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-04-24T10:53:36,170  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,171  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,171  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,171  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,171  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b4937c6 created in the thread with id: 1
2024-04-24T10:53:36,173  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c from thread id: 1
2024-04-24T10:53:36,173  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,173  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,173  INFO [main] parse.CalcitePlanner: Creating table default.employee position=13
2024-04-24T10:53:36,174  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,174  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,174  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b4937c6 will be shutdown
2024-04-24T10:53:36,174  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@79b7d980 created in the thread with id: 1
2024-04-24T10:53:36,176  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,176  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,176  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:53:36,177  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:53:36,177  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:36,177  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.009 seconds
2024-04-24T10:53:36,177  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:53:36,177  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:53:36,177  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:53:36,177  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:36,177  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@79b7d980 will be shutdown
2024-04-24T10:53:36,177  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,177  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-04-24T10:53:36,179  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,180  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,180  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,180  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,180  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa7cc82 created in the thread with id: 1
2024-04-24T10:53:36,182  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8 from thread id: 1
2024-04-24T10:53:36,182  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,182  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,182  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:employee, dbName:default, owner:alex, createTime:1713981216, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:emp_id, type:int, comment:null), FieldSchema(name:emp_name, type:string, comment:null), FieldSchema(name:emp_start_date, type:string, comment:null), FieldSchema(name:emp_gender, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:emp_country, type:string, comment:null), FieldSchema(name:emp_state, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:53:36,185  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/employee
2024-04-24T10:53:36,205  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:36,205  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=23}
2024-04-24T10:53:36,205  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.028 seconds
2024-04-24T10:53:36,223  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:53:36,224  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-595886ed-7a92-48c2-8ab1-ed5f04088694
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,309  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,309  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:36,310  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:53:36,311  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,313  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,323  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,343  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,344  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,344  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,360  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,361  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,361  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:36,362  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:53:36,363  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,365  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,373  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,393  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,393  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,408  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,409  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:36,409  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:53:36,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,412  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,421  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,440  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,441  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,441  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,456  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,457  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:53:36,457  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.stats.column.autogather=true, javax.jdo.option.ConnectionUserName=APP, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:53:36,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,460  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:36,471  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,497  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,498  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,501  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:53:36,508  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:53:36,508  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:53:36,517  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:53:36,520  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 5
2024-04-24T10:53:36,521  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 map-only splittees.
2024-04-24T10:53:36,521  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 out of total 5 MR operators.
2024-04-24T10:53:36,521  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:53:36,527  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:36,529  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:53:36,530  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:53:36,531  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:53:36,532  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:53:36,532  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:53:36,532  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713981216532-0
2024-04-24T10:53:36,543  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:53:36,545  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,562  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,562  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:36,563  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,563  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:36,563  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ebd65a, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,563  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ebd65a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f2e445d created in the thread with id: 1970
2024-04-24T10:53:36,565  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ebd65a from thread id: 1970
2024-04-24T10:53:36,565  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:36,565  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:36,565  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:36,565  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ebd65a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f2e445d will be shutdown
2024-04-24T10:53:36,565  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,565  INFO [JobControl] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-04-24T10:53:36,566  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,567  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:36,567  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4af77452, with PersistenceManager: null will be shutdown
2024-04-24T10:53:36,567  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4af77452, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3f4575ba created in the thread with id: 1970
2024-04-24T10:53:36,568  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4af77452 from thread id: 1970
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,585  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,586  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,586  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,604  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,604  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,604  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,605  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,605  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,605  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:36,622  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:36,623  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:36,623  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:36,623  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:36,623  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:36,623  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:36,637  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:53:36,641  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:53:36,644  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:53:36,644  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:36,644  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:53:36,644  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:53:36,664  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:53:36,677  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1073063223_0037
2024-04-24T10:53:36,678  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:53:36,753  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,753  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-04-24T10:53:36,753  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,753  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-04-24T10:53:36,753  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:36,753  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-04-24T10:53:36,773  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:53:36,773  INFO [Thread-1844] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:53:36,779  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,779  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,782  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,782  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,786  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,786  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,789  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,789  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,789  INFO [Thread-1844] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:53:36,791  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,791  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,804  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,804  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,818  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,818  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,832  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,832  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,847  INFO [Thread-1844] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:53:36,847  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1073063223_0037_m_000000_0
2024-04-24T10:53:36,854  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,854  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,860  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,860  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,865  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,865  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,869  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,869  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,872  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,872  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,874  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,874  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,876  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,876  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,878  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,878  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,878  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:53:36,878  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 132
Input split[0]:
   Length = 132
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:53:36,880  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:53:36,880  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/input.data:0+132
2024-04-24T10:53:36,881  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:53:36,882  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-24T10:53:36,884  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-24T10:53:36,888  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,888  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,895  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,895  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,900  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,900  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,905  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,906  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,945  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:36,946  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1073063223_0037_m_000000_0 is done. And is in the process of committing
2024-04-24T10:53:36,947  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,947  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,951  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:53:36,951  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1073063223_0037_m_000000_0 is allowed to commit now
2024-04-24T10:53:36,952  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,952  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,953  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1073063223_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/employee/_SCRATCH0,35664538134866786/emp_country=IN/emp_state=TN
2024-04-24T10:53:36,954  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,954  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,956  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1073063223_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/employee/_SCRATCH0,23590908583449588/emp_country=IN/emp_state=KA
2024-04-24T10:53:36,957  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,957  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,958  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1073063223_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/employee/_SCRATCH0,7043701187116093/emp_country=IN/emp_state=KL
2024-04-24T10:53:36,959  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:53:36,959  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:53:36,960  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1073063223_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/warehouse/employee/_SCRATCH0,6492311825510149/emp_country=IN/emp_state=AP
2024-04-24T10:53:36,961  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:53:36,961  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1073063223_0037_m_000000_0' done.
2024-04-24T10:53:36,961  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1073063223_0037_m_000000_0: Counters: 19
	File System Counters
		FILE: Number of bytes read=107859
		FILE: Number of bytes written=21158261
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=0
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=983040000
	MultiStoreCounters
		Output records in _0_employee=1
		Output records in _1_employee=1
		Output records in _2_employee=1
		Output records in _3_employee=1
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:53:36,961  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1073063223_0037_m_000000_0
2024-04-24T10:53:36,961  INFO [Thread-1844] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:53:37,003  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,004  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,004  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,005  INFO [Thread-1844] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:53:37,006  INFO [Thread-1844] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:37,006  INFO [Thread-1844] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:53:37,006  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2af29cc, with PersistenceManager: null will be shutdown
2024-04-24T10:53:37,006  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2af29cc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19a66810 created in the thread with id: 1990
2024-04-24T10:53:37,007  INFO [Thread-1844] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2af29cc from thread id: 1990
2024-04-24T10:53:37,007  INFO [Thread-1844] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:53:37,008  INFO [Thread-1844] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:53:37,008  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:53:37,008  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2af29cc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19a66810 will be shutdown
2024-04-24T10:53:37,008  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:53:37,008  INFO [Thread-1844] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-04-24T10:53:37,008  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,010  INFO [Thread-1844] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:53:37,010  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ce478b8, with PersistenceManager: null will be shutdown
2024-04-24T10:53:37,010  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ce478b8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@c8277ce created in the thread with id: 1990
2024-04-24T10:53:37,011  INFO [Thread-1844] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ce478b8 from thread id: 1990
2024-04-24T10:53:37,044  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1073063223_0037
2024-04-24T10:53:37,044  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,AP,KA,KL,TN
2024-04-24T10:53:37,044  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-24T10:53:37,045  INFO [main] mapReduceLayer.MapReduceLauncher: 50% complete
2024-04-24T10:53:37,045  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1073063223_0037]
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,054  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,055  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_state=TN, emp_country=IN}].
2024-04-24T10:53:37,056  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,TN]	
2024-04-24T10:53:37,062  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:53:37,062  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:53:37,071  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:53:37,071  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 458
2024-04-24T10:53:37,102  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,135  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,136  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,136  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,137  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,183  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,184  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,184  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,184  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,184  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,184  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,184  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,184  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,194  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KA}].
2024-04-24T10:53:37,195  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KA]	
2024-04-24T10:53:37,199  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:53:37,199  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:53:37,205  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:53:37,206  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 458
2024-04-24T10:53:37,235  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,270  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,271  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,271  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,272  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,314  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,314  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,315  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,323  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_state=KL, emp_country=IN}].
2024-04-24T10:53:37,325  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KL]	
2024-04-24T10:53:37,328  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:53:37,328  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:53:37,335  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:53:37,336  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 456
2024-04-24T10:53:37,361  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,394  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,394  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,394  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,396  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,434  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,435  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,435  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,444  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=AP}].
2024-04-24T10:53:37,445  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,AP]	
2024-04-24T10:53:37,448  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:53:37,448  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:53:37,454  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:53:37,455  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 456
2024-04-24T10:53:37,479  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:53:37,512  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:53:37,512  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:53:37,513  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:53:37,513  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:53:37,513  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:53:37,515  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:53:37,677  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:37,679  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:37,681  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:37,697  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:53:37,697  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:37,697  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:53:37,697  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:37,698  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:53:37,698  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:37,698  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:53:37,698  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:53:37,699  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:53:37,699  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:53:36	2024-04-24 10:53:37	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1073063223_0037	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,AP,KA,KL,TN	MULTI_QUERY,MAP_ONLY	employee,employee,employee,employee,

Input(s):
Successfully read 4 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713981175622/input.data"

Output(s):
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"

Counters:
Total records written : 4
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1073063223_0037


2024-04-24T10:53:37,702  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:37,703  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:53:37,704  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:53:37,706  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): select * from employee
2024-04-24T10:53:37,706  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9
2024-04-24T10:53:37,706  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:53:37,706  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-24T10:53:37,706  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T10:53:37,707  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:37,715  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:53:37,715  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T10:53:37,715  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T10:53:37,716  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-24T10:53:37,726  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.employee	
2024-04-24T10:53:37,734  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.employee, projIndxSet: [0, 1, 2, 3, 4, 5], allowMissingStats: true
2024-04-24T10:53:37,734  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.employee	
2024-04-24T10:53:37,764  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.employee	
2024-04-24T10:53:37,779  WARN [main] calcite.RelOptHiveTable: No Stats for default@employee, Columns: emp_name, emp_start_date, emp_id, emp_gender
No Stats for default@employee, Columns: emp_name, emp_start_date, emp_id, emp_gender
2024-04-24T10:53:37,779  INFO [main] SessionState: No Stats for default@employee, Columns: emp_name, emp_start_date, emp_id, emp_gender
2024-04-24T10:53:37,821  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T10:53:37,821  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T10:53:37,821  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T10:53:37,823  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/0667c7d3-9bc5-4574-860a-79db4b302eea/hive_2024-04-24_10-53-37_706_8966781423140636495-1/-mr-10001/.hive-staging_hive_2024-04-24_10-53-37_706_8966781423140636495-1
2024-04-24T10:53:37,831  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-24T10:53:37,832  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-24T10:53:37,833  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-24T10:53:37,834  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9
2024-04-24T10:53:37,834  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:53:37,834  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:employee.emp_id, type:int, comment:null), FieldSchema(name:employee.emp_name, type:string, comment:null), FieldSchema(name:employee.emp_start_date, type:string, comment:null), FieldSchema(name:employee.emp_gender, type:string, comment:null), FieldSchema(name:employee.emp_country, type:string, comment:null), FieldSchema(name:employee.emp_state, type:string, comment:null)], properties:null)
2024-04-24T10:53:37,835  INFO [main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-04-24T10:53:37,835  INFO [main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-04-24T10:53:37,835  INFO [main] exec.SelectOperator: SELECT struct<emp_id:int,emp_name:string,emp_start_date:string,emp_gender:string,emp_country:string,emp_state:string>
2024-04-24T10:53:37,835  INFO [main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-04-24T10:53:37,836  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:53:37,836  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {listPartitions_(String, String, short)=29, getAllTableConstraints_(AllTableConstraintsRequest)=4, flushCache_()=0, getTable_(GetTableRequest)=9, getAggrColStatsFor_(String, String, List, List, String, String)=14, isCompatibleWith_(Configuration)=0}
2024-04-24T10:53:37,836  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.13 seconds
2024-04-24T10:53:37,836  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:53:37,836  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:53:37,836  INFO [main] ql.Driver: Executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9): select * from employee
2024-04-24T10:53:37,836  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:53:37,836  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:53:37,836  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424105336_b312ac83-4e1f-4afe-a2c3-3322330b6fa9); Time taken: 0.0 seconds
2024-04-24T10:53:37,840  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:37,841  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:53:37,846  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:37,846  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:53:37,846  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:53:37,851  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:37,852  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:53:37,852  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:53:37,857  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:53:37,857  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:53:37,857  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:53:37,859  INFO [main] exec.TableScanOperator: Closing Operator: TS[0]
2024-04-24T10:53:37,859  INFO [main] exec.TableScanOperator: RECORDS_OUT_OPERATOR_TS_0:4, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T10:53:37,859  INFO [main] exec.SelectOperator: Closing Operator: SEL[1]
2024-04-24T10:53:37,859  INFO [main] exec.SelectOperator: RECORDS_OUT_OPERATOR_SEL_1:4, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T10:53:37,859  INFO [main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-04-24T10:53:37,859  INFO [main] exec.ListSinkOperator: RECORDS_OUT_OPERATOR_LIST_SINK_3:4, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T10:53:37,860  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:53:37,869  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testStaticPartitioningMultiPartColsNoData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.315"/>
  <testcase name="testStoreMultiTables" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.457"/>
  <testcase name="testStoreWithNoSchema" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.878"/>
  <testcase name="testNoAlias" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.284"/>
  <testcase name="testStoreFuncSimple" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.817"/>
</testsuite>