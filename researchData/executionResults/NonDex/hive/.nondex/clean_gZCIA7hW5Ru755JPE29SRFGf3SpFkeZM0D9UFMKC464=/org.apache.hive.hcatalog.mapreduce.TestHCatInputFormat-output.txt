SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,115447 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@7f31245a
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@54d9d12d]...
DEBUG StatusLogger Reconfiguration started for context[name=7f31245a] at URI null (org.apache.logging.log4j.core.LoggerContext@54d9d12d) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@4b2bac3f
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@13b6aecc
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,024902 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/core/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/core/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", modulate="true", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(max="30", min="null", fileIndex="null", compressionLevel="null", ={}, stopCustomActionsOnError="null", tempCompressedFilePattern="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileName="/home/alex/Repositories/hive/hcatalog/core/target/tmp/log/hive.log", filePattern="/home/alex/Repositories/hive/hcatalog/core/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertise="null", advertiseURI="null", filePermissions="null", fileOwner="null", fileGroup="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/core/target/tmp/log/hive.log seek to 3738807
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/core/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T08:24:11.132-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-08:24:12.771, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-08:24:12.772, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@13b6aecc initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@13b6aecc
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@13b6aecc OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@29f69090...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@29f69090 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@79dc5318
TRACE StatusLogger Reregistering context (1/1): '7f31245a' org.apache.logging.log4j.core.LoggerContext@54d9d12d
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=7f31245a] at URI /home/alex/Repositories/hive/hcatalog/core/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@54d9d12d) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@54d9d12d] started OK.
2024-04-24T08:24:12,971  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-24T08:24:13,009  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/hcatalog/core/target/testconf/hive-site.xml
2024-04-24T08:24:13,366  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T08:24:13,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:24:13,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:24:13,436  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:24:13,437  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:24:13,437  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:24:13,437  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:24:13,438  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:24:13,438  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:24:13,438  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:24:13,439  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:24:13,439  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:24:13,676  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:13,861  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:24:13,900  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:13,909  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T08:24:13,909  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T08:24:13,940  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:24:13,945  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T08:24:14,656  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:24:14,661  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T08:24:15,309  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T08:24:15,309  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486, with PersistenceManager: null will be shutdown
2024-04-24T08:24:15,333  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@27ace0b1 created in the thread with id: 1
2024-04-24T08:24:17,784  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T08:24:17,784  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T08:24:17,784  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486 from thread id: 1
2024-04-24T08:24:18,044  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T08:24:18,076  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T08:24:18,106  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T08:24:18,107  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T08:24:18,229  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T08:24:18,238  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T08:24:18,239  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T08:24:18,243  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T08:24:18,278  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:24:18,281  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T08:24:18,283  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:24:18,284  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T08:24:18,285  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:24:18,288  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T08:24:18,290  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:24:18,290  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T08:24:18,295  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T08:24:18,296  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T08:24:18,297  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T08:24:18,300  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = a07b5c93-bf3a-43af-86ff-beb3fde9da32
2024-04-24T08:24:18,454  INFO [main] SessionState: Hive Session ID = a07b5c93-bf3a-43af-86ff-beb3fde9da32
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:24:18,467  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:24:18,553  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/a07b5c93-bf3a-43af-86ff-beb3fde9da32
2024-04-24T08:24:18,557  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/a07b5c93-bf3a-43af-86ff-beb3fde9da32
2024-04-24T08:24:18,561  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/a07b5c93-bf3a-43af-86ff-beb3fde9da32/_tmp_space.db
2024-04-24T08:24:18,563  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data/intString.seq
2024-04-24T08:24:18,605  INFO [main] compress.CodecPool: Got brand-new compressor [.deflate]
2024-04-24T08:24:18,681  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): drop table if exists test_bad_records
2024-04-24T08:24:20,001  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,006  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,010  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,011  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,011  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,012  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,013  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:24:20,075  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:20,076  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:20,077  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@27ace0b1 will be shutdown
2024-04-24T08:24:20,078  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3d3c886f created in the thread with id: 1
2024-04-24T08:24:20,092  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:20,093  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:20,116  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T08:24:20,362  INFO [main] reflections.Reflections: Reflections took 162 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:24:20,484  INFO [main] reflections.Reflections: Reflections took 82 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:24:20,567  INFO [main] reflections.Reflections: Reflections took 77 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:24:20,578  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:20,681  INFO [main] reflections.Reflections: Reflections took 76 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:24:20,751  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:20,753  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:20,758  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:20,758  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, flushCache_()=0, getAllFunctions_()=47}
2024-04-24T08:24:20,758  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 2.08 seconds
2024-04-24T08:24:20,759  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:20,760  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:20,763  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): drop table if exists test_bad_records
2024-04-24T08:24:20,767  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:24:20,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:20,788  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:20,788  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-24T08:24:20,788  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 0.025 seconds
2024-04-24T08:24:20,789  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:24:20,881  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308
2024-04-24T08:24:20,884  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:24:20,910  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a07b5c93-bf3a-43af-86ff-beb3fde9da32, clientType=HIVECLI]
2024-04-24T08:24:20,912  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T08:24:20,913  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:24:20,913  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7c853486, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3d3c886f will be shutdown
2024-04-24T08:24:20,914  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:20,914  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T08:24:20,915  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:20,916  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:24:20,916  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:20,917  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c, with PersistenceManager: null will be shutdown
2024-04-24T08:24:20,918  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1796b2d4 created in the thread with id: 1
2024-04-24T08:24:20,927  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c from thread id: 1
2024-04-24T08:24:20,927  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:20,928  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:20,937  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T08:24:20,943  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:20,944  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:20,944  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1796b2d4 will be shutdown
2024-04-24T08:24:20,945  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@611c3eae created in the thread with id: 1
2024-04-24T08:24:20,949  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:20,950  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:20,952  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:24:20,964  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308
2024-04-24T08:24:20,964  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:20,965  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:20,965  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:20,965  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:24:20,966  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 0.176 seconds
2024-04-24T08:24:20,966  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:20,967  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:20,967  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:24:20,967  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:24:20,967  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T08:24:20,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:24:20,968  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bf570c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@611c3eae will be shutdown
2024-04-24T08:24:20,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:20,968  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-24T08:24:21,080  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:21,080  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:24:21,080  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:21,081  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: null will be shutdown
2024-04-24T08:24:21,081  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@59262a90 created in the thread with id: 1
2024-04-24T08:24:21,085  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439 from thread id: 1
2024-04-24T08:24:21,086  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:21,086  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:21,086  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713972260, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:myint, type:int, comment:from deserializer), FieldSchema(name:mystring, type:string, comment:from deserializer), FieldSchema(name:underscore_int, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numRows=0, rawDataSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, numFiles=0, bucketing_version=2, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:24:21,094  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/warehouse/test_bad_records
2024-04-24T08:24:21,220  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:21,220  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=134}
2024-04-24T08:24:21,220  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 0.253 seconds
2024-04-24T08:24:21,221  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data' into table test_bad_records
2024-04-24T08:24:21,226  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308
2024-04-24T08:24:21,232  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,316  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,356  INFO [main] compress.CodecPool: Got brand-new decompressor [.deflate]
2024-04-24T08:24:21,365  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308
2024-04-24T08:24:21,365  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:21,365  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:21,366  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:21,366  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=86, flushCache_()=0}
2024-04-24T08:24:21,366  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 0.145 seconds
2024-04-24T08:24:21,366  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:21,366  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:21,366  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data' into table test_bad_records
2024-04-24T08:24:21,366  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T08:24:21,368  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data
2024-04-24T08:24:21,368  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,378  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,392  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,411  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:24:21,411  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:24:21,453  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T08:24:21,453  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,463  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,463  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T08:24:21,463  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,472  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,475  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:24:21,475  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:24:21,514  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T08:24:21,514  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:21,514  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=39, alter_table_(String, String, String, Table, EnvironmentContext, String)=77}
2024-04-24T08:24:21,514  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082413_5fbaed41-2960-40a9-8b80-7a19384d5308); Time taken: 0.148 seconds
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:24:21,569  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:24:21,570  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:24:21,570  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:24:21,570  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:24:21,570  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:24:21,570  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:24:21,571  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T08:24:21,571  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T08:24:21,584  INFO [main] common.HiveClientCache: Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
2024-04-24T08:24:21,595  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:21,595  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:21,595  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@59262a90 will be shutdown
2024-04-24T08:24:21,596  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5ed65e4b created in the thread with id: 1
2024-04-24T08:24:21,606  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:21,607  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:21,628  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T08:24:21,639  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:21,647  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:21,785  INFO [main] beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2024-04-24T08:24:21,799  WARN [main] impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
2024-04-24T08:24:21,813  INFO [main] impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-04-24T08:24:21,813  INFO [main] impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-24T08:24:21,857  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T08:24:21,866  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T08:24:21,876  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T08:24:21,887  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T08:24:21,933  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T08:24:21,967  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local703330947_0001
2024-04-24T08:24:21,967  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T08:24:22,084  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T08:24:22,085  INFO [main] mapreduce.Job: Running job: job_local703330947_0001
2024-04-24T08:24:22,087  INFO [Thread-51] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T08:24:22,098  INFO [Thread-51] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:24:22,098  INFO [Thread-51] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:24:22,098  INFO [Thread-51] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T08:24:22,118  INFO [Thread-51] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T08:24:22,118  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local703330947_0001_m_000000_0
2024-04-24T08:24:22,144  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:24:22,144  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:24:22,160  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T08:24:22,162  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@71f4ee7f
2024-04-24T08:24:22,208  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=myint,mystring,underscore_int, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713972261}
2024-04-24T08:24:22,212  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T08:24:22,212  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T08:24:22,212  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T08:24:22,212  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T08:24:22,213  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T08:24:22,213  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T08:24:22,213  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T08:24:22,213  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T08:24:22,213  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T08:24:22,214  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T08:24:22,215  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T08:24:22,216  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T08:24:22,216  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T08:24:22,216  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T08:24:22,216  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,216  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 21	21	21	
2024-04-24T08:24:22,217  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 22	22	22	
2024-04-24T08:24:22,217  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 23	23	23	
2024-04-24T08:24:22,217  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 24	24	24	
2024-04-24T08:24:22,217  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 25	25	25	
2024-04-24T08:24:22,217  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 26	26	26	
2024-04-24T08:24:22,218  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 27	27	27	
2024-04-24T08:24:22,218  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 28	28	28	
2024-04-24T08:24:22,218  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 29	29	29	
2024-04-24T08:24:22,218  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (3 out of 30 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,219  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 31	31	31	
2024-04-24T08:24:22,219  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 32	32	32	
2024-04-24T08:24:22,219  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 33	33	33	
2024-04-24T08:24:22,219  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 34	34	34	
2024-04-24T08:24:22,219  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 35	35	35	
2024-04-24T08:24:22,220  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 36	36	36	
2024-04-24T08:24:22,220  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 37	37	37	
2024-04-24T08:24:22,220  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 38	38	38	
2024-04-24T08:24:22,220  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 39	39	39	
2024-04-24T08:24:22,220  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (4 out of 40 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 41	41	41	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 42	42	42	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 43	43	43	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 44	44	44	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 45	45	45	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 46	46	46	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 47	47	47	
2024-04-24T08:24:22,221  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 48	48	48	
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 49	49	49	
2024-04-24T08:24:22,222  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (5 out of 50 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 51	51	51	
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 52	52	52	
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 53	53	53	
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 54	54	54	
2024-04-24T08:24:22,222  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 55	55	55	
2024-04-24T08:24:22,223  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 56	56	56	
2024-04-24T08:24:22,223  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 57	57	57	
2024-04-24T08:24:22,223  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 58	58	58	
2024-04-24T08:24:22,223  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 59	59	59	
2024-04-24T08:24:22,223  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (6 out of 60 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 61	61	61	
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 62	62	62	
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 63	63	63	
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 64	64	64	
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 65	65	65	
2024-04-24T08:24:22,224  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 66	66	66	
2024-04-24T08:24:22,225  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 67	67	67	
2024-04-24T08:24:22,225  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 68	68	68	
2024-04-24T08:24:22,225  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 69	69	69	
2024-04-24T08:24:22,225  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (7 out of 70 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,225  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 71	71	71	
2024-04-24T08:24:22,225  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 72	72	72	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 73	73	73	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 74	74	74	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 75	75	75	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 76	76	76	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 77	77	77	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 78	78	78	
2024-04-24T08:24:22,226  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 79	79	79	
2024-04-24T08:24:22,226  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (8 out of 80 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,227  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 81	81	81	
2024-04-24T08:24:22,227  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 82	82	82	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 83	83	83	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 84	84	84	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 85	85	85	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 86	86	86	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 87	87	87	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 88	88	88	
2024-04-24T08:24:22,228  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 89	89	89	
2024-04-24T08:24:22,228  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (9 out of 90 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,229  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 91	91	91	
2024-04-24T08:24:22,229  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 92	92	92	
2024-04-24T08:24:22,229  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 93	93	93	
2024-04-24T08:24:22,229  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 94	94	94	
2024-04-24T08:24:22,230  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 95	95	95	
2024-04-24T08:24:22,230  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 96	96	96	
2024-04-24T08:24:22,230  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 97	97	97	
2024-04-24T08:24:22,230  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 98	98	98	
2024-04-24T08:24:22,230  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 99	99	99	
2024-04-24T08:24:22,230  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (10 out of 100 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:22,234  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T08:24:22,241  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local703330947_0001_m_000000_0 is done. And is in the process of committing
2024-04-24T08:24:22,242  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T08:24:22,242  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local703330947_0001_m_000000_0 is allowed to commit now
2024-04-24T08:24:22,243  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local703330947_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/test_bad_record_handling_output
2024-04-24T08:24:22,244  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/warehouse/test_bad_records/intString.seq:0+4027
2024-04-24T08:24:22,244  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local703330947_0001_m_000000_0' done.
2024-04-24T08:24:22,246  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local703330947_0001_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=521397
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=796917760
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T08:24:22,247  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local703330947_0001_m_000000_0
2024-04-24T08:24:22,247  INFO [Thread-51] mapred.LocalJobRunner: map task executor complete.
2024-04-24T08:24:23,094  INFO [main] mapreduce.Job: Job job_local703330947_0001 running in uber mode : false
2024-04-24T08:24:23,096  INFO [main] mapreduce.Job:  map 100% reduce 0%
2024-04-24T08:24:23,103  INFO [main] mapreduce.Job: Job job_local703330947_0001 completed successfully
2024-04-24T08:24:23,119  INFO [main] mapreduce.Job: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=521397
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=796917760
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T08:24:23,179  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:24:23,179  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:24:23,179  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:24:23,179  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:24:23,180  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:24:23,183  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:23,183  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:23,184  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5ed65e4b will be shutdown
2024-04-24T08:24:23,184  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6fd7aba8 created in the thread with id: 1
2024-04-24T08:24:23,189  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = a3e04816-2b6e-4f88-ac48-5c8bb225b830
2024-04-24T08:24:23,189  INFO [main] SessionState: Hive Session ID = a3e04816-2b6e-4f88-ac48-5c8bb225b830
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:24:23,190  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T08:24:23,196  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/a3e04816-2b6e-4f88-ac48-5c8bb225b830
2024-04-24T08:24:23,199  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/a3e04816-2b6e-4f88-ac48-5c8bb225b830
2024-04-24T08:24:23,202  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/a3e04816-2b6e-4f88-ac48-5c8bb225b830/_tmp_space.db
2024-04-24T08:24:23,202  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data/intString.seq
2024-04-24T08:24:23,211  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): drop table if exists test_bad_records
2024-04-24T08:24:23,212  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,223  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,223  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:23,223  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:23,223  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:23,223  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=11, flushCache_()=0}
2024-04-24T08:24:23,223  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.012 seconds
2024-04-24T08:24:23,224  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:23,224  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:23,224  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): drop table if exists test_bad_records
2024-04-24T08:24:23,224  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:24:23,225  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,235  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,235  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,245  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,246  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,465  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:23,465  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=229, getTable_(GetTableRequest)=10}
2024-04-24T08:24:23,465  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.24 seconds
2024-04-24T08:24:23,465  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:24:23,466  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814
2024-04-24T08:24:23,467  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:24:23,467  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a3e04816-2b6e-4f88-ac48-5c8bb225b830, clientType=HIVECLI]
2024-04-24T08:24:23,467  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T08:24:23,467  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:24:23,467  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b332439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6fd7aba8 will be shutdown
2024-04-24T08:24:23,467  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:23,467  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T08:24:23,468  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:23,469  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:24:23,469  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:23,469  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735, with PersistenceManager: null will be shutdown
2024-04-24T08:24:23,469  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46994f26 created in the thread with id: 1
2024-04-24T08:24:23,477  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735 from thread id: 1
2024-04-24T08:24:23,477  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:23,477  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:23,477  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T08:24:23,478  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:23,478  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:23,479  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46994f26 will be shutdown
2024-04-24T08:24:23,479  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b29cdea created in the thread with id: 1
2024-04-24T08:24:23,482  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:23,482  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:23,482  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:24:23,487  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814
2024-04-24T08:24:23,487  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:23,487  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:23,487  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:23,487  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:24:23,487  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.022 seconds
2024-04-24T08:24:23,487  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:23,487  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:23,487  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T08:24:23,488  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:24:23,488  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T08:24:23,488  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:24:23,488  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@25589735, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b29cdea will be shutdown
2024-04-24T08:24:23,488  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:23,488  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-24T08:24:23,490  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:24:23,491  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:24:23,491  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:24:23,491  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e1291a4, with PersistenceManager: null will be shutdown
2024-04-24T08:24:23,492  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e1291a4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23bd2f6e created in the thread with id: 1
2024-04-24T08:24:23,495  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e1291a4 from thread id: 1
2024-04-24T08:24:23,495  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:24:23,496  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:24:23,496  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713972263, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:myint, type:int, comment:from deserializer), FieldSchema(name:mystring, type:string, comment:from deserializer), FieldSchema(name:underscore_int, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numRows=0, rawDataSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, numFiles=0, bucketing_version=2, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:24:23,503  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/warehouse/test_bad_records
2024-04-24T08:24:23,538  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:23,538  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=41}
2024-04-24T08:24:23,538  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.051 seconds
2024-04-24T08:24:23,538  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data' into table test_bad_records
2024-04-24T08:24:23,539  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814
2024-04-24T08:24:23,540  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,576  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,579  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814
2024-04-24T08:24:23,579  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:24:23,579  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:24:23,579  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:24:23,579  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=38, flushCache_()=0}
2024-04-24T08:24:23,579  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.041 seconds
2024-04-24T08:24:23,580  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:24:23,580  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:24:23,580  INFO [main] ql.Driver: Executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data' into table test_bad_records
2024-04-24T08:24:23,580  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T08:24:23,580  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713972252843/data
2024-04-24T08:24:23,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,590  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,592  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,602  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,611  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:24:23,612  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:24:23,634  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T08:24:23,635  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,644  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,644  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T08:24:23,645  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,653  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,655  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T08:24:23,655  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T08:24:23,681  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T08:24:23,681  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:24:23,681  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=37, alter_table_(String, String, String, Table, EnvironmentContext, String)=48}
2024-04-24T08:24:23,681  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424082423_06a6076b-bf4b-44af-9a53-4284a670b814); Time taken: 0.101 seconds
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T08:24:23,722  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T08:24:23,723  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T08:24:23,723  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:24:23,723  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T08:24:23,723  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T08:24:23,723  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T08:24:23,724  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T08:24:23,725  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T08:24:23,729  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T08:24:23,732  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T08:24:23,739  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T08:24:23,745  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T08:24:23,752  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T08:24:23,756  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T08:24:23,762  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T08:24:23,768  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T08:24:23,788  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T08:24:23,803  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1381196881_0002
2024-04-24T08:24:23,803  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T08:24:23,859  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T08:24:23,859  INFO [main] mapreduce.Job: Running job: job_local1381196881_0002
2024-04-24T08:24:23,859  INFO [Thread-96] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T08:24:23,860  INFO [Thread-96] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:24:23,860  INFO [Thread-96] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:24:23,860  INFO [Thread-96] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T08:24:23,868  INFO [Thread-96] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T08:24:23,868  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1381196881_0002_m_000000_0
2024-04-24T08:24:23,892  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:23,893  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-24T08:24:23,893  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:23,893  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
2024-04-24T08:24:23,897  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T08:24:23,897  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T08:24:23,897  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T08:24:23,899  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@fa990fb
2024-04-24T08:24:23,919  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=myint,mystring,underscore_int, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713972263}
2024-04-24T08:24:23,919  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T08:24:23,919  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T08:24:23,919  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T08:24:23,920  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:23,920  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T08:24:23,921  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T08:24:23,921  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T08:24:23,922 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: 2 out of 20 crosses configured threshold (0.009999999776482582)
2024-04-24T08:24:23,922  INFO [Thread-96] mapred.LocalJobRunner: map task executor complete.
2024-04-24T08:24:23,923  WARN [Thread-96] mapred.LocalJobRunner: job_local1381196881_0002
java.lang.Exception: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.1.0.jar:?]
Caused by: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader$InputErrorTracker.incErrors(HCatRecordReader.java:282) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:196) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
2024-04-24T08:24:24,860  INFO [main] mapreduce.Job: Job job_local1381196881_0002 running in uber mode : false
2024-04-24T08:24:24,860  INFO [main] mapreduce.Job:  map 0% reduce 0%
2024-04-24T08:24:24,861  INFO [main] mapreduce.Job: Job job_local1381196881_0002 failed with state FAILED due to: NA
2024-04-24T08:24:24,861  INFO [main] mapreduce.Job: Counters: 0
2024-04-24T08:24:24,900  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:24:24,900  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -7
