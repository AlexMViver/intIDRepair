SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,073635 seconds to load 250 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@24569dba]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@24569dba) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@5965d37
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@782859e
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,019322 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 132 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/ql/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/ql/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={DRFA, console}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(header="null", noConsoleNoAnsi="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", alwaysWriteExceptions="null", footer="null", Configuration(HiveLog4j2Test), disableAnsi="null", charset="null", PatternSelector=null, Replace=null)
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(direct="null", target="SYSTEM_ERR", follow="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), ={}, Filter=null)
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(noConsoleNoAnsi="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, disableAnsi="null", Configuration(HiveLog4j2Test), header="null", charset="null", Replace=null, footer="null", alwaysWriteExceptions="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(modulate="true", interval="1", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(Configuration(HiveLog4j2Test), stopCustomActionsOnError="null", tempCompressedFilePattern="null", compressionLevel="null", fileIndex="null", min="null", max="30", ={})
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(advertise="null", fileName="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), fileGroup="null", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), fileOwner="null", filePattern="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", advertiseURI="null", filePermissions="null", bufferedIo="null", immediateFlush="null", bufferSize="null", ignoreExceptions="null", Configuration(HiveLog4j2Test), name="DRFA", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ={}, Filter=null)
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/ql/target/tmp/log/hive.log seek to 3058217515
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/ql/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-23T21:41:00.173-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/24-00:00:00.000, nextFileTime=2024/04/23-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/23-21:41:01.874, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/24-00:00:00.000, nextFileTime=2024/04/23-00:00:00.000, prevFileTime=2024/04/23-00:00:00.000, current=2024/04/23-21:41:01.875, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@782859e initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@782859e
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@782859e OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@10d68fcd...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@10d68fcd OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@4bff64c2
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@24569dba
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/ql/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@24569dba) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@24569dba] started OK.
2024-04-23T21:41:02,002  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/ql/target/testconf/hive-site.xml
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-23T21:41:02,360  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-23T21:41:02,433  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:02,433  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:02,434  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:02,434  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:02,434  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:02,434  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:02,435  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:02,435  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:02,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:02,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:02,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:02,444  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
2024-04-23T21:41:03,259  INFO [main] utils.TestTxnDbUtil: Reinitializing the metastore db with hive-schema-4.0.0.derby.sql on the database jdbc:derby:memory:/home/alex/Repositories/hive/ql/target/tmp/junit_metastore_db;create=true
Hive Session ID = d9eac5e1-2891-4ec5-b2a3-6dac722f964d
2024-04-23T21:41:04,189  INFO [main] SessionState: Hive Session ID = d9eac5e1-2891-4ec5-b2a3-6dac722f964d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:04,200  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
log4j: Trying to find [log4j.xml] using context classloader org.apache.hadoop.hive.ql.exec.UDFClassLoader@52f57666.
log4j: Trying to find [log4j.xml] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.xml] using ClassLoader.getSystemResource().
log4j: Trying to find [log4j.properties] using context classloader org.apache.hadoop.hive.ql.exec.UDFClassLoader@52f57666.
log4j: Trying to find [log4j.properties] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.properties] using ClassLoader.getSystemResource().
log4j: Could not find resource: [null].
log4j:WARN No appenders could be found for logger (org.apache.htrace.core.Tracer).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2024-04-23T21:41:04,548  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/d9eac5e1-2891-4ec5-b2a3-6dac722f964d
2024-04-23T21:41:04,552  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/d9eac5e1-2891-4ec5-b2a3-6dac722f964d
2024-04-23T21:41:04,555  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/d9eac5e1-2891-4ec5-b2a3-6dac722f964d/_tmp_space.db
2024-04-23T21:41:05,461  INFO [main] reflections.Reflections: Reflections took 211 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T21:41:05,631  INFO [main] reflections.Reflections: Reflections took 128 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T21:41:05,755  INFO [main] reflections.Reflections: Reflections took 116 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T21:41:06,325  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,325  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,325  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,327  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,329  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,331  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,335  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T21:41:06,405  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:06,543  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:06,570  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:06,574  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-23T21:41:06,574  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-23T21:41:06,586  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T21:41:06,591  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-23T21:41:06,607  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T21:41:06,611  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-23T21:41:07,033  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-23T21:41:07,033  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cdb3c85, with PersistenceManager: null will be shutdown
2024-04-23T21:41:07,060  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cdb3c85, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b6fb197 created in the thread with id: 1
2024-04-23T21:41:09,090  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cdb3c85 from thread id: 1
2024-04-23T21:41:09,106  INFO [main] metastore.HMSHandler: Setting location of default catalog, as it hasn't been done after upgrade
2024-04-23T21:41:09,138  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-23T21:41:09,186  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-23T21:41:09,207  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-23T21:41:09,210  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-23T21:41:09,288  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-23T21:41:09,295  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-23T21:41:09,296  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-23T21:41:09,297  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-23T21:41:09,298  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-23T21:41:09,300  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-23T21:41:09,326  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T21:41:09,328  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-23T21:41:09,330  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T21:41:09,330  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-23T21:41:09,332  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T21:41:09,335  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-23T21:41:09,337  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T21:41:09,338  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-23T21:41:09,341  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-23T21:41:09,343  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:09,482  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:09,518  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-23T21:41:09,592  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([1]) with min_open_txn: 1
2024-04-23T21:41:09,606  INFO [main] lockmgr.DbTxnManager: Opened txnid:1
2024-04-23T21:41:09,631  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d9eac5e1-2891-4ec5-b2a3-6dac722f964d, clientType=HIVECLI]
2024-04-23T21:41:09,633  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:09,635  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:09,635  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cdb3c85, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b6fb197 will be shutdown
2024-04-23T21:41:09,636  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:09,636  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-23T21:41:09,637  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:09,639  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:09,639  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:09,640  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0, with PersistenceManager: null will be shutdown
2024-04-23T21:41:09,641  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3e04abc5 created in the thread with id: 1
2024-04-23T21:41:09,645  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0 from thread id: 1
2024-04-23T21:41:09,646  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:09,646  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:09,705  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:09,706  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:09,707  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3e04abc5 will be shutdown
2024-04-23T21:41:09,708  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@308a9264 created in the thread with id: 1
2024-04-23T21:41:09,713  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:09,714  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:09,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933669, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:09,751  INFO [main] utils.MetaStoreServerUtils: Updating table stats for T
2024-04-23T21:41:09,751  INFO [main] utils.MetaStoreServerUtils: Updated size of table T to 0
2024-04-23T21:41:09,862  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933669, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{numFilesErasureCoded=0, totalSize=0, transactional=true, transactional_properties=default, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:09,870  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:09,899  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:09,941  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:09,945  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:09,966  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:10,022  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:10,030  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:10,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:10,064  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:10,066  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update T set b = 5 where b > 5> as 
<insert into table `default`.`T` select ROW__ID,`a`,`b` from `default`.`T` sort by ROW__ID >
2024-04-23T21:41:10,070  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:10,075  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:10,088  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:10,089  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:10,089  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:10,091  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:10,104  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:10,108  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:10,112  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:10,115  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:10,127  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:10,127  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:10,128  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:11,279  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T21:41:11,449  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T21:41:11,453  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T21:41:11,934  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0, 1], allowMissingStats: true
2024-04-23T21:41:11,943  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T21:41:11,967  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a, b
No Stats for default@t, Columns: a, b
2024-04-23T21:41:11,967  INFO [main] SessionState: No Stats for default@t, Columns: a, b
2024-04-23T21:41:12,114  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:12,115  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:12,115  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:12,115  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:12,127  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:12,202 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateAllNonPartitionedWhere(TestUpdateDeleteSemanticAnalyzer.java:150) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:12,203 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:12,204  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:12,216  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:12,216  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:12,453  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:12,471  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:12,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:12,716  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:12,716  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:12,717  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:12,718  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c
2024-04-23T21:41:12,722  INFO [main] SessionState: Hive Session ID = 9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:12,722  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:12,729  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c
2024-04-23T21:41:12,733  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c
2024-04-23T21:41:12,737  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c/_tmp_space.db
2024-04-23T21:41:12,738  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:12,738  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:12,738  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@393ae7a0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@308a9264 will be shutdown
2024-04-23T21:41:12,739  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:12,739  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-23T21:41:12,741  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:12,743  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:12,743  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:12,744  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@574e4184, with PersistenceManager: null will be shutdown
2024-04-23T21:41:12,745  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@574e4184, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@50d6af87 created in the thread with id: 1
2024-04-23T21:41:12,775  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@574e4184 from thread id: 1
2024-04-23T21:41:12,776  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:12,776  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:12,787  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([2]) with min_open_txn: 1
2024-04-23T21:41:12,788  INFO [main] lockmgr.DbTxnManager: Opened txnid:2
2024-04-23T21:41:12,788  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9ad2b88e-e2b3-4ad0-be0c-cdaceb75ef6c, clientType=HIVECLI]
2024-04-23T21:41:12,788  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:12,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:12,789  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@574e4184, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@50d6af87 will be shutdown
2024-04-23T21:41:12,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:12,789  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-23T21:41:12,790  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:12,792  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:12,792  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:12,792  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67, with PersistenceManager: null will be shutdown
2024-04-23T21:41:12,793  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12948e7a created in the thread with id: 1
2024-04-23T21:41:12,796  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67 from thread id: 1
2024-04-23T21:41:12,796  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:12,796  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:12,806  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:12,806  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:12,806  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12948e7a will be shutdown
2024-04-23T21:41:12,807  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2566247d created in the thread with id: 1
2024-04-23T21:41:12,810  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:12,810  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:12,811  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933672, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:12,819  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:12,871  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933672, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:12,878  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:12,899  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:12,922  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:12,924  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:12,936  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:12,970  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:12,977  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:12,992  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,002  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,002  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:13,004  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:13,005  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:13,005  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:13,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:13,016  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,016  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:13,016  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:13,017  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,026  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,027  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:13,027  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:13,027  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:13,040  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:13,100  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [2], allowMissingStats: true
2024-04-23T21:41:13,101  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T21:41:13,219  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:13,219  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:13,219  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:13,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,230  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,231  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:13,235 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:50) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteAllPartitioned(TestUpdateDeleteSemanticAnalyzer.java:98) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:13,237  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:13,247  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,248  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:13,398  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,411  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,412  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:13,604  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:13,605  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:13,606  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:13,606  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:13,606  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:13,606  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:13,607  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 85a304a1-a68a-4bfe-9796-517fb41317a3
2024-04-23T21:41:13,611  INFO [main] SessionState: Hive Session ID = 85a304a1-a68a-4bfe-9796-517fb41317a3
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:13,612  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:13,620  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/85a304a1-a68a-4bfe-9796-517fb41317a3
2024-04-23T21:41:13,623  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/85a304a1-a68a-4bfe-9796-517fb41317a3
2024-04-23T21:41:13,627  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/85a304a1-a68a-4bfe-9796-517fb41317a3/_tmp_space.db
2024-04-23T21:41:13,628  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:13,629  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:13,629  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38318d67, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2566247d will be shutdown
2024-04-23T21:41:13,629  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:13,629  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-23T21:41:13,631  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:13,633  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:13,634  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:13,634  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c450960, with PersistenceManager: null will be shutdown
2024-04-23T21:41:13,635  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c450960, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7021bb55 created in the thread with id: 1
2024-04-23T21:41:13,667  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c450960 from thread id: 1
2024-04-23T21:41:13,668  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:13,668  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:13,683  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([3]) with min_open_txn: 1
2024-04-23T21:41:13,684  INFO [main] lockmgr.DbTxnManager: Opened txnid:3
2024-04-23T21:41:13,684  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=85a304a1-a68a-4bfe-9796-517fb41317a3, clientType=HIVECLI]
2024-04-23T21:41:13,685  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:13,685  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:13,685  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c450960, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7021bb55 will be shutdown
2024-04-23T21:41:13,685  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:13,686  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-23T21:41:13,688  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:13,691  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:13,691  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:13,692  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a, with PersistenceManager: null will be shutdown
2024-04-23T21:41:13,692  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1300112e created in the thread with id: 1
2024-04-23T21:41:13,698  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a from thread id: 1
2024-04-23T21:41:13,698  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:13,698  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:13,710  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:13,710  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:13,710  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1300112e will be shutdown
2024-04-23T21:41:13,710  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66c88fce created in the thread with id: 1
2024-04-23T21:41:13,715  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:13,715  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:13,716  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933673, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:13,727  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:13,795  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933673, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:13,801  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:13,826  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,835  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,837  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:13,846  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:13,876  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:13,883  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:13,897  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,906  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,906  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where a > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:13,908  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:13,909  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:13,909  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:13,910  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:13,920  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,920  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:13,920  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:13,920  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:13,930  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:13,930  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:13,930  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:13,930  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:13,943  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:14,007  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T21:41:14,059  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T21:41:14,061  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:14,087  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T21:41:14,087  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T21:41:14,157  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:14,158  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:14,158  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:14,158  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:14,169  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,169  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:14,186  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 3
2024-04-23T21:41:14,186  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [3])
2024-04-23T21:41:14,192  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_21-41-13_906_4972955808176972610-1
2024-04-23T21:41:14,202  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T21:41:14,269  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:14,270  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:14,270  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:14,270  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0], VALUE._col0=Column[_col1]}
2024-04-23T21:41:14,301  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:14,302  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_5 and SEL_6 as it was introduced by enforce bucketing/sorting.
2024-04-23T21:41:14,304  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:14,313  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,313  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:14,450  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:14,460  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,460  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:14,614  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:14,614  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:14,614  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:14,614  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:14,614  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:14,615  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:14,616  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = eb0bdb69-3d05-4e11-81e6-e378b4083207
2024-04-23T21:41:14,619  INFO [main] SessionState: Hive Session ID = eb0bdb69-3d05-4e11-81e6-e378b4083207
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:14,620  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:14,627  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/eb0bdb69-3d05-4e11-81e6-e378b4083207
2024-04-23T21:41:14,630  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/eb0bdb69-3d05-4e11-81e6-e378b4083207
2024-04-23T21:41:14,633  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/eb0bdb69-3d05-4e11-81e6-e378b4083207/_tmp_space.db
2024-04-23T21:41:14,635  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:14,636  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:14,636  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5482b32a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66c88fce will be shutdown
2024-04-23T21:41:14,636  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:14,636  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
2024-04-23T21:41:14,637  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:14,638  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:14,638  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:14,639  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3ef7064b, with PersistenceManager: null will be shutdown
2024-04-23T21:41:14,639  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3ef7064b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7def08fc created in the thread with id: 1
2024-04-23T21:41:14,665  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3ef7064b from thread id: 1
2024-04-23T21:41:14,665  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:14,665  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:14,676  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([4]) with min_open_txn: 1
2024-04-23T21:41:14,676  INFO [main] lockmgr.DbTxnManager: Opened txnid:4
2024-04-23T21:41:14,677  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=eb0bdb69-3d05-4e11-81e6-e378b4083207, clientType=HIVECLI]
2024-04-23T21:41:14,677  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:14,677  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:14,677  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3ef7064b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7def08fc will be shutdown
2024-04-23T21:41:14,678  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:14,678  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -7
2024-04-23T21:41:14,679  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:14,680  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:14,680  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:14,681  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a, with PersistenceManager: null will be shutdown
2024-04-23T21:41:14,681  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4569b6a9 created in the thread with id: 1
2024-04-23T21:41:14,684  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a from thread id: 1
2024-04-23T21:41:14,684  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:14,684  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:14,693  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:14,693  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:14,694  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4569b6a9 will be shutdown
2024-04-23T21:41:14,695  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35fd987b created in the thread with id: 1
2024-04-23T21:41:14,697  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:14,698  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:14,698  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933674, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:14,703  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:14,743  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933674, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:14,749  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:14,771  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:14,790  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,791  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:14,804  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:14,843  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:14,852  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:14,867  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T21:41:14,869  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:14,879  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,879  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:14,879  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:14,879  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:14,879  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:14,880  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:14,891  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:14,891  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:14,891  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:14,891  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:14,923  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-23T21:41:15,126  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:15,126  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:15,126  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:15,127  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-23T21:41:15,133  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:15,133  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:15,136  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:15,136  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:15,151  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,151  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:15,159  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T21:41:15,161  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T21:41:15,177  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 4
2024-04-23T21:41:15,177  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [4])
2024-04-23T21:41:15,178  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_21-41-14_633_6456830237873463300-1
2024-04-23T21:41:15,185  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table u in query insert into table U partition (ds) values ('abc', 3, 'today'), ('ghi', 5, 'tomorrow')
2024-04-23T21:41:15,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:15,203  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,218  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:15,222  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:15,237  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,237  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:15,237  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:15,267  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/eb0bdb69-3d05-4e11-81e6-e378b4083207/hive_2024-04-23_21-41-15_185_1458898942566436748-1/-mr-10000/.hive-staging_hive_2024-04-23_21-41-15_185_1458898942566436748-1
2024-04-23T21:41:15,278  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T21:41:15,297  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 oldColExprMap: {VALUE._col5=Column[_col6], VALUE._col1=Column[_col2], VALUE._col6=Column[_col7], VALUE._col0=Column[_col1], VALUE._col2=Column[_col3], VALUE._col7=Column[_col8], VALUE._col8=Column[_col9], KEY._col0=Column[_col0], VALUE._col3=Column[_col4], VALUE._col4=Column[_col5]}
2024-04-23T21:41:15,297  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 newColExprMap: {VALUE._col7=Column[_col8], VALUE._col4=Column[_col5], VALUE._col5=Column[_col6], VALUE._col6=Column[_col7], VALUE._col8=Column[_col9], VALUE._col3=Column[_col4], VALUE._col2=Column[_col3], VALUE._col1=Column[_col2], KEY._col0=Column[_col0], VALUE._col0=Column[_col1]}
2024-04-23T21:41:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0], VALUE._col0=Column[_col1]}
2024-04-23T21:41:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col1=Column[_col2], VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:15,301  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-23T21:41:15,303  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:15,303  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_5 and SEL_6 as it was introduced by enforce bucketing/sorting.
2024-04-23T21:41:15,305  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_15 and SEL_16 as parent of FS_7 and child of SEL_3
2024-04-23T21:41:15,305  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:15,363  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:15,374  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,426  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:15,429  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:15,429  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T21:41:15,457  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T21:41:15,457  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T21:41:15,457  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T21:41:15,458  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T21:41:15,458  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:15,458  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:15,458  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:15,458  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T21:41:15,459  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:15,459  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:15,459  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map vectorization enabled: false
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map vectorized: false
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map enabledConditionsNotMet: [Could not enable vectorization due to partition column names size 1 is greater than the number of table column names size 0 IS false]
2024-04-23T21:41:15,465  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.NullRowsInputFormat]
2024-04-23T21:41:15,466  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:15,466  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:15,466  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T21:41:15,466  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:15,466  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:15,467  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T21:41:15,467  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T21:41:15,471  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T21:41:15,562  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0, Stage-3
  Stage-3 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: _dummy_table
            Row Limit Per Split: 1
            GatherStats: false
            Select Operator
              expressions: array(const struct('abc',3,'today'),const struct('ghi',5,'tomorrow')) (type: array<struct<col1:string,col2:int,col3:string>>)
              outputColumnNames: _col0
              UDTF Operator
                function name: inline
                Select Operator
                  expressions: col1 (type: string), CAST( col2 AS STRING) (type: string), col3 (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Reduce Output Operator
                    bucketingVersion: 1
                    key expressions: _col2 (type: string), _bucket_number (type: string), _col0 (type: string)
                    null sort order: aaa
                    numBuckets: 2
                    sort order: +++
                    Map-reduce partition columns: _col2 (type: string)
                    tag: -1
                    value expressions: _col1 (type: string)
                    auto parallelism: false
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                    outputColumnNames: a, b, ds
                    Group By Operator
                      aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
                      keys: ds (type: string)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      File Output Operator
                        bucketingVersion: 1
                        compressed: false
                        GlobalTableId: 0
                        directory: file:MASKED-OUT
                        NumFilesPerFileSink: 1
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            properties:
                              column.name.delimiter ,
                              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
                              columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                              escape.delim \
                              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                        TotalFiles: 1
                        GatherStats: false
                        MultiFileSpray: false
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: dummy_path
            input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns 
              columns.types 
              file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              location file:MASKED-OUT
              name _dummy_database._dummy_table
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
            serde: org.apache.hadoop.hive.serde2.NullStructSerDe
          
              input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucketing_version 2
                column.name.delimiter ,
                columns 
                columns.comments 
                columns.types 
                file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:MASKED-OUT
                name _dummy_database._dummy_table
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
              serde: org.apache.hadoop.hive.serde2.NullStructSerDe
              name: _dummy_database._dummy_table
            name: _dummy_database._dummy_table
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: string), VALUE._col1 (type: string), KEY._col2 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _col2, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: INSERT

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.u
          Is Table Level Stats: false

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              key expressions: _col0 (type: string)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: _col0 (type: string)
              tag: -1
              value expressions: _col1 (type: int), _col2 (type: struct<count:bigint,sum:double,input:int>), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary), _col6 (type: int), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: bigint), _col9 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
              columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
                columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col1,0)) (type: bigint), COALESCE(_col2,0) (type: double), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col6,0)) (type: bigint), COALESCE(_col7,0) (type: double), (_col3 - _col8) (type: bigint), COALESCE(ndv_compute_bit_vector(_col9),0) (type: bigint), _col9 (type: binary), _col0 (type: string)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11,_col12
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T21:41:15,563  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:15,575  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:15,741  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:15,754  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:15,754  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:15,925  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:15,931  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:15,931  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:15,932  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 185af926-24f5-4baa-8a1b-96934581c7df
2024-04-23T21:41:15,935  INFO [main] SessionState: Hive Session ID = 185af926-24f5-4baa-8a1b-96934581c7df
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:15,935  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:15,942  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/185af926-24f5-4baa-8a1b-96934581c7df
2024-04-23T21:41:15,944  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/185af926-24f5-4baa-8a1b-96934581c7df
2024-04-23T21:41:15,947  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/185af926-24f5-4baa-8a1b-96934581c7df/_tmp_space.db
2024-04-23T21:41:15,948  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:15,949  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:15,949  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@387bbd7a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35fd987b will be shutdown
2024-04-23T21:41:15,949  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:15,949  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -8
2024-04-23T21:41:15,950  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:15,951  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:15,951  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:15,952  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4fecdf0b, with PersistenceManager: null will be shutdown
2024-04-23T21:41:15,952  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4fecdf0b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33317d26 created in the thread with id: 1
2024-04-23T21:41:15,976  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4fecdf0b from thread id: 1
2024-04-23T21:41:15,976  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:15,976  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:15,988  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([5]) with min_open_txn: 1
2024-04-23T21:41:15,988  INFO [main] lockmgr.DbTxnManager: Opened txnid:5
2024-04-23T21:41:15,989  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=185af926-24f5-4baa-8a1b-96934581c7df, clientType=HIVECLI]
2024-04-23T21:41:15,989  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:15,989  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:15,989  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4fecdf0b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33317d26 will be shutdown
2024-04-23T21:41:15,989  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:15,989  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -9
2024-04-23T21:41:15,990  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:15,991  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:15,991  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:15,992  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea, with PersistenceManager: null will be shutdown
2024-04-23T21:41:15,992  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@40f40221 created in the thread with id: 1
2024-04-23T21:41:15,995  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea from thread id: 1
2024-04-23T21:41:15,995  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:15,995  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:16,003  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:16,003  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:16,004  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@40f40221 will be shutdown
2024-04-23T21:41:16,004  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@365ad794 created in the thread with id: 1
2024-04-23T21:41:16,007  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:16,007  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:16,008  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933675, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:16,016  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:16,064  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933676, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:16,072  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:16,093  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:16,118  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,119  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:16,130  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:16,163  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:16,170  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:16,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:16,203  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,204  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update T set b = 5> as 
<insert into table `default`.`T` select ROW__ID,`a`,`b` from `default`.`T` sort by ROW__ID >
2024-04-23T21:41:16,206  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:16,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:16,224  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,224  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:16,224  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:16,226  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:16,241  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,242  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:16,242  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:16,242  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:16,258  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,258  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:16,258  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:16,272  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T21:41:16,310  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T21:41:16,312  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T21:41:16,364  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0], allowMissingStats: true
2024-04-23T21:41:16,364  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T21:41:16,380  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a
No Stats for default@t, Columns: a
2024-04-23T21:41:16,380  INFO [main] SessionState: No Stats for default@t, Columns: a
2024-04-23T21:41:16,439  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:16,439  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:16,440  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:16,440  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:16,449  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,453 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateAllNonPartitioned(TestUpdateDeleteSemanticAnalyzer.java:140) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:16,455 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:16,455  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:16,467  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,467  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:16,613  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:16,627  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:16,628  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:16,791  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:16,791  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:16,791  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:16,792  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:16,793  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 5ef4cf85-3250-4666-81f7-aca81c2f8ba0
2024-04-23T21:41:16,796  INFO [main] SessionState: Hive Session ID = 5ef4cf85-3250-4666-81f7-aca81c2f8ba0
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:16,796  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:16,802  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/5ef4cf85-3250-4666-81f7-aca81c2f8ba0
2024-04-23T21:41:16,805  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/5ef4cf85-3250-4666-81f7-aca81c2f8ba0
2024-04-23T21:41:16,807  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/5ef4cf85-3250-4666-81f7-aca81c2f8ba0/_tmp_space.db
2024-04-23T21:41:16,809  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:16,809  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:16,809  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@279b83ea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@365ad794 will be shutdown
2024-04-23T21:41:16,810  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:16,810  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -10
2024-04-23T21:41:16,811  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:16,812  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:16,813  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:16,813  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5cfadadc, with PersistenceManager: null will be shutdown
2024-04-23T21:41:16,813  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5cfadadc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3c247c8d created in the thread with id: 1
2024-04-23T21:41:16,836  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5cfadadc from thread id: 1
2024-04-23T21:41:16,836  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:16,836  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:16,847  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([6]) with min_open_txn: 1
2024-04-23T21:41:16,847  INFO [main] lockmgr.DbTxnManager: Opened txnid:6
2024-04-23T21:41:16,848  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5ef4cf85-3250-4666-81f7-aca81c2f8ba0, clientType=HIVECLI]
2024-04-23T21:41:16,848  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:16,848  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:16,848  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5cfadadc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3c247c8d will be shutdown
2024-04-23T21:41:16,848  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:16,848  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -11
2024-04-23T21:41:16,849  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:16,850  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:16,850  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:16,851  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21, with PersistenceManager: null will be shutdown
2024-04-23T21:41:16,851  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1582dfec created in the thread with id: 1
2024-04-23T21:41:16,853  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21 from thread id: 1
2024-04-23T21:41:16,853  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:16,854  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:16,861  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:16,861  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:16,862  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1582dfec will be shutdown
2024-04-23T21:41:16,862  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d4f35cb created in the thread with id: 1
2024-04-23T21:41:16,864  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:16,864  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:16,865  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933676, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:16,869  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:16,928  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933676, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:16,938  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:16,964  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,000  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,002  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:17,017  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:17,061  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:17,072  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:17,094  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,109  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,109  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where ds = 'today' and b > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:17,110  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:17,113  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,129  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,129  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:17,129  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:17,131  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:17,147  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,148  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:17,148  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:17,148  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,164  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,164  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:17,164  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:17,164  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:17,189  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:17,232  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T21:41:17,234  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T21:41:17,355  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T21:41:17,437  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1, 2], allowMissingStats: true
2024-04-23T21:41:17,438  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:17,460  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a, b
No Stats for default@u, Columns: a, b
2024-04-23T21:41:17,460  INFO [main] SessionState: No Stats for default@u, Columns: a, b
2024-04-23T21:41:17,542  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:17,543  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:17,543  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:17,543  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,553  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,553  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:17,558 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateOnePartitionWhere(TestUpdateDeleteSemanticAnalyzer.java:193) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:17,559 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:17,559  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:17,568  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,568  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:17,710  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:17,724  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:17,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:17,855  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:17,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:17,857  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:17,857  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:17,857  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:17,858  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = a2995b6c-6b57-4cdf-9811-0702ca9b7520
2024-04-23T21:41:17,861  INFO [main] SessionState: Hive Session ID = a2995b6c-6b57-4cdf-9811-0702ca9b7520
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:17,862  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:17,867  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a2995b6c-6b57-4cdf-9811-0702ca9b7520
2024-04-23T21:41:17,870  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a2995b6c-6b57-4cdf-9811-0702ca9b7520
2024-04-23T21:41:17,872  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a2995b6c-6b57-4cdf-9811-0702ca9b7520/_tmp_space.db
2024-04-23T21:41:17,873  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:17,873  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:17,874  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@768dcd21, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d4f35cb will be shutdown
2024-04-23T21:41:17,874  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:17,874  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -12
2024-04-23T21:41:17,875  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:17,876  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:17,876  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:17,877  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33a4cdba, with PersistenceManager: null will be shutdown
2024-04-23T21:41:17,877  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33a4cdba, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@571f6b61 created in the thread with id: 1
2024-04-23T21:41:17,897  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33a4cdba from thread id: 1
2024-04-23T21:41:17,897  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:17,898  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:17,908  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([7]) with min_open_txn: 1
2024-04-23T21:41:17,908  INFO [main] lockmgr.DbTxnManager: Opened txnid:7
2024-04-23T21:41:17,908  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a2995b6c-6b57-4cdf-9811-0702ca9b7520, clientType=HIVECLI]
2024-04-23T21:41:17,909  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:17,909  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:17,909  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33a4cdba, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@571f6b61 will be shutdown
2024-04-23T21:41:17,909  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:17,910  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -13
2024-04-23T21:41:17,911  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:17,912  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:17,912  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:17,912  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815, with PersistenceManager: null will be shutdown
2024-04-23T21:41:17,913  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7bf56370 created in the thread with id: 1
2024-04-23T21:41:17,915  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815 from thread id: 1
2024-04-23T21:41:17,915  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:17,915  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:17,923  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:17,923  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:17,924  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7bf56370 will be shutdown
2024-04-23T21:41:17,924  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@710959b3 created in the thread with id: 1
2024-04-23T21:41:17,927  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:17,927  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:17,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933677, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:17,933  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:17,974  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933677, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:17,979  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:17,999  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,020  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,022  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:18,031  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:18,057  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:18,063  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:18,077  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,087  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,087  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where ds = 'today'> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:18,088  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:18,089  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:18,089  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:18,090  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:18,097  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,098  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:18,098  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:18,098  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,106  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,107  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:18,107  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:18,107  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:18,120  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:18,187  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T21:41:18,225  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [2], allowMissingStats: true
2024-04-23T21:41:18,281  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:18,281  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-23T21:41:18,282  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:18,282  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-23T21:41:18,299  INFO [main] stats.BasicStats: Number of partishes : 1
2024-04-23T21:41:18,315  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:18,315  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:18,316  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:18,316  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,323  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,323  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:18,327 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:50) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteOnePartition(TestUpdateDeleteSemanticAnalyzer.java:118) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:18,328 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:18,328  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:18,336  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,336  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:18,409  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,417  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,417  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:18,531  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:18,532  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:18,532  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:18,532  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:18,532  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = ebb772cf-a24a-4160-b74d-4984becedfb0
2024-04-23T21:41:18,535  INFO [main] SessionState: Hive Session ID = ebb772cf-a24a-4160-b74d-4984becedfb0
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:18,536  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:18,541  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/ebb772cf-a24a-4160-b74d-4984becedfb0
2024-04-23T21:41:18,544  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/ebb772cf-a24a-4160-b74d-4984becedfb0
2024-04-23T21:41:18,546  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/ebb772cf-a24a-4160-b74d-4984becedfb0/_tmp_space.db
2024-04-23T21:41:18,547  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:18,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:18,548  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d893815, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@710959b3 will be shutdown
2024-04-23T21:41:18,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:18,548  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-23T21:41:18,549  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:18,550  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:18,550  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:18,551  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@a1c87e6, with PersistenceManager: null will be shutdown
2024-04-23T21:41:18,551  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@a1c87e6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5e5616f2 created in the thread with id: 1
2024-04-23T21:41:18,571  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@a1c87e6 from thread id: 1
2024-04-23T21:41:18,571  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:18,571  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:18,580  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([8]) with min_open_txn: 1
2024-04-23T21:41:18,581  INFO [main] lockmgr.DbTxnManager: Opened txnid:8
2024-04-23T21:41:18,581  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ebb772cf-a24a-4160-b74d-4984becedfb0, clientType=HIVECLI]
2024-04-23T21:41:18,581  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:18,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:18,581  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@a1c87e6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5e5616f2 will be shutdown
2024-04-23T21:41:18,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:18,582  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-23T21:41:18,583  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:18,584  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:18,584  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:18,584  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283, with PersistenceManager: null will be shutdown
2024-04-23T21:41:18,585  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3bb47d36 created in the thread with id: 1
2024-04-23T21:41:18,587  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283 from thread id: 1
2024-04-23T21:41:18,587  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:18,587  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:18,595  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:18,595  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:18,595  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3bb47d36 will be shutdown
2024-04-23T21:41:18,596  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@c2c496d created in the thread with id: 1
2024-04-23T21:41:18,598  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:18,598  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:18,598  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933678, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:18,602  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:18,637  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933678, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:18,642  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:18,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,670  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,671  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:18,680  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:18,711  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:18,718  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:18,730  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,737  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,738  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where ds = 'today'> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:18,739  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:18,740  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,749  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,750  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:18,750  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:18,751  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:18,759  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,760  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:18,760  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:18,760  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,768  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,769  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:18,769  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:18,769  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:18,781  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:18,815  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T21:41:18,816  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T21:41:18,843  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T21:41:18,882  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T21:41:18,883  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:18,903  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T21:41:18,903  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T21:41:18,975  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:18,975  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:18,976  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:18,976  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:18,987  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:18,987  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:18,991 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateOnePartition(TestUpdateDeleteSemanticAnalyzer.java:182) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:18,992 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:18,992  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:18,999  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,000  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:19,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:19,136  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,137  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:19,266  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:19,267  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:19,267  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:19,267  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:19,267  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 2f105aaf-93bb-4f9c-8766-843381aafb00
2024-04-23T21:41:19,270  INFO [main] SessionState: Hive Session ID = 2f105aaf-93bb-4f9c-8766-843381aafb00
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:19,271  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:19,276  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/2f105aaf-93bb-4f9c-8766-843381aafb00
2024-04-23T21:41:19,279  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/2f105aaf-93bb-4f9c-8766-843381aafb00
2024-04-23T21:41:19,282  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/2f105aaf-93bb-4f9c-8766-843381aafb00/_tmp_space.db
2024-04-23T21:41:19,283  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:19,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:19,283  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@369ad283, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@c2c496d will be shutdown
2024-04-23T21:41:19,284  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:19,284  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -18
2024-04-23T21:41:19,285  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:19,286  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:19,286  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:19,287  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46287b6b, with PersistenceManager: null will be shutdown
2024-04-23T21:41:19,287  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46287b6b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1dc1f0a3 created in the thread with id: 1
2024-04-23T21:41:19,292  INFO [Heartbeater-0] lockmgr.DbTxnManager: Sending heartbeat for txnid:2 and lockid:0 queryId=null txnid:0
2024-04-23T21:41:19,296  INFO [Heartbeater-0] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:19,297  INFO [Heartbeater-0] metastore.HMSHandler: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:19,297  INFO [Heartbeater-0] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:19,298  INFO [Heartbeater-0] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@224c32ed, with PersistenceManager: null will be shutdown
2024-04-23T21:41:19,299  INFO [Heartbeater-0] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@224c32ed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7e68a06e created in the thread with id: 47
2024-04-23T21:41:19,321  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46287b6b from thread id: 1
2024-04-23T21:41:19,321  INFO [Heartbeater-0] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@224c32ed from thread id: 47
2024-04-23T21:41:19,321  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:19,321  INFO [Heartbeater-0] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:19,321  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:19,321  INFO [Heartbeater-0] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:19,336  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([9]) with min_open_txn: 1
2024-04-23T21:41:19,337  INFO [main] lockmgr.DbTxnManager: Opened txnid:9
2024-04-23T21:41:19,337  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=2f105aaf-93bb-4f9c-8766-843381aafb00, clientType=HIVECLI]
2024-04-23T21:41:19,337  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:19,337  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:19,337  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46287b6b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1dc1f0a3 will be shutdown
2024-04-23T21:41:19,338  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:19,338  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -19
2024-04-23T21:41:19,339  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:19,340  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:19,340  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:19,340  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894, with PersistenceManager: null will be shutdown
2024-04-23T21:41:19,341  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b7c4e0e created in the thread with id: 1
2024-04-23T21:41:19,343  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894 from thread id: 1
2024-04-23T21:41:19,343  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:19,343  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:19,351  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:19,351  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:19,352  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b7c4e0e will be shutdown
2024-04-23T21:41:19,352  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@422f7081 created in the thread with id: 1
2024-04-23T21:41:19,355  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:19,355  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:19,355  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933679, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:19,362  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:19,405  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933679, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:19,414  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:19,443  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:19,469  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,471  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:19,483  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:19,521  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:19,528  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:19,543  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:19,552  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,553  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from T where a > 5> as 
<insert into table `default`.`T` select ROW__ID from `default`.`T` sort by ROW__ID >
2024-04-23T21:41:19,553  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:19,554  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:19,554  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:19,556  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:19,564  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,564  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:19,564  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:19,565  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:19,573  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,574  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:19,574  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:19,586  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T21:41:19,649  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0], allowMissingStats: true
2024-04-23T21:41:19,650  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T21:41:19,662  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a
No Stats for default@t, Columns: a
2024-04-23T21:41:19,662  INFO [main] SessionState: No Stats for default@t, Columns: a
2024-04-23T21:41:19,727  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:19,728  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:19,728  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:19,728  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:19,735  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,739 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:50) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteWhereNoPartition(TestUpdateDeleteSemanticAnalyzer.java:88) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:19,739 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:19,740  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:19,748  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,749  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:19,853  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:19,860  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:19,860  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:19,976  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:19,976  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:19,977  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:19,978  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:19,978  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = c757d84a-925c-4490-8792-33df02d0ae4b
2024-04-23T21:41:19,980  INFO [main] SessionState: Hive Session ID = c757d84a-925c-4490-8792-33df02d0ae4b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:19,981  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:19,987  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/c757d84a-925c-4490-8792-33df02d0ae4b
2024-04-23T21:41:19,989  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/c757d84a-925c-4490-8792-33df02d0ae4b
2024-04-23T21:41:19,992  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/c757d84a-925c-4490-8792-33df02d0ae4b/_tmp_space.db
2024-04-23T21:41:19,992  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:19,993  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:19,993  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@62ea4894, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@422f7081 will be shutdown
2024-04-23T21:41:19,993  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:19,993  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -20
2024-04-23T21:41:19,994  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:19,995  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:19,995  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:19,996  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6fff29f4, with PersistenceManager: null will be shutdown
2024-04-23T21:41:19,996  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6fff29f4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@22f1d27e created in the thread with id: 1
2024-04-23T21:41:20,017  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6fff29f4 from thread id: 1
2024-04-23T21:41:20,017  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,017  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,028  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([10]) with min_open_txn: 1
2024-04-23T21:41:20,029  INFO [main] lockmgr.DbTxnManager: Opened txnid:10
2024-04-23T21:41:20,029  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c757d84a-925c-4490-8792-33df02d0ae4b, clientType=HIVECLI]
2024-04-23T21:41:20,029  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:20,029  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:20,029  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6fff29f4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@22f1d27e will be shutdown
2024-04-23T21:41:20,030  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:20,030  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -21
2024-04-23T21:41:20,030  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:20,032  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:20,032  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:20,032  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439, with PersistenceManager: null will be shutdown
2024-04-23T21:41:20,033  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@50c43f7d created in the thread with id: 1
2024-04-23T21:41:20,035  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439 from thread id: 1
2024-04-23T21:41:20,035  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,035  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,043  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:20,043  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:20,044  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@50c43f7d will be shutdown
2024-04-23T21:41:20,044  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@710f4a91 created in the thread with id: 1
2024-04-23T21:41:20,046  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,046  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,047  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933680, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:20,052  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:20,088  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933680, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:20,094  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:20,112  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,120  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,121  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:20,132  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:20,158  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:20,165  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:20,178  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,187  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,187  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where b > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:20,188  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:20,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,198  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,198  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:20,198  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:20,199  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:20,207  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,207  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:20,207  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:20,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,216  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,216  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:20,216  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:20,216  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:20,228  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:20,260  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T21:41:20,261  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T21:41:20,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T21:41:20,313  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1, 2], allowMissingStats: true
2024-04-23T21:41:20,314  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:20,330  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: b, a
No Stats for default@u, Columns: b, a
2024-04-23T21:41:20,330  INFO [main] SessionState: No Stats for default@u, Columns: b, a
2024-04-23T21:41:20,395  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:20,396  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:20,396  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:20,396  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,404  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,404  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:20,408 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateAllPartitionedWhere(TestUpdateDeleteSemanticAnalyzer.java:171) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:20,409 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:20,409  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:20,417  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,417  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:20,650  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:20,650  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -22
2024-04-23T21:41:20,679  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,687  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,687  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:20,809  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:20,810  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:20,810  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:20,810  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = e8ecf558-5a4d-48fa-a983-dce294ca70c1
2024-04-23T21:41:20,813  INFO [main] SessionState: Hive Session ID = e8ecf558-5a4d-48fa-a983-dce294ca70c1
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:20,813  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:20,819  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/e8ecf558-5a4d-48fa-a983-dce294ca70c1
2024-04-23T21:41:20,821  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/e8ecf558-5a4d-48fa-a983-dce294ca70c1
2024-04-23T21:41:20,824  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/e8ecf558-5a4d-48fa-a983-dce294ca70c1/_tmp_space.db
2024-04-23T21:41:20,824  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:20,825  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:20,825  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58ca3439, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@710f4a91 will be shutdown
2024-04-23T21:41:20,825  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:20,825  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -23
2024-04-23T21:41:20,826  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:20,827  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:20,827  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:20,828  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d4de6f7, with PersistenceManager: null will be shutdown
2024-04-23T21:41:20,828  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d4de6f7, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c32bf87 created in the thread with id: 1
2024-04-23T21:41:20,848  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d4de6f7 from thread id: 1
2024-04-23T21:41:20,848  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,848  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,857  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([11]) with min_open_txn: 1
2024-04-23T21:41:20,858  INFO [main] lockmgr.DbTxnManager: Opened txnid:11
2024-04-23T21:41:20,858  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e8ecf558-5a4d-48fa-a983-dce294ca70c1, clientType=HIVECLI]
2024-04-23T21:41:20,858  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:20,858  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:20,858  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d4de6f7, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c32bf87 will be shutdown
2024-04-23T21:41:20,858  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:20,858  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-23T21:41:20,859  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:20,860  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:20,860  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:20,861  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce, with PersistenceManager: null will be shutdown
2024-04-23T21:41:20,861  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@58544d28 created in the thread with id: 1
2024-04-23T21:41:20,863  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce from thread id: 1
2024-04-23T21:41:20,863  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,863  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,870  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:20,870  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:20,871  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@58544d28 will be shutdown
2024-04-23T21:41:20,871  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66ffee26 created in the thread with id: 1
2024-04-23T21:41:20,873  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:20,873  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:20,873  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933680, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:20,879  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:20,919  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933680, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:20,925  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:20,941  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:20,957  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:20,958  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:20,968  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:20,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:20,999  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:21,011  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:21,018  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,018  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from T> as 
<insert into table `default`.`T` select ROW__ID from `default`.`T` sort by ROW__ID >
2024-04-23T21:41:21,019  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:21,020  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:21,020  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:21,021  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:21,028  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,029  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:21,029  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:21,029  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:21,036  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,036  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:21,036  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:21,048  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T21:41:21,126  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:21,126  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:21,126  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:21,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:21,134  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,137 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:50) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteAllNonPartitioned(TestUpdateDeleteSemanticAnalyzer.java:78) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:21,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:21,146  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,146  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:21,276  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:21,288  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,288  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:21,425  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:21,426  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:21,426  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:21,426  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:21,426  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:21,426  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb
2024-04-23T21:41:21,429  INFO [main] SessionState: Hive Session ID = 23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:21,430  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:21,436  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb
2024-04-23T21:41:21,438  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb
2024-04-23T21:41:21,441  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb/_tmp_space.db
2024-04-23T21:41:21,442  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:21,443  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:21,443  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c472bce, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66ffee26 will be shutdown
2024-04-23T21:41:21,443  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:21,443  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-23T21:41:21,444  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:21,445  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:21,445  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:21,446  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@75321ba4, with PersistenceManager: null will be shutdown
2024-04-23T21:41:21,446  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@75321ba4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f6b8c1f created in the thread with id: 1
2024-04-23T21:41:21,464  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@75321ba4 from thread id: 1
2024-04-23T21:41:21,464  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:21,465  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:21,477  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([12]) with min_open_txn: 1
2024-04-23T21:41:21,478  INFO [main] lockmgr.DbTxnManager: Opened txnid:12
2024-04-23T21:41:21,478  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=23cd695b-c2a5-4e88-b3b4-f5b974cdfbcb, clientType=HIVECLI]
2024-04-23T21:41:21,478  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:21,478  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:21,478  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@75321ba4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f6b8c1f will be shutdown
2024-04-23T21:41:21,479  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:21,479  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -26
2024-04-23T21:41:21,480  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:21,480  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:21,481  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:21,481  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424, with PersistenceManager: null will be shutdown
2024-04-23T21:41:21,481  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72fdb79d created in the thread with id: 1
2024-04-23T21:41:21,483  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424 from thread id: 1
2024-04-23T21:41:21,483  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:21,483  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:21,490  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:21,490  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:21,490  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72fdb79d will be shutdown
2024-04-23T21:41:21,490  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@121c65d0 created in the thread with id: 1
2024-04-23T21:41:21,492  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:21,492  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:21,493  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933681, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:21,497  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:21,531  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933681, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:21,536  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:21,553  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:21,561  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,562  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:21,572  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:21,601  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:21,606  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:21,619  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:21,629  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,630  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where ds = 'today' and a > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:21,631  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:21,632  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:21,632  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:21,633  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:21,647  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,648  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:21,648  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:21,648  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:21,661  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,661  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:21,661  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:21,661  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:21,681  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:21,764  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T21:41:21,800  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T21:41:21,800  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:21,816  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T21:41:21,816  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T21:41:21,883  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:21,883  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:21,883  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:21,883  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:21,891  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,892  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:21,895 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:50) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testDeleteOnePartitionWhere(TestUpdateDeleteSemanticAnalyzer.java:129) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:21,896 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:21,896  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:21,903  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:21,903  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:22,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,014  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,014  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:22,144  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:22,145  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:22,145  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:22,145  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:22,145  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:22,145  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 03ca4117-1832-416b-aca7-d6f9a30f3381
2024-04-23T21:41:22,148  INFO [main] SessionState: Hive Session ID = 03ca4117-1832-416b-aca7-d6f9a30f3381
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:22,149  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:22,155  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/03ca4117-1832-416b-aca7-d6f9a30f3381
2024-04-23T21:41:22,158  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/03ca4117-1832-416b-aca7-d6f9a30f3381
2024-04-23T21:41:22,161  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/03ca4117-1832-416b-aca7-d6f9a30f3381/_tmp_space.db
2024-04-23T21:41:22,162  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:22,162  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:22,163  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4680e424, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@121c65d0 will be shutdown
2024-04-23T21:41:22,163  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:22,163  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -27
2024-04-23T21:41:22,165  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,166  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:22,166  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,166  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@f02ae3, with PersistenceManager: null will be shutdown
2024-04-23T21:41:22,167  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@f02ae3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2e02f8c7 created in the thread with id: 1
2024-04-23T21:41:22,190  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@f02ae3 from thread id: 1
2024-04-23T21:41:22,190  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,190  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,199  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([13]) with min_open_txn: 1
2024-04-23T21:41:22,200  INFO [main] lockmgr.DbTxnManager: Opened txnid:13
2024-04-23T21:41:22,200  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=03ca4117-1832-416b-aca7-d6f9a30f3381, clientType=HIVECLI]
2024-04-23T21:41:22,200  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:22,200  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:22,201  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@f02ae3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2e02f8c7 will be shutdown
2024-04-23T21:41:22,201  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:22,201  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -28
2024-04-23T21:41:22,202  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,202  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:22,202  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,203  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e, with PersistenceManager: null will be shutdown
2024-04-23T21:41:22,203  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6d078b9 created in the thread with id: 1
2024-04-23T21:41:22,205  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e from thread id: 1
2024-04-23T21:41:22,205  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,205  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,212  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,212  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,213  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6d078b9 will be shutdown
2024-04-23T21:41:22,213  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@49588fc8 created in the thread with id: 1
2024-04-23T21:41:22,215  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,215  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,216  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933682, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:22,221  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:22,258  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933682, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:22,263  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:22,286  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,303  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,304  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:22,312  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:22,339  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:22,344  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:22,356  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,363  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,364  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T21:41:22,365  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T21:41:22,366  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,373  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,373  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:22,373  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:22,374  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:22,380  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,381  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:22,381  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:22,381  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,388  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,388  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:22,388  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:22,388  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:22,399  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:22,427  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T21:41:22,428  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T21:41:22,447  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T21:41:22,448  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T21:41:22,474  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:22,486  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T21:41:22,487  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T21:41:22,528  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T21:41:22,529  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T21:41:22,529  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T21:41:22,529  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,536  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,536  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T21:41:22,539 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed, skipping CBO. 
org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (struct<writeid:bigint,bucketid:int,rowid:bigint>). Possible choices: _FUNC_(bigint)  _FUNC_(boolean)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(struct<writeid:bigint,bucketid:bigint,rowid:int>)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void)  
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:130) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.MethodUtils.getMethodInternal(MethodUtils.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.TimestampCastRestrictorResolver.getEvalMethod(TimestampCastRestrictorResolver.java:68) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:168) ~[classes/:?]
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:149) ~[classes/:?]
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:235) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:584) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory.createFuncCallExpr(ExprNodeDescExprFactory.java:97) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.getFuncExprNodeDescWithUdfData(TypeCheckProcFactory.java:762) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory$DefaultExprProcessor.createConversionCast(TypeCheckProcFactory.java:783) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketColsForUpdateDelete(SemanticAnalyzer.java:8725) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6906) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:7335) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11063) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10938) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11853) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11723) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:626) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12558) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:456) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:67) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:208) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeUpdate(UpdateDeleteSemanticAnalyzer.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyze(UpdateDeleteSemanticAnalyzer.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.analyzeInternal(RewriteSemanticAnalyzer.java:72) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317) ~[classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.parseAndAnalyze(TestUpdateDeleteSemanticAnalyzer.java:290) ~[test-classes/:?]
	at org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.testUpdateAllPartitioned(TestUpdateDeleteSemanticAnalyzer.java:161) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.jar:4.13]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.jar:4.13]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_402]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_402]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_402]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_402]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_402]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_402]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.6.2.jar:5.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211) ~[junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132) [junit-platform-launcher-1.6.2.jar:1.6.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123) [surefire-junit-platform-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-23T21:41:22,540 ERROR [main] parse.UpdateDeleteSemanticAnalyzer: CBO failed due to missing column stats (see previous errors), skipping CBO
2024-04-23T21:41:22,540  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:22,546  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:22,643  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,650  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,650  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:22,751  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:22,752  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:22,752  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:22,752  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:22,752  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:22,752  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:22,752  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 5d33ff5a-311a-40a2-b6d1-9938bc2a97ae
2024-04-23T21:41:22,755  INFO [main] SessionState: Hive Session ID = 5d33ff5a-311a-40a2-b6d1-9938bc2a97ae
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:22,756  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:22,761  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/5d33ff5a-311a-40a2-b6d1-9938bc2a97ae
2024-04-23T21:41:22,763  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/5d33ff5a-311a-40a2-b6d1-9938bc2a97ae
2024-04-23T21:41:22,766  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/5d33ff5a-311a-40a2-b6d1-9938bc2a97ae/_tmp_space.db
2024-04-23T21:41:22,767  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:22,767  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:22,767  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61725c0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@49588fc8 will be shutdown
2024-04-23T21:41:22,767  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:22,767  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -29
2024-04-23T21:41:22,768  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,769  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:22,769  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,769  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b5aaf90, with PersistenceManager: null will be shutdown
2024-04-23T21:41:22,770  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b5aaf90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2d393a72 created in the thread with id: 1
2024-04-23T21:41:22,785  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b5aaf90 from thread id: 1
2024-04-23T21:41:22,786  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,786  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,795  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([14]) with min_open_txn: 1
2024-04-23T21:41:22,795  INFO [main] lockmgr.DbTxnManager: Opened txnid:14
2024-04-23T21:41:22,795  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5d33ff5a-311a-40a2-b6d1-9938bc2a97ae, clientType=HIVECLI]
2024-04-23T21:41:22,795  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:22,796  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:22,796  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b5aaf90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2d393a72 will be shutdown
2024-04-23T21:41:22,796  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:22,796  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -30
2024-04-23T21:41:22,796  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,797  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:22,797  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,798  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112, with PersistenceManager: null will be shutdown
2024-04-23T21:41:22,798  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2d583f20 created in the thread with id: 1
2024-04-23T21:41:22,799  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112 from thread id: 1
2024-04-23T21:41:22,799  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,800  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,805  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:22,805  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:22,806  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2d583f20 will be shutdown
2024-04-23T21:41:22,806  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@70916ae5 created in the thread with id: 1
2024-04-23T21:41:22,808  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:22,808  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:22,808  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933682, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:22,811  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:22,840  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933682, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:22,844  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:22,861  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:22,879  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,880  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:22,891  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:22,914  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:22,920  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:22,933  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T21:41:22,936  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:22,936  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:22,936  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T21:41:22,943  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,943  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:22,943  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:22,944  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:22,950  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:22,951  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:22,951  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:22,962  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T21:41:22,994  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1], allowMissingStats: true
2024-04-23T21:41:22,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T21:41:23,020  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T21:41:23,035  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: b, a
No Stats for default@u, Columns: b, a
2024-04-23T21:41:23,035  INFO [main] SessionState: No Stats for default@u, Columns: b, a
2024-04-23T21:41:23,075  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,075  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,075  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,075  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,082  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,084  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T21:41:23,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T21:41:23,094  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 14
2024-04-23T21:41:23,095  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [14])
2024-04-23T21:41:23,095  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_21-41-22_766_3426556985612256971-1
2024-04-23T21:41:23,101  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table t in query insert into table T select a, b from U
2024-04-23T21:41:23,103  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,110  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,116  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,119  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,132  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,132  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,132  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,138  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/5d33ff5a-311a-40a2-b6d1-9938bc2a97ae/hive_2024-04-23_21-41-23_101_362840898503835521-1/-mr-10000/.hive-staging_hive_2024-04-23_21-41-23_101_362840898503835521-1
2024-04-23T21:41:23,146  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T21:41:23,158  INFO [main] optimizer.ColumnPrunerProcFactory: RS 8 oldColExprMap: {VALUE._col2=Column[_col2], VALUE._col0=Column[_col0], VALUE._col6=Column[_col6], VALUE._col7=Column[_col7], VALUE._col1=Column[_col1], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col8=Column[_col8], VALUE._col3=Column[_col3]}
2024-04-23T21:41:23,158  INFO [main] optimizer.ColumnPrunerProcFactory: RS 8 newColExprMap: {VALUE._col4=Column[_col4], VALUE._col0=Column[_col0], VALUE._col5=Column[_col5], VALUE._col3=Column[_col3], VALUE._col2=Column[_col2], VALUE._col6=Column[_col6], VALUE._col8=Column[_col8], VALUE._col7=Column[_col7], VALUE._col1=Column[_col1]}
2024-04-23T21:41:23,159  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:23,159  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:23,161  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:23,161  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:23,170  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,177  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,189  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:23,190  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:23,190  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T21:41:23,191  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:23,192  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:23,192  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:23,192  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T21:41:23,193  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T21:41:23,195  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:23,196  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:23,196  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T21:41:23,196  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T21:41:23,196  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T21:41:23,217  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-5 is a root stage
  Stage-4 depends on stages: Stage-5
  Stage-6 depends on stages: Stage-4, Stage-7
  Stage-7 depends on stages: Stage-5

STAGE PLANS:
  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            GatherStats: false
            Select Operator
              expressions: a (type: string), b (type: string)
              outputColumnNames: _col0, _col1
              Reduce Output Operator
                bucketingVersion: 1
                key expressions: _col0 (type: string)
                null sort order: a
                numBuckets: -1
                sort order: +
                Map-reduce partition columns: _col0 (type: string)
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false
          Select Operator
            expressions: _col0 (type: string), _col1 (type: string)
            outputColumnNames: a, b
            Group By Operator
              aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
              minReductionHashAggr: 0.99
              mode: hash
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
              File Output Operator
                bucketingVersion: 1
                compressed: false
                GlobalTableId: 0
                directory: file:MASKED-OUT
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      column.name.delimiter ,
                      columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                      columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-4
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: INSERT

  Stage: Stage-6
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.t
          Is Table Level Stats: true

  Stage: Stage-7
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              null sort order: 
              numBuckets: -1
              sort order: 
              tag: -1
              value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
              columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T21:41:23,218  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,224  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,224  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:23,327  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:23,334  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,335  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T21:41:23,455  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T21:41:23,456  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 3e1fb526-f08e-4091-8ee0-033a1a7377db
2024-04-23T21:41:23,458  INFO [main] SessionState: Hive Session ID = 3e1fb526-f08e-4091-8ee0-033a1a7377db
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:23,458  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T21:41:23,464  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/3e1fb526-f08e-4091-8ee0-033a1a7377db
2024-04-23T21:41:23,467  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/3e1fb526-f08e-4091-8ee0-033a1a7377db
2024-04-23T21:41:23,470  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/3e1fb526-f08e-4091-8ee0-033a1a7377db/_tmp_space.db
2024-04-23T21:41:23,471  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T21:41:23,471  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:23,471  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@148a3112, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@70916ae5 will be shutdown
2024-04-23T21:41:23,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:23,472  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -31
2024-04-23T21:41:23,473  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:23,474  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:23,474  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:23,474  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@391dff72, with PersistenceManager: null will be shutdown
2024-04-23T21:41:23,475  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@391dff72, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3b36f8b0 created in the thread with id: 1
2024-04-23T21:41:23,493  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@391dff72 from thread id: 1
2024-04-23T21:41:23,493  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:23,493  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:23,504  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([15]) with min_open_txn: 1
2024-04-23T21:41:23,505  INFO [main] lockmgr.DbTxnManager: Opened txnid:15
2024-04-23T21:41:23,505  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3e1fb526-f08e-4091-8ee0-033a1a7377db, clientType=HIVECLI]
2024-04-23T21:41:23,505  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T21:41:23,506  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T21:41:23,506  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@391dff72, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3b36f8b0 will be shutdown
2024-04-23T21:41:23,506  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T21:41:23,506  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -32
2024-04-23T21:41:23,507  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:23,509  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T21:41:23,509  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:23,509  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f272319, with PersistenceManager: null will be shutdown
2024-04-23T21:41:23,510  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f272319, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2aa2a6e6 created in the thread with id: 1
2024-04-23T21:41:23,511  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f272319 from thread id: 1
2024-04-23T21:41:23,512  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:23,512  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:23,518  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T21:41:23,518  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T21:41:23,519  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f272319, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2aa2a6e6 will be shutdown
2024-04-23T21:41:23,519  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f272319, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@454e6931 created in the thread with id: 1
2024-04-23T21:41:23,521  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T21:41:23,521  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T21:41:23,521  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713933683, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:23,528  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T21:41:23,564  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713933683, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional=true, transactional_properties=default}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T21:41:23,572  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T21:41:23,590  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:23,603  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,605  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:23,615  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T21:41:23,645  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T21:41:23,653  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T21:41:23,665  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T21:41:23,667  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,674  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,674  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T21:41:23,674  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,674  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,674  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,674  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,681  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,681  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T21:41:23,681  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T21:41:23,701  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-23T21:41:23,774  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,774  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,774  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,775  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-23T21:41:23,778  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,778  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,778  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,778  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,785  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,788  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T21:41:23,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T21:41:23,798  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 15
2024-04-23T21:41:23,799  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [15])
2024-04-23T21:41:23,800  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_21-41-23_470_8485987348820984071-1
2024-04-23T21:41:23,805  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table t in query insert into table T values ('abc', 3), ('ghi', null)
2024-04-23T21:41:23,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,815  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,818  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T21:41:23,821  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,829  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,830  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T21:41:23,830  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T21:41:23,835  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/3e1fb526-f08e-4091-8ee0-033a1a7377db/hive_2024-04-23_21-41-23_806_5394935565273218707-1/-mr-10000/.hive-staging_hive_2024-04-23_21-41-23_806_5394935565273218707-1
2024-04-23T21:41:23,843  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T21:41:23,853  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 oldColExprMap: {VALUE._col3=Column[_col3], VALUE._col0=Column[_col0], VALUE._col8=Column[_col8], VALUE._col1=Column[_col1], VALUE._col6=Column[_col6], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col7=Column[_col7], VALUE._col2=Column[_col2]}
2024-04-23T21:41:23,853  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 newColExprMap: {VALUE._col5=Column[_col5], VALUE._col3=Column[_col3], VALUE._col8=Column[_col8], VALUE._col6=Column[_col6], VALUE._col0=Column[_col0], VALUE._col7=Column[_col7], VALUE._col2=Column[_col2], VALUE._col4=Column[_col4], VALUE._col1=Column[_col1]}
2024-04-23T21:41:23,853  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T21:41:23,853  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {KEY.reducesinkkey0=Column[_col0], VALUE._col0=Column[_col1]}
2024-04-23T21:41:23,854  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-23T21:41:23,856  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:23,856  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T21:41:23,867  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T21:41:23,875  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,886  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:23,887  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:23,887  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorization enabled: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorized: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map enabledConditionsNotMet: [Could not enable vectorization due to partition column names size 1 is greater than the number of table column names size 0 IS false]
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.NullRowsInputFormat]
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T21:41:23,888  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T21:41:23,889  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T21:41:23,889  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T21:41:23,889  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T21:41:23,915  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-9 is a root stage
  Stage-8 depends on stages: Stage-9
  Stage-10 depends on stages: Stage-8, Stage-11
  Stage-11 depends on stages: Stage-9

STAGE PLANS:
  Stage: Stage-9
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: _dummy_table
            Row Limit Per Split: 1
            GatherStats: false
            Select Operator
              expressions: array(const struct('abc',3),const struct('ghi',null)) (type: array<struct<col1:string,col2:int>>)
              outputColumnNames: _col0
              UDTF Operator
                function name: inline
                Select Operator
                  expressions: col1 (type: string), CAST( col2 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Reduce Output Operator
                    bucketingVersion: 1
                    key expressions: _col0 (type: string)
                    null sort order: a
                    numBuckets: -1
                    sort order: +
                    Map-reduce partition columns: _col0 (type: string)
                    tag: -1
                    value expressions: _col1 (type: string)
                    auto parallelism: false
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: dummy_path
            input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns 
              columns.types 
              file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              location file:MASKED-OUT
              name _dummy_database._dummy_table
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
            serde: org.apache.hadoop.hive.serde2.NullStructSerDe
          
              input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucketing_version 2
                column.name.delimiter ,
                columns 
                columns.comments 
                columns.types 
                file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:MASKED-OUT
                name _dummy_database._dummy_table
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
              serde: org.apache.hadoop.hive.serde2.NullStructSerDe
              name: _dummy_database._dummy_table
            name: _dummy_database._dummy_table
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false
          Select Operator
            expressions: _col0 (type: string), _col1 (type: string)
            outputColumnNames: a, b
            Group By Operator
              aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
              minReductionHashAggr: 0.99
              mode: hash
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
              File Output Operator
                bucketingVersion: 1
                compressed: false
                GlobalTableId: 0
                directory: file:MASKED-OUT
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      column.name.delimiter ,
                      columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                      columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-8
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: INSERT

  Stage: Stage-10
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.t
          Is Table Level Stats: true

  Stage: Stage-11
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              null sort order: 
              numBuckets: -1
              sort order: 
              tag: -1
              value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
              columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T21:41:23,916  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T21:41:23,927  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:23,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T21:41:24,032  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T21:41:24,039  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T21:41:24,040  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T21:41:24,330  INFO [pool-2-thread-1] lockmgr.DbTxnManager: Shutting down Heartbeater thread pool.
