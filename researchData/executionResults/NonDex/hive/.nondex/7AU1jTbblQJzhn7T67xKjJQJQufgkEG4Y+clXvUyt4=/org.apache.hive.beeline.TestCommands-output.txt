SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,077191 seconds to load 250 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@6ca18a14]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@6ca18a14) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@205d38da
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,032860 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 132 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/beeline/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/beeline/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={DRFA, console}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(PatternSelector=null, noConsoleNoAnsi="null", alwaysWriteExceptions="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", disableAnsi="null", header="null", Configuration(HiveLog4j2Test), Replace=null, footer="null", charset="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(direct="null", follow="null", target="SYSTEM_ERR", immediateFlush="null", bufferedIo="null", bufferSize="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), ignoreExceptions="null", ={}, Filter=null)
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", footer="null", charset="null", alwaysWriteExceptions="null", PatternSelector=null, Configuration(HiveLog4j2Test), disableAnsi="null", noConsoleNoAnsi="null", Replace=null, header="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(modulate="true", maxRandomDelay="null", interval="1")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(tempCompressedFilePattern="null", max="30", ={}, fileIndex="null", min="null", Configuration(HiveLog4j2Test), compressionLevel="null", stopCustomActionsOnError="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(append="null", filePattern="/home/alex/Repositories/hive/beeline/target/tmp/log/hive.log.%d{yyyy-MM-dd}", fileGroup="null", filePermissions="null", fileOwner="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), advertise="null", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertiseURI="null", fileName="/home/alex/Repositories/hive/beeline/target/tmp/log/hive.log", bufferedIo="null", bufferSize="null", immediateFlush="null", Configuration(HiveLog4j2Test), PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", ignoreExceptions="null", ={}, Filter=null)
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/beeline/target/tmp/log/hive.log seek to 11750072
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T08:14:52.358-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-08:15:00.301, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-08:15:00.302, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@342c38f8...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@342c38f8 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@1f2f9244
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@6ca18a14
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/beeline/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@6ca18a14) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@6ca18a14] started OK.
Connecting to jdbc:hive2://
2024-04-24T08:15:00,712  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/beeline/target/test-classes/hive-site.xml
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-24T08:15:01,063  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T08:15:01,146  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
2024-04-24T08:15:01,147  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:15:01,148  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
Hive Session ID = c737a8bc-15ac-41a1-b63d-6dcacebb866b
2024-04-24T08:15:01,195  INFO [main] SessionState: Hive Session ID = c737a8bc-15ac-41a1-b63d-6dcacebb866b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:01,211  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:01,533  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/c737a8bc-15ac-41a1-b63d-6dcacebb866b
2024-04-24T08:15:01,536  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/c737a8bc-15ac-41a1-b63d-6dcacebb866b
2024-04-24T08:15:01,539  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/c737a8bc-15ac-41a1-b63d-6dcacebb866b/_tmp_space.db
2024-04-24T08:15:01,564  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c737a8bc-15ac-41a1-b63d-6dcacebb866b, clientType=HIVESERVER2]
2024-04-24T08:15:01,671  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:15:01,823  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:01,860  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:15:01,867  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T08:15:01,867  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T08:15:01,887  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:15:01,892  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T08:15:02,202  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:15:02,206  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T08:15:02,750  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T08:15:02,750  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: null will be shutdown
2024-04-24T08:15:02,773  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56637cff created in the thread with id: 1
2024-04-24T08:15:04,695  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T08:15:04,695  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T08:15:04,695  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae from thread id: 1
2024-04-24T08:15:04,773  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T08:15:04,776  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T08:15:04,801  INFO [main] metastore.HMSHandler: No user is added in admin role, since config is empty
2024-04-24T08:15:04,808  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T08:15:04,829  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:15:04,832  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T08:15:04,834  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:15:04,835  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T08:15:04,837  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:15:04,840  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T08:15:04,841  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:15:04,842  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T08:15:04,847  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T08:15:04,848  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T08:15:04,849  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T08:15:04,850  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T08:15:04,851  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T08:15:04,853  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:15:05,007  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:15:05,934  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,936  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,936  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,937  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,941  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,951  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:05,954  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:15:06,039  INFO [main] service.CompositeService: Operation log root directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs
2024-04-24T08:15:06,041  INFO [main] service.CompositeService: HiveServer2: Background operation thread pool size: 100
2024-04-24T08:15:06,041  INFO [main] service.CompositeService: HiveServer2: Background operation thread wait queue size: 100
2024-04-24T08:15:06,041  INFO [main] service.CompositeService: HiveServer2: Background operation thread keepalive time: 10 seconds
2024-04-24T08:15:06,043  INFO [main] service.CompositeService: Connections limit are user: 0 ipaddress: 0 user-ipaddress: 0
2024-04-24T08:15:06,046  INFO [main] cleanup.EventualCleanupService: EventualCleanupService started with 10 threads and queue of size 10000
2024-04-24T08:15:06,053  INFO [main] service.AbstractService: Service:OperationManager is inited.
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.NullAppender].
DEBUG StatusLogger createNullAppender()
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-2
DEBUG StatusLogger Log4j2 ConfigurationScheduler starting 2 threads
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.NullAppender].
DEBUG StatusLogger createNullAppender()
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-3
2024-04-24T08:15:06,068  INFO [main] service.AbstractService: Service:SessionManager is inited.
2024-04-24T08:15:06,069  INFO [main] service.AbstractService: Service:CLIService is inited.
2024-04-24T08:15:06,069  INFO [main] service.AbstractService: Service:OperationManager is started.
2024-04-24T08:15:06,069  INFO [main] service.AbstractService: Service:SessionManager is started.
2024-04-24T08:15:06,070  INFO [main] service.AbstractService: Service:CLIService is started.
2024-04-24T08:15:06,071  INFO [main] service.AbstractService: Service:ThriftBinaryCLIService is inited.
2024-04-24T08:15:06,073  INFO [main] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V10
2024-04-24T08:15:06,078  INFO [main] thrift.ThriftCLIService: Creating Hive session handle for user [null] from IP null
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:06,085  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:06,096  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:06,100  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:06,104  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/_tmp_space.db
2024-04-24T08:15:06,104  INFO [main] session.SessionState: Reloading auxiliary JAR files
2024-04-24T08:15:06,104  WARN [main] session.SessionState: Configuration hive.reloadable.aux.jars.path not specified
2024-04-24T08:15:06,106  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:15:06,107  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:15:06,109  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56637cff will be shutdown
2024-04-24T08:15:06,110  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35e98af created in the thread with id: 1
2024-04-24T08:15:06,115  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:15:06,115  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:15:06,117  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T08:15:06,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:15:06,143  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35e98af will be shutdown
2024-04-24T08:15:06,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:15:06,143  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T08:15:06,145  INFO [main] session.HiveSessionImpl: Operation log session directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:06,149  INFO [main] service.CompositeService: Session opened, SessionHandle [9ca3dd31-965f-4584-a8ad-7e4e63d11d72], current sessions:1
2024-04-24T08:15:06,153  INFO [main] thrift.ThriftCLIService: Login attempt is successful for user : null
Connected to: Apache Hive (version 4.0.0-SNAPSHOT)
Error: Couldn't load manifest attributes. (state=,code=0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
2024-04-24T08:15:06,231  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] session.HiveSessionImpl: executing select 3
2024-04-24T08:15:06,245  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] operation.OperationManager: Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a72a991d-d5ca-4dd9-88e7-861462d3388c] SessionHandle [9ca3dd31-965f-4584-a8ad-7e4e63d11d72]
2024-04-24T08:15:06,249  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] common.LogUtils: Thread context registration is done.
DEBUG StatusLogger Building Plugin[name=filter, class=org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter].
DEBUG StatusLogger createFilter(loggingLevel="EXECUTION")
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(disableAnsi="null", Replace=null, footer="null", noConsoleNoAnsi="null", PatternSelector=null, alwaysWriteExceptions="null", header="null", pattern="%-5p : %m%n", Configuration(HiveLog4j2Test), charset="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348", append="null", name="query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), NameFilter(org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter with name NameFilter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348
DEBUG StatusLogger Building Plugin[name=, class=org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter].
DEBUG StatusLogger createFilter()
DEBUG StatusLogger Building Plugin[name=, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(footer="null", Configuration(HiveLog4j2Test), charset="null", header="null", disableAnsi="null", alwaysWriteExceptions="null", Replace=null, PatternSelector=null, noConsoleNoAnsi="null", pattern="%-5p : %m%n")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348.test", append="null", name="test-query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), test-filter(org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter with name test-filter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348.test
2024-04-24T08:15:06,265  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] operation.SQLOperation: [opType=EXECUTE_STATEMENT, queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348, startTime=1713971706240, sessionId=9ca3dd31-965f-4584-a8ad-7e4e63d11d72, createTime=1713971706081, userName=null, ipAddress=null]
2024-04-24T08:15:06,351  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] ql.Driver: Compiling command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348): select 3
2024-04-24T08:15:06,390  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:15:06,391  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
2024-04-24T08:15:06,391  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:15:06,394  INFO [Metastore-RuntimeStats-Loader-1] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:15:06,396  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:06,396  INFO [Metastore-RuntimeStats-Loader-1] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:15:06,398  INFO [Metastore-RuntimeStats-Loader-1] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6b4ad267, with PersistenceManager: null will be shutdown
2024-04-24T08:15:06,398  INFO [Metastore-RuntimeStats-Loader-1] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6b4ad267, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@783baf3 created in the thread with id: 86
2024-04-24T08:15:06,407  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6b4ad267 from thread id: 86
2024-04-24T08:15:06,407  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:15:06,408  INFO [Metastore-RuntimeStats-Loader-1] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:15:06,409  INFO [Metastore-RuntimeStats-Loader-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_runtime_stats	
2024-04-24T08:15:07,160  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:07,162  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: null will be shutdown
2024-04-24T08:15:07,162  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@361cd35c created in the thread with id: 1
2024-04-24T08:15:07,167  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211 from thread id: 1
2024-04-24T08:15:07,432  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] reflections.Reflections: Reflections took 227 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:15:07,646  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] reflections.Reflections: Reflections took 154 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:15:07,810  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] reflections.Reflections: Reflections took 157 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:15:07,893  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Starting caching scope for: alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348
2024-04-24T08:15:07,896  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:15:07,896  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9ca3dd31-965f-4584-a8ad-7e4e63d11d72, clientType=HIVESERVER2]
2024-04-24T08:15:07,900  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-24T08:15:07,900  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T08:15:07,900  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T08:15:07,906  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T08:15:07,916  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-24T08:15:09,405  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-24T08:15:10,329  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T08:15:10,332  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-24T08:15:10,349  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T08:15:10,349  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T08:15:10,405  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/hive_2024-04-24_08-15-06_295_5472339636628871245-1/-mr-10001/.hive-staging_hive_2024-04-24_08-15-06_295_5472339636628871245-1
2024-04-24T08:15:10,454  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-24T08:15:10,554  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-24T08:15:10,577  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Completed plan generation
2024-04-24T08:15:10,577  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] parse.CalcitePlanner: Ending caching scope for: alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348
2024-04-24T08:15:10,577  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:15:10,579  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
2024-04-24T08:15:10,600  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-04-24T08:15:10,602  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-04-24T08:15:10,606  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.SelectOperator: SELECT null
2024-04-24T08:15:10,607  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-04-24T08:15:10,612  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:15:10,613  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=9, getAllFunctions_()=25, isCompatibleWith_(Configuration)=0, getAllTableConstraints_(AllTableConstraintsRequest)=137}
2024-04-24T08:15:10,614  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] ql.Driver: Completed compiling command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348); Time taken: 4.263 seconds
2024-04-24T08:15:10,616  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] common.LogUtils: Unregistered logging context.
2024-04-24T08:15:10,618  INFO [HiveServer2-Background-Pool: Thread-93] common.LogUtils: Thread context registration is done.
2024-04-24T08:15:10,618  INFO [HiveServer2-Background-Pool: Thread-93] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:15:10,619  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:15:10,624  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Executing command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348): select 3
2024-04-24T08:15:10,629  INFO [HiveServer2-Background-Pool: Thread-93] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:15:10,629  INFO [HiveServer2-Background-Pool: Thread-93] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:15:10,629  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Completed executing command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348); Time taken: 0.006 seconds
2024-04-24T08:15:10,630  INFO [HiveServer2-Background-Pool: Thread-93] common.LogUtils: Unregistered logging context.
INFO  : Compiling command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348): select 3
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348); Time taken: 4.263 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348): select 3
INFO  : Completed executing command(queryId=alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348); Time taken: 0.006 seconds
DEBUG : Shutting down query select 3
2024-04-24T08:15:10,654  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] operation.OperationManager: Closing operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a72a991d-d5ca-4dd9-88e7-861462d3388c]
2024-04-24T08:15:10,654  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] operation.OperationManager: Removed queryId: alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348 corresponding to operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=a72a991d-d5ca-4dd9-88e7-861462d3388c] with tag: null
2024-04-24T08:15:10,656  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] ql.Context: Deleting scratch dir: file:/home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/hive_2024-04-24_08-15-06_295_5472339636628871245-1
2024-04-24T08:15:10,656  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] cleanup.EventualCleanupService: Delete file:/home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/hive_2024-04-24_08-15-06_295_5472339636628871245-1 operation was queued
2024-04-24T08:15:10,656  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.TableScanOperator: Closing Operator: TS[0]
2024-04-24T08:15:10,657  INFO [EventualCleanupService thread 0] cleanup.EventualCleanupService: Deleted file:/home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/hive_2024-04-24_08-15-06_295_5472339636628871245-1
2024-04-24T08:15:10,657  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.TableScanOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_TS_0:0, 
2024-04-24T08:15:10,657  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.SelectOperator: Closing Operator: SEL[1]
2024-04-24T08:15:10,658  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.SelectOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_SEL_1:0, 
2024-04-24T08:15:10,658  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-04-24T08:15:10,658  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] exec.ListSinkOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_LIST_SINK_3:0, 
2024-04-24T08:15:10,658  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] operation.SQLOperation: Closing operation log /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72/alex_20240424081506_95c5b1a1-5370-4a91-b636-6a9110ac5348 without delay
Error: Failed to wait for operation to complete (state=08S01,code=0)
Closing: 0: jdbc:hive2://
2024-04-24T08:15:10,661  INFO [main] service.CompositeService: Session closed, SessionHandle [9ca3dd31-965f-4584-a8ad-7e4e63d11d72], current sessions:0
2024-04-24T08:15:10,662  INFO [9ca3dd31-965f-4584-a8ad-7e4e63d11d72 main] session.HiveSessionImpl: Operation log session directory is deleted: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:10,663  INFO [main] cleanup.EventualCleanupService: Delete /tmp/hive/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72 operation was queued
2024-04-24T08:15:10,663  INFO [main] cleanup.EventualCleanupService: Delete /home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72 operation was queued
2024-04-24T08:15:10,664  INFO [EventualCleanupService thread 1] cleanup.EventualCleanupService: Deleted /tmp/hive/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:10,664  INFO [EventualCleanupService thread 2] cleanup.EventualCleanupService: Deleted /home/alex/Repositories/hive/beeline/target/tmp/alex/9ca3dd31-965f-4584-a8ad-7e4e63d11d72
2024-04-24T08:15:10,672  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:15:10,672  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@361cd35c will be shutdown
2024-04-24T08:15:10,672  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:15:10,672  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
Connecting to jdbc:hive2://
2024-04-24T08:15:10,762  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
2024-04-24T08:15:10,762  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:15:10,762  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
Hive Session ID = e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b
2024-04-24T08:15:10,764  INFO [main] SessionState: Hive Session ID = e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:10,764  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:10,775  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b
2024-04-24T08:15:10,779  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b
2024-04-24T08:15:10,783  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b/_tmp_space.db
2024-04-24T08:15:10,784  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e98b42d1-d6a7-4b2a-a7e0-955d2c224f8b, clientType=HIVESERVER2]
2024-04-24T08:15:10,785  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:15:10,786  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:10,786  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:15:10,787  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: null will be shutdown
2024-04-24T08:15:10,787  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5a8b42a3 created in the thread with id: 1
2024-04-24T08:15:10,792  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2 from thread id: 1
2024-04-24T08:15:10,793  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:15:10,793  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:15:10,794  INFO [main] service.CompositeService: Operation log root directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs
2024-04-24T08:15:10,794  INFO [main] service.CompositeService: HiveServer2: Background operation thread pool size: 100
2024-04-24T08:15:10,794  INFO [main] service.CompositeService: HiveServer2: Background operation thread wait queue size: 100
2024-04-24T08:15:10,794  INFO [main] service.CompositeService: HiveServer2: Background operation thread keepalive time: 10 seconds
2024-04-24T08:15:10,794  INFO [main] service.CompositeService: Connections limit are user: 0 ipaddress: 0 user-ipaddress: 0
2024-04-24T08:15:10,795  INFO [main] cleanup.EventualCleanupService: EventualCleanupService started with 10 threads and queue of size 10000
ERROR StatusLogger Log4j2 ConfigurationScheduler attempted to increment scheduled items after start
ERROR StatusLogger Log4j2 ConfigurationScheduler attempted to increment scheduled items after start
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:OperationManager is inited.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:SessionManager is inited.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:CLIService is inited.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:OperationManager is started.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:SessionManager is started.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:CLIService is started.
2024-04-24T08:15:10,798  INFO [main] service.AbstractService: Service:ThriftBinaryCLIService is inited.
2024-04-24T08:15:10,798  INFO [main] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V10
2024-04-24T08:15:10,798  INFO [main] thrift.ThriftCLIService: Creating Hive session handle for user [null] from IP null
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:10,800  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:15:10,808  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:10,811  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:10,815  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e2978039-7bb4-4871-a325-df26553e10f6/_tmp_space.db
2024-04-24T08:15:10,815  INFO [main] session.SessionState: Reloading auxiliary JAR files
2024-04-24T08:15:10,815  WARN [main] session.SessionState: Configuration hive.reloadable.aux.jars.path not specified
2024-04-24T08:15:10,815  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:15:10,815  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5a8b42a3 will be shutdown
2024-04-24T08:15:10,815  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:15:10,816  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T08:15:10,816  INFO [main] session.HiveSessionImpl: Operation log session directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:10,816  INFO [main] service.CompositeService: Session opened, SessionHandle [e2978039-7bb4-4871-a325-df26553e10f6], current sessions:1
2024-04-24T08:15:10,816  INFO [main] thrift.ThriftCLIService: Login attempt is successful for user : null
Connected to: Apache Hive (version 4.0.0-SNAPSHOT)
Error: Couldn't load manifest attributes. (state=,code=0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
2024-04-24T08:15:10,819  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] session.HiveSessionImpl: executing create table t1(x int)
2024-04-24T08:15:10,821  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] operation.OperationManager: Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=97be1141-7a8e-42cb-97f9-e11b83777ccc] SessionHandle [e2978039-7bb4-4871-a325-df26553e10f6]
2024-04-24T08:15:10,821  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] common.LogUtils: Thread context registration is done.
DEBUG StatusLogger Building Plugin[name=filter, class=org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter].
DEBUG StatusLogger createFilter(loggingLevel="EXECUTION")
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(Replace=null, charset="null", PatternSelector=null, Configuration(HiveLog4j2Test), alwaysWriteExceptions="null", footer="null", disableAnsi="null", pattern="%-5p : %m%n", noConsoleNoAnsi="null", header="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6/alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625", append="null", name="query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), NameFilter(org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter with name NameFilter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6/alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625
DEBUG StatusLogger Building Plugin[name=, class=org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter].
DEBUG StatusLogger createFilter()
DEBUG StatusLogger Building Plugin[name=, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(Configuration(HiveLog4j2Test), Replace=null, charset="null", noConsoleNoAnsi="null", header="null", alwaysWriteExceptions="null", disableAnsi="null", PatternSelector=null, footer="null", pattern="%-5p : %m%n")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6/alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625.test", append="null", name="test-query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), test-filter(org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter with name test-filter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6/alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625.test
2024-04-24T08:15:10,830  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] operation.SQLOperation: [opType=EXECUTE_STATEMENT, queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625, startTime=1713971710820, sessionId=e2978039-7bb4-4871-a325-df26553e10f6, createTime=1713971710798, userName=null, ipAddress=null]
2024-04-24T08:15:10,832  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] ql.Driver: Compiling command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625): create table t1(x int)
2024-04-24T08:15:10,835  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:15:10,836  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:10,836  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:15:10,837  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: null will be shutdown
2024-04-24T08:15:10,838  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a69014e created in the thread with id: 1
2024-04-24T08:15:10,842  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa from thread id: 1
2024-04-24T08:15:10,842  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:15:10,843  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:15:10,843  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] parse.CalcitePlanner: Starting caching scope for: alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625
2024-04-24T08:15:10,843  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:15:10,844  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e2978039-7bb4-4871-a325-df26553e10f6, clientType=HIVESERVER2]
2024-04-24T08:15:10,844  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] parse.CalcitePlanner: Creating table default.t1 position=13
2024-04-24T08:15:10,849  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:15:11,041  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] reflections.Reflections: Reflections took 185 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:15:11,170  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] parse.CalcitePlanner: Ending caching scope for: alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625
2024-04-24T08:15:11,170  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:15:11,170  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:15:11,170  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:15:11,171  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=5, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T08:15:11,171  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] ql.Driver: Completed compiling command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625); Time taken: 0.338 seconds
2024-04-24T08:15:11,173  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] common.LogUtils: Unregistered logging context.
2024-04-24T08:15:11,173  INFO [HiveServer2-Background-Pool: Thread-115] common.LogUtils: Thread context registration is done.
2024-04-24T08:15:11,174  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:15:11,174  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:15:11,174  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Executing command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625): create table t1(x int)
2024-04-24T08:15:11,178  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:15:11,276  INFO [HiveServer2-Background-Pool: Thread-115] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:t1, dbName:default, owner:alex, createTime:1713971711, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:x, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"x":"true"}}, numFilesErasureCoded=0, numFiles=0, rawDataSize=0, totalSize=0, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:15:11,278  INFO [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:15:11,279  INFO [HiveServer2-Background-Pool: Thread-115] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2078b5cc, with PersistenceManager: null will be shutdown
2024-04-24T08:15:11,279  INFO [HiveServer2-Background-Pool: Thread-115] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2078b5cc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@38c1dd7e created in the thread with id: 115
2024-04-24T08:15:11,285  INFO [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2078b5cc from thread id: 115
2024-04-24T08:15:11,460  WARN [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
2024-04-24T08:15:11,462 ERROR [HiveServer2-Background-Pool: Thread-115] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy38.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233)
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2024-04-24T08:15:11,477 ERROR [HiveServer2-Background-Pool: Thread-115] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
2024-04-24T08:15:11,478 ERROR [HiveServer2-Background-Pool: Thread-115] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
2024-04-24T08:15:11,481  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T08:15:11,481  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.t1 already exists) retryPossible: false
2024-04-24T08:15:11,481  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.t1 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
2024-04-24T08:15:11,481 ERROR [HiveServer2-Background-Pool: Thread-115] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
2024-04-24T08:15:11,482  INFO [HiveServer2-Background-Pool: Thread-115] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:15:11,482  INFO [HiveServer2-Background-Pool: Thread-115] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T08:15:11,482  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Completed executing command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625); Time taken: 0.308 seconds
2024-04-24T08:15:11,486 ERROR [HiveServer2-Background-Pool: Thread-115] operation.SQLOperation: Error running hive query
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:246) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) ~[classes/:?]
	... 11 more
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) ~[classes/:?]
	... 11 more
2024-04-24T08:15:11,487  INFO [HiveServer2-Background-Pool: Thread-115] common.LogUtils: Unregistered logging context.
INFO  : Compiling command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625): create table t1(x int)
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625); Time taken: 0.338 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625): create table t1(x int)
INFO  : Starting task [Stage-0:DDL] in serial mode
DEBUG : Task getting executed using mapred tag : alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625,userid=null
ERROR : Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
ERROR : DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
ERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
DEBUG : Shutting down query create table t1(x int)
INFO  : Completed executing command(queryId=alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625); Time taken: 0.308 seconds
DEBUG : Shutting down query create table t1(x int)
2024-04-24T08:15:11,496  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] operation.OperationManager: Closing operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=97be1141-7a8e-42cb-97f9-e11b83777ccc]
2024-04-24T08:15:11,496  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] operation.OperationManager: Removed queryId: alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625 corresponding to operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=97be1141-7a8e-42cb-97f9-e11b83777ccc] with tag: null
2024-04-24T08:15:11,496  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] operation.SQLOperation: Closing operation log /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6/alex_20240424081510_07cc1123-1d4d-4e0a-9b87-a6a2e8636625 without delay
Error: Failed to wait for operation to complete (state=08S01,code=0)
Closing: 0: jdbc:hive2://
2024-04-24T08:15:11,497  INFO [main] service.CompositeService: Session closed, SessionHandle [e2978039-7bb4-4871-a325-df26553e10f6], current sessions:0
2024-04-24T08:15:11,498  INFO [e2978039-7bb4-4871-a325-df26553e10f6 main] session.HiveSessionImpl: Operation log session directory is deleted: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:11,499  INFO [main] cleanup.EventualCleanupService: Delete /tmp/hive/alex/e2978039-7bb4-4871-a325-df26553e10f6 operation was queued
2024-04-24T08:15:11,499  INFO [main] cleanup.EventualCleanupService: Delete /home/alex/Repositories/hive/beeline/target/tmp/alex/e2978039-7bb4-4871-a325-df26553e10f6 operation was queued
2024-04-24T08:15:11,518  INFO [EventualCleanupService thread 1] cleanup.EventualCleanupService: Deleted /home/alex/Repositories/hive/beeline/target/tmp/alex/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:11,518  INFO [EventualCleanupService thread 0] cleanup.EventualCleanupService: Deleted /tmp/hive/alex/e2978039-7bb4-4871-a325-df26553e10f6
2024-04-24T08:15:11,522  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:15:11,522  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a69014e will be shutdown
2024-04-24T08:15:11,522  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:15:11,522  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
