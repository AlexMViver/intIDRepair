SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,073354 seconds to load 250 plugins from sun.misc.Launcher$AppClassLoader@7f31245a
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@5149d738]...
DEBUG StatusLogger Reconfiguration started for context[name=7f31245a] at URI null (org.apache.logging.log4j.core.LoggerContext@5149d738) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@1750fbeb
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@5c80cf32
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,018339 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 132 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/ql/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/ql/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", modulate="true", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(max="30", min="null", fileIndex="null", compressionLevel="null", ={}, stopCustomActionsOnError="null", tempCompressedFilePattern="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileName="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log", filePattern="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertise="null", advertiseURI="null", filePermissions="null", fileOwner="null", fileGroup="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/ql/target/tmp/log/hive.log seek to 1574563258
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/ql/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-23T17:46:03.132-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/24-00:00:00.000, nextFileTime=2024/04/23-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/23-17:46:04.834, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/24-00:00:00.000, nextFileTime=2024/04/23-00:00:00.000, prevFileTime=2024/04/23-00:00:00.000, current=2024/04/23-17:46:04.835, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@5c80cf32 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@5c80cf32
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@5c80cf32 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@6200f9cb...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@6200f9cb OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@3e821657
TRACE StatusLogger Reregistering context (1/1): '7f31245a' org.apache.logging.log4j.core.LoggerContext@5149d738
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=7f31245a] at URI /home/alex/Repositories/hive/ql/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@5149d738) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@5149d738] started OK.
2024-04-23T17:46:04,923  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/ql/target/testconf/hive-site.xml
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-23T17:46:05,314  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-23T17:46:05,378  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:05,379  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:05,379  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:05,379  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:05,380  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:05,380  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:05,381  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:05,381  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:05,381  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:05,381  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:05,382  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:05,388  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
2024-04-23T17:46:06,135  INFO [main] utils.TestTxnDbUtil: Reinitializing the metastore db with hive-schema-4.0.0.derby.sql on the database jdbc:derby:memory:/home/alex/Repositories/hive/ql/target/tmp/junit_metastore_db;create=true
Hive Session ID = 9f3b93a3-9e46-4199-a75a-e71d142b97f0
2024-04-23T17:46:06,883  INFO [main] SessionState: Hive Session ID = 9f3b93a3-9e46-4199-a75a-e71d142b97f0
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:06,894  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
log4j: Trying to find [log4j.xml] using context classloader org.apache.hadoop.hive.ql.exec.UDFClassLoader@6f94fb9d.
log4j: Trying to find [log4j.xml] using sun.misc.Launcher$AppClassLoader@7f31245a class loader.
log4j: Trying to find [log4j.xml] using ClassLoader.getSystemResource().
log4j: Trying to find [log4j.properties] using context classloader org.apache.hadoop.hive.ql.exec.UDFClassLoader@6f94fb9d.
log4j: Trying to find [log4j.properties] using sun.misc.Launcher$AppClassLoader@7f31245a class loader.
log4j: Trying to find [log4j.properties] using ClassLoader.getSystemResource().
log4j: Could not find resource: [null].
log4j:WARN No appenders could be found for logger (org.apache.htrace.core.Tracer).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2024-04-23T17:46:07,218  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/9f3b93a3-9e46-4199-a75a-e71d142b97f0
2024-04-23T17:46:07,221  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/9f3b93a3-9e46-4199-a75a-e71d142b97f0
2024-04-23T17:46:07,225  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/9f3b93a3-9e46-4199-a75a-e71d142b97f0/_tmp_space.db
2024-04-23T17:46:08,077  INFO [main] reflections.Reflections: Reflections took 140 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T17:46:08,186  INFO [main] reflections.Reflections: Reflections took 75 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T17:46:08,260  INFO [main] reflections.Reflections: Reflections took 68 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-23T17:46:08,787  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,790  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,793  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,794  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,794  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,795  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,796  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-23T17:46:08,852  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:08,986  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:09,012  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:09,015  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-23T17:46:09,015  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-23T17:46:09,027  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T17:46:09,031  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-23T17:46:09,047  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T17:46:09,051  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-23T17:46:09,420  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-23T17:46:09,420  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4dc52559, with PersistenceManager: null will be shutdown
2024-04-23T17:46:09,447  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4dc52559, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@8641b7d created in the thread with id: 1
2024-04-23T17:46:11,372  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4dc52559 from thread id: 1
2024-04-23T17:46:11,385  INFO [main] metastore.HMSHandler: Setting location of default catalog, as it hasn't been done after upgrade
2024-04-23T17:46:11,416  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-23T17:46:11,462  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-23T17:46:11,477  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-23T17:46:11,479  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-23T17:46:11,548  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-23T17:46:11,555  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-23T17:46:11,556  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-23T17:46:11,559  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-23T17:46:11,583  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T17:46:11,587  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-23T17:46:11,589  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T17:46:11,600  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-23T17:46:11,602  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-23T17:46:11,604  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-23T17:46:11,606  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-23T17:46:11,606  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-23T17:46:11,609  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-23T17:46:11,610  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-23T17:46:11,611  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-23T17:46:11,613  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:11,724  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:11,745  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-23T17:46:11,810  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([1]) with min_open_txn: 1
2024-04-23T17:46:11,825  INFO [main] lockmgr.DbTxnManager: Opened txnid:1
2024-04-23T17:46:11,849  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9f3b93a3-9e46-4199-a75a-e71d142b97f0, clientType=HIVECLI]
2024-04-23T17:46:11,851  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:11,853  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:11,853  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4dc52559, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@8641b7d will be shutdown
2024-04-23T17:46:11,853  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:11,853  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-23T17:46:11,855  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:11,856  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:11,856  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:11,857  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301, with PersistenceManager: null will be shutdown
2024-04-23T17:46:11,858  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@18db3b3c created in the thread with id: 1
2024-04-23T17:46:11,863  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301 from thread id: 1
2024-04-23T17:46:11,863  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:11,863  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:11,920  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:11,920  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:11,921  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@18db3b3c will be shutdown
2024-04-23T17:46:11,922  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7db47323 created in the thread with id: 1
2024-04-23T17:46:11,926  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:11,927  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:11,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919571, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:11,956  INFO [main] utils.MetaStoreServerUtils: Updating table stats for T
2024-04-23T17:46:11,957  INFO [main] utils.MetaStoreServerUtils: Updated size of table T to 0
2024-04-23T17:46:12,033  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919572, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{totalSize=0, transactional_properties=default, numFilesErasureCoded=0, transactional=true, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:12,037  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:12,057  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:12,093  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:12,097  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:12,111  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:12,151  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:12,156  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:12,171  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:12,178  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:12,180  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update T set b = 5 where b > 5> as 
<insert into table `default`.`T` select ROW__ID,`a`,`b` from `default`.`T` sort by ROW__ID >
2024-04-23T17:46:12,184  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:12,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:12,198  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:12,200  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:12,201  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:12,202  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:12,210  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:12,213  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:12,217  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:12,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:12,226  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:12,227  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:12,227  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:13,318  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T17:46:13,434  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T17:46:13,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T17:46:13,849  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0, 1], allowMissingStats: true
2024-04-23T17:46:13,857  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T17:46:13,875  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a, b
No Stats for default@t, Columns: a, b
2024-04-23T17:46:13,875  INFO [main] SessionState: No Stats for default@t, Columns: a, b
2024-04-23T17:46:13,993  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:13,994  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:13,994  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:13,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:14,004  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:14,091  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 1
2024-04-23T17:46:14,092  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [1])
2024-04-23T17:46:14,098  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-12_179_7926664944593310043-1
2024-04-23T17:46:14,110  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:14,183  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,184  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,184  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,184  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,184  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,184  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:14,220  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:14,307  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:14,315  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:14,344  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:14,346  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:14,347  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:14,369  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:14,369  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:14,369  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:14,370  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:14,376  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:14,376  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:14,388  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:14,391  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:14,447  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t
            filterExpr: (UDFToDouble(b) > 5.0D) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(b) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string)
                outputColumnNames: _col0, _col1
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  value expressions: _col1 (type: string)
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: t
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            properties:
              bucket_count 2
              bucket_field_name a
              column.name.delimiter ,
              columns a,b
              columns.types string:string
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.t
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              transactional true
              transactional_properties default
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
            name: default.t
      Truncated Path -> Alias:
        /t [t]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string)
          outputColumnNames: _col0, _col1, _col2
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1, _col2
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: UPDATE

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:14,448  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:14,455  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:14,456  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:14,654  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:14,662  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:14,662  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:14,862  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:14,862  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:14,862  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:14,863  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:14,863  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:14,863  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:14,863  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:14,863  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:14,864  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:14,864  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:14,864  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:14,865  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 0cf7990d-8008-4bd3-93ee-4b836271f76c
2024-04-23T17:46:14,868  INFO [main] SessionState: Hive Session ID = 0cf7990d-8008-4bd3-93ee-4b836271f76c
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:14,869  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:14,877  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/0cf7990d-8008-4bd3-93ee-4b836271f76c
2024-04-23T17:46:14,880  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/0cf7990d-8008-4bd3-93ee-4b836271f76c
2024-04-23T17:46:14,883  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/0cf7990d-8008-4bd3-93ee-4b836271f76c/_tmp_space.db
2024-04-23T17:46:14,884  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:14,885  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:14,885  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@71fb8301, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7db47323 will be shutdown
2024-04-23T17:46:14,885  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:14,885  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-23T17:46:14,887  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:14,888  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:14,888  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:14,889  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28f6a008, with PersistenceManager: null will be shutdown
2024-04-23T17:46:14,889  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28f6a008, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7fa86ddd created in the thread with id: 1
2024-04-23T17:46:14,914  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28f6a008 from thread id: 1
2024-04-23T17:46:14,915  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:14,915  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:14,926  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([2]) with min_open_txn: 1
2024-04-23T17:46:14,926  INFO [main] lockmgr.DbTxnManager: Opened txnid:2
2024-04-23T17:46:14,927  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0cf7990d-8008-4bd3-93ee-4b836271f76c, clientType=HIVECLI]
2024-04-23T17:46:14,927  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:14,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:14,927  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28f6a008, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7fa86ddd will be shutdown
2024-04-23T17:46:14,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:14,927  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-23T17:46:14,928  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:14,929  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:14,930  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:14,930  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425, with PersistenceManager: null will be shutdown
2024-04-23T17:46:14,930  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b1cedfd created in the thread with id: 1
2024-04-23T17:46:14,933  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425 from thread id: 1
2024-04-23T17:46:14,934  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:14,934  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:14,943  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:14,943  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:14,944  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b1cedfd will be shutdown
2024-04-23T17:46:14,944  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5cf6ba1c created in the thread with id: 1
2024-04-23T17:46:14,948  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:14,948  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:14,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:14,954  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:14,995  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:14,998  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:15,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,039  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,041  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:15,053  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:15,086  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:15,092  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:15,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,111  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,112  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:15,114  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:15,115  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:15,115  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:15,117  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:15,123  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,123  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:15,123  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:15,124  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,129  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,130  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:15,130  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:15,130  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:15,133  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:15,185  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [2], allowMissingStats: true
2024-04-23T17:46:15,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T17:46:15,267  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:15,267  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:15,268  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:15,268  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,274  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,275  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:15,287  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 2
2024-04-23T17:46:15,288  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [2])
2024-04-23T17:46:15,288  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-15_111_4770307515881211805-1
2024-04-23T17:46:15,295  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 4 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 4 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:15,298  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:15,300  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:15,302  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_4 and SEL_5 as it was introduced by enforce bucketing/sorting.
2024-04-23T17:46:15,304  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_7 and SEL_8 as parent of FS_6 and child of SEL_3
2024-04-23T17:46:15,329  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:15,335  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,336  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:15,337  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:15,337  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:15,338  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:15,338  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:15,338  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:15,338  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:15,338  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:15,339  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:15,340  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:15,340  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:15,340  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:15,341  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:15,341  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:15,362  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-5 is a root stage
  Stage-6 depends on stages: Stage-5
  Stage-4 depends on stages: Stage-6
  Stage-7 depends on stages: Stage-4

STAGE PLANS:
  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), ds (type: string)
              outputColumnNames: _col0, _col1
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-6
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col1 (type: string), _bucket_number (type: string), _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: aaa
              numBuckets: 2
              sort order: +++
              Map-reduce partition columns: _col1 (type: string)
              tag: -1
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), KEY._col1 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-4
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: DELETE

  Stage: Stage-7
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:15,364  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:15,369  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,369  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:15,508  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,517  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:15,682  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:15,682  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:15,683  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:15,684  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 8f1dc49a-2870-4abf-b5ff-29264960e431
2024-04-23T17:46:15,687  INFO [main] SessionState: Hive Session ID = 8f1dc49a-2870-4abf-b5ff-29264960e431
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:15,688  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:15,695  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/8f1dc49a-2870-4abf-b5ff-29264960e431
2024-04-23T17:46:15,698  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/8f1dc49a-2870-4abf-b5ff-29264960e431
2024-04-23T17:46:15,701  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/8f1dc49a-2870-4abf-b5ff-29264960e431/_tmp_space.db
2024-04-23T17:46:15,702  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:15,703  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:15,703  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@208b8425, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5cf6ba1c will be shutdown
2024-04-23T17:46:15,703  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:15,703  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-23T17:46:15,705  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:15,706  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:15,707  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:15,707  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21d478f3, with PersistenceManager: null will be shutdown
2024-04-23T17:46:15,708  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21d478f3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19ea65f4 created in the thread with id: 1
2024-04-23T17:46:15,738  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21d478f3 from thread id: 1
2024-04-23T17:46:15,738  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:15,739  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:15,753  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([3]) with min_open_txn: 1
2024-04-23T17:46:15,753  INFO [main] lockmgr.DbTxnManager: Opened txnid:3
2024-04-23T17:46:15,754  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=8f1dc49a-2870-4abf-b5ff-29264960e431, clientType=HIVECLI]
2024-04-23T17:46:15,754  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:15,754  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:15,754  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21d478f3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19ea65f4 will be shutdown
2024-04-23T17:46:15,755  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:15,755  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-23T17:46:15,756  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:15,757  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:15,757  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:15,758  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c, with PersistenceManager: null will be shutdown
2024-04-23T17:46:15,758  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a25c9ad created in the thread with id: 1
2024-04-23T17:46:15,763  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c from thread id: 1
2024-04-23T17:46:15,763  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:15,763  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:15,776  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:15,776  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:15,776  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a25c9ad will be shutdown
2024-04-23T17:46:15,777  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71c2cb26 created in the thread with id: 1
2024-04-23T17:46:15,781  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:15,782  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:15,782  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919575, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:15,790  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:15,831  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919575, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:15,835  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:15,856  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,871  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,872  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:15,880  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:15,904  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:15,908  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:15,916  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,921  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,922  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where a > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:15,923  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:15,924  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:15,925  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:15,926  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:15,932  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,932  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:15,932  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:15,933  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:15,938  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:15,938  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:15,938  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:15,938  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:15,941  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:16,013  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T17:46:16,044  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T17:46:16,045  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:16,063  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T17:46:16,063  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T17:46:16,129  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:16,129  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:16,129  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:16,130  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,137  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,138  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:16,149  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 3
2024-04-23T17:46:16,150  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [3])
2024-04-23T17:46:16,151  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-15_922_2458848427713596329-1
2024-04-23T17:46:16,157  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:16,164  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,164  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,164  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,164  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,166  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:16,166  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_5 and SEL_6 as it was introduced by enforce bucketing/sorting.
2024-04-23T17:46:16,167  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_9 and SEL_10 as parent of FS_7 and child of SEL_4
2024-04-23T17:46:16,188  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:16,194  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,195  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:16,196  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:16,196  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:16,197  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:16,197  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:16,197  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:16,198  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:16,199  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:16,199  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:16,201  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:16,201  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:16,220  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-9 is a root stage
  Stage-10 depends on stages: Stage-9
  Stage-8 depends on stages: Stage-10
  Stage-11 depends on stages: Stage-8

STAGE PLANS:
  Stage: Stage-9
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: (UDFToDouble(a) > 5.0D) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(a) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), ds (type: string)
                outputColumnNames: _col0, _col1
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  value expressions: _col1 (type: string)
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-10
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col1 (type: string), _bucket_number (type: string), _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: aaa
              numBuckets: 2
              sort order: +++
              Map-reduce partition columns: _col1 (type: string)
              tag: -1
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), KEY._col1 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-8
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: DELETE

  Stage: Stage-11
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:16,221  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:16,227  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,228  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:16,305  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,311  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,311  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:16,494  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:16,495  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:16,495  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:16,495  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:16,495  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:16,495  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:16,495  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 4995e76d-9986-484d-95a4-7a07cb63fcfb
2024-04-23T17:46:16,498  INFO [main] SessionState: Hive Session ID = 4995e76d-9986-484d-95a4-7a07cb63fcfb
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:16,499  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:16,506  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/4995e76d-9986-484d-95a4-7a07cb63fcfb
2024-04-23T17:46:16,508  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/4995e76d-9986-484d-95a4-7a07cb63fcfb
2024-04-23T17:46:16,511  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/4995e76d-9986-484d-95a4-7a07cb63fcfb/_tmp_space.db
2024-04-23T17:46:16,515  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:16,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:16,515  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@732ae29c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71c2cb26 will be shutdown
2024-04-23T17:46:16,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:16,515  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
2024-04-23T17:46:16,517  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:16,518  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:16,518  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:16,519  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d306fd5, with PersistenceManager: null will be shutdown
2024-04-23T17:46:16,519  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d306fd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@8ae7e03 created in the thread with id: 1
2024-04-23T17:46:16,544  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d306fd5 from thread id: 1
2024-04-23T17:46:16,544  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:16,544  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:16,557  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([4]) with min_open_txn: 1
2024-04-23T17:46:16,558  INFO [main] lockmgr.DbTxnManager: Opened txnid:4
2024-04-23T17:46:16,558  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4995e76d-9986-484d-95a4-7a07cb63fcfb, clientType=HIVECLI]
2024-04-23T17:46:16,558  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:16,559  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:16,559  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d306fd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@8ae7e03 will be shutdown
2024-04-23T17:46:16,559  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:16,559  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -7
2024-04-23T17:46:16,560  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:16,561  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:16,561  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:16,562  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90, with PersistenceManager: null will be shutdown
2024-04-23T17:46:16,562  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2083bcfd created in the thread with id: 1
2024-04-23T17:46:16,565  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90 from thread id: 1
2024-04-23T17:46:16,566  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:16,566  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:16,575  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:16,575  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:16,576  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2083bcfd will be shutdown
2024-04-23T17:46:16,576  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1bce19db created in the thread with id: 1
2024-04-23T17:46:16,580  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:16,580  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:16,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919576, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:16,583  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:16,613  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919576, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:16,616  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:16,632  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,651  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,652  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:16,660  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:16,682  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:16,686  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:16,695  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T17:46:16,697  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,702  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,702  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:16,702  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:16,702  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:16,702  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:16,702  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,707  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,707  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:16,707  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:16,707  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:16,720  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-23T17:46:16,808  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:16,808  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:16,808  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:16,809  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-23T17:46:16,812  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:16,812  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:16,814  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:16,814  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:16,819  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,819  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:16,823  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T17:46:16,824  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T17:46:16,834  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 4
2024-04-23T17:46:16,834  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [4])
2024-04-23T17:46:16,835  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-16_512_3512508361975409995-1
2024-04-23T17:46:16,841  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table u in query insert into table U partition (ds) values ('abc', 3, 'today'), ('ghi', 5, 'tomorrow')
2024-04-23T17:46:16,844  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:16,849  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,860  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:16,863  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:16,867  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,867  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:16,867  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:16,903  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/4995e76d-9986-484d-95a4-7a07cb63fcfb/hive_2024-04-23_17-46-16_841_1282711345924280409-1/-mr-10000/.hive-staging_hive_2024-04-23_17-46-16_841_1282711345924280409-1
2024-04-23T17:46:16,912  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:16,919  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 oldColExprMap: {KEY._col0=Column[_col0], VALUE._col2=Column[_col3], VALUE._col3=Column[_col4], VALUE._col4=Column[_col5], VALUE._col5=Column[_col6], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], VALUE._col6=Column[_col7], VALUE._col7=Column[_col8], VALUE._col8=Column[_col9]}
2024-04-23T17:46:16,919  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 newColExprMap: {KEY._col0=Column[_col0], VALUE._col2=Column[_col3], VALUE._col3=Column[_col4], VALUE._col4=Column[_col5], VALUE._col5=Column[_col6], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], VALUE._col6=Column[_col7], VALUE._col7=Column[_col8], VALUE._col8=Column[_col9]}
2024-04-23T17:46:16,920  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,920  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:16,921  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-23T17:46:16,922  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:16,922  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_5 and SEL_6 as it was introduced by enforce bucketing/sorting.
2024-04-23T17:46:16,923  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_15 and SEL_16 as parent of FS_7 and child of SEL_3
2024-04-23T17:46:16,923  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:16,933  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:16,937  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,948  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:16,949  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map vectorization enabled: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map vectorized: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map enabledConditionsNotMet: [Could not enable vectorization due to partition column names size 1 is greater than the number of table column names size 0 IS false]
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.NullRowsInputFormat]
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:16,950  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:16,951  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T17:46:16,951  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T17:46:16,952  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T17:46:16,967  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-13 is a root stage
  Stage-12 depends on stages: Stage-13
  Stage-14 depends on stages: Stage-12, Stage-15
  Stage-15 depends on stages: Stage-13

STAGE PLANS:
  Stage: Stage-13
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: _dummy_table
            Row Limit Per Split: 1
            GatherStats: false
            Select Operator
              expressions: array(const struct('abc',3,'today'),const struct('ghi',5,'tomorrow')) (type: array<struct<col1:string,col2:int,col3:string>>)
              outputColumnNames: _col0
              UDTF Operator
                function name: inline
                Select Operator
                  expressions: col1 (type: string), CAST( col2 AS STRING) (type: string), col3 (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Reduce Output Operator
                    bucketingVersion: 1
                    key expressions: _col2 (type: string), _bucket_number (type: string), _col0 (type: string)
                    null sort order: aaa
                    numBuckets: 2
                    sort order: +++
                    Map-reduce partition columns: _col2 (type: string)
                    tag: -1
                    value expressions: _col1 (type: string)
                    auto parallelism: false
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                    outputColumnNames: a, b, ds
                    Group By Operator
                      aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
                      keys: ds (type: string)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      File Output Operator
                        bucketingVersion: 1
                        compressed: false
                        GlobalTableId: 0
                        directory: file:MASKED-OUT
                        NumFilesPerFileSink: 1
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            properties:
                              column.name.delimiter ,
                              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
                              columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                              escape.delim \
                              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                        TotalFiles: 1
                        GatherStats: false
                        MultiFileSpray: false
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: dummy_path
            input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns 
              columns.types 
              file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              location file:MASKED-OUT
              name _dummy_database._dummy_table
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
            serde: org.apache.hadoop.hive.serde2.NullStructSerDe
          
              input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucketing_version 2
                column.name.delimiter ,
                columns 
                columns.comments 
                columns.types 
                file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:MASKED-OUT
                name _dummy_database._dummy_table
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
              serde: org.apache.hadoop.hive.serde2.NullStructSerDe
              name: _dummy_database._dummy_table
            name: _dummy_database._dummy_table
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: string), VALUE._col1 (type: string), KEY._col2 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _col2, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-12
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: INSERT

  Stage: Stage-14
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.u
          Is Table Level Stats: false

  Stage: Stage-15
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              key expressions: _col0 (type: string)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: _col0 (type: string)
              tag: -1
              value expressions: _col1 (type: int), _col2 (type: struct<count:bigint,sum:double,input:int>), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary), _col6 (type: int), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: bigint), _col9 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
              columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9
                columns.types string,int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col1,0)) (type: bigint), COALESCE(_col2,0) (type: double), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col6,0)) (type: bigint), COALESCE(_col7,0) (type: double), (_col3 - _col8) (type: bigint), COALESCE(ndv_compute_bit_vector(_col9),0) (type: bigint), _col9 (type: binary), _col0 (type: string)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11,_col12
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T17:46:16,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:16,972  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:16,973  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:17,041  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:17,046  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,046  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:17,148  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:17,149  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:17,149  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:17,149  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:17,149  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:17,155  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:17,155  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 370c4664-8fb4-43e8-9749-a70c5324221b
2024-04-23T17:46:17,158  INFO [main] SessionState: Hive Session ID = 370c4664-8fb4-43e8-9749-a70c5324221b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:17,159  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:17,165  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/370c4664-8fb4-43e8-9749-a70c5324221b
2024-04-23T17:46:17,168  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/370c4664-8fb4-43e8-9749-a70c5324221b
2024-04-23T17:46:17,171  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/370c4664-8fb4-43e8-9749-a70c5324221b/_tmp_space.db
2024-04-23T17:46:17,172  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:17,172  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:17,172  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@dd5ea90, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1bce19db will be shutdown
2024-04-23T17:46:17,172  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:17,172  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -8
2024-04-23T17:46:17,173  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,173  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:17,173  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,174  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2a4137a2, with PersistenceManager: null will be shutdown
2024-04-23T17:46:17,174  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2a4137a2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71a9e31b created in the thread with id: 1
2024-04-23T17:46:17,196  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2a4137a2 from thread id: 1
2024-04-23T17:46:17,196  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,196  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,207  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([5]) with min_open_txn: 1
2024-04-23T17:46:17,207  INFO [main] lockmgr.DbTxnManager: Opened txnid:5
2024-04-23T17:46:17,208  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=370c4664-8fb4-43e8-9749-a70c5324221b, clientType=HIVECLI]
2024-04-23T17:46:17,208  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:17,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:17,208  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2a4137a2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71a9e31b will be shutdown
2024-04-23T17:46:17,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:17,208  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -9
2024-04-23T17:46:17,209  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,210  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:17,210  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,210  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509, with PersistenceManager: null will be shutdown
2024-04-23T17:46:17,210  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@20ca2c3f created in the thread with id: 1
2024-04-23T17:46:17,213  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509 from thread id: 1
2024-04-23T17:46:17,213  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,213  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,221  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,221  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,221  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@20ca2c3f will be shutdown
2024-04-23T17:46:17,221  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@47e6004c created in the thread with id: 1
2024-04-23T17:46:17,224  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,224  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,224  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:17,227  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:17,257  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:17,260  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:17,272  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:17,282  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:17,291  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:17,313  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:17,317  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:17,325  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:17,330  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,330  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update T set b = 5> as 
<insert into table `default`.`T` select ROW__ID,`a`,`b` from `default`.`T` sort by ROW__ID >
2024-04-23T17:46:17,331  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:17,333  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:17,338  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,338  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:17,338  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:17,339  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:17,344  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,344  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:17,344  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:17,344  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:17,349  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,349  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:17,349  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:17,351  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T17:46:17,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T17:46:17,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T17:46:17,410  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0], allowMissingStats: true
2024-04-23T17:46:17,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T17:46:17,417  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a
No Stats for default@t, Columns: a
2024-04-23T17:46:17,417  INFO [main] SessionState: No Stats for default@t, Columns: a
2024-04-23T17:46:17,448  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:17,449  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:17,449  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:17,449  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:17,453  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,463  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 5
2024-04-23T17:46:17,464  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [5])
2024-04-23T17:46:17,464  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-17_330_5191099504056490915-1
2024-04-23T17:46:17,470  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:17,472  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,472  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,472  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,472  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,473  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,473  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:17,474  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:17,495  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:17,499  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,500  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:17,500  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:17,500  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:17,501  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:17,502  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:17,502  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:17,503  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:17,503  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:17,517  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-17 is a root stage
  Stage-18 depends on stages: Stage-17
  Stage-16 depends on stages: Stage-18
  Stage-19 depends on stages: Stage-16

STAGE PLANS:
  Stage: Stage-17
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string)
              outputColumnNames: _col0, _col1
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: t
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            properties:
              bucket_count 2
              bucket_field_name a
              column.name.delimiter ,
              columns a,b
              columns.types string:string
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.t
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              transactional true
              transactional_properties default
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
            name: default.t
      Truncated Path -> Alias:
        /t [t]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string)
          outputColumnNames: _col0, _col1, _col2
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-18
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1, _col2
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-16
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: UPDATE

  Stage: Stage-19
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:17,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:17,521  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,522  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:17,643  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:17,655  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,656  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:17,789  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:17,789  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:17,789  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:17,789  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:17,789  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:17,790  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:17,790  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 073e4d7e-3a41-45b3-ab27-a25949ae3ace
2024-04-23T17:46:17,793  INFO [main] SessionState: Hive Session ID = 073e4d7e-3a41-45b3-ab27-a25949ae3ace
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:17,794  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:17,800  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/073e4d7e-3a41-45b3-ab27-a25949ae3ace
2024-04-23T17:46:17,802  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/073e4d7e-3a41-45b3-ab27-a25949ae3ace
2024-04-23T17:46:17,805  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/073e4d7e-3a41-45b3-ab27-a25949ae3ace/_tmp_space.db
2024-04-23T17:46:17,806  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:17,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:17,807  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@61d87509, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@47e6004c will be shutdown
2024-04-23T17:46:17,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:17,807  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -10
2024-04-23T17:46:17,808  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,809  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:17,809  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,809  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d8bf514, with PersistenceManager: null will be shutdown
2024-04-23T17:46:17,810  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d8bf514, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@32be0ebd created in the thread with id: 1
2024-04-23T17:46:17,834  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d8bf514 from thread id: 1
2024-04-23T17:46:17,834  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,834  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,847  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([6]) with min_open_txn: 1
2024-04-23T17:46:17,847  INFO [main] lockmgr.DbTxnManager: Opened txnid:6
2024-04-23T17:46:17,847  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=073e4d7e-3a41-45b3-ab27-a25949ae3ace, clientType=HIVECLI]
2024-04-23T17:46:17,847  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:17,848  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:17,848  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3d8bf514, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@32be0ebd will be shutdown
2024-04-23T17:46:17,848  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:17,848  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -11
2024-04-23T17:46:17,849  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,849  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:17,849  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,850  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c, with PersistenceManager: null will be shutdown
2024-04-23T17:46:17,850  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@234a08ea created in the thread with id: 1
2024-04-23T17:46:17,853  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c from thread id: 1
2024-04-23T17:46:17,853  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,853  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,863  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:17,863  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:17,863  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@234a08ea will be shutdown
2024-04-23T17:46:17,864  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@44a1e5d2 created in the thread with id: 1
2024-04-23T17:46:17,867  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:17,867  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:17,868  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:17,871  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:17,914  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:17,917  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:17,934  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:17,946  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:17,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:17,958  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:17,988  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:17,993  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:18,004  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,010  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,011  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where ds = 'today' and b > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:18,012  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:18,014  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,020  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,021  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:18,021  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:18,022  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:18,028  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,028  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:18,028  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:18,029  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,034  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,035  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:18,035  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:18,035  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:18,038  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:18,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T17:46:18,088  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T17:46:18,209  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T17:46:18,282  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1, 2], allowMissingStats: true
2024-04-23T17:46:18,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:18,293  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a, b
No Stats for default@u, Columns: a, b
2024-04-23T17:46:18,293  INFO [main] SessionState: No Stats for default@u, Columns: a, b
2024-04-23T17:46:18,339  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:18,339  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:18,340  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:18,340  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,344  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,344  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:18,355  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 6
2024-04-23T17:46:18,356  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [6])
2024-04-23T17:46:18,356  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-18_011_3023775147386598393-1
2024-04-23T17:46:18,362  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:18,394  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 oldColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,395  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 newColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,395  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,395  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,395  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,395  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,399  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:18,421  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:18,425  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,426  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:18,427  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:18,427  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:18,428  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:18,430  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:18,430  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:18,430  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:18,430  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:18,430  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:18,431  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:18,431  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:18,431  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:18,431  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:18,431  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:18,431  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:18,445  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-21 is a root stage
  Stage-22 depends on stages: Stage-21
  Stage-20 depends on stages: Stage-22
  Stage-23 depends on stages: Stage-20

STAGE PLANS:
  Stage: Stage-21
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: ((UDFToDouble(b) > 5.0D) and (ds = 'today')) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(b) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string)
                outputColumnNames: _col0, _col1
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  value expressions: _col1 (type: string)
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string), 'today' (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-22
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), VALUE._col1 (type: string), VALUE._col2 (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-20
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: UPDATE

  Stage: Stage-23
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:18,445  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:18,450  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,450  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:18,553  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,558  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,558  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:18,650  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:18,650  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:18,650  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:18,651  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:18,651  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = e030680c-e9c9-4220-9841-5a5672508007
2024-04-23T17:46:18,654  INFO [main] SessionState: Hive Session ID = e030680c-e9c9-4220-9841-5a5672508007
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:18,655  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:18,660  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/e030680c-e9c9-4220-9841-5a5672508007
2024-04-23T17:46:18,663  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/e030680c-e9c9-4220-9841-5a5672508007
2024-04-23T17:46:18,665  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/e030680c-e9c9-4220-9841-5a5672508007/_tmp_space.db
2024-04-23T17:46:18,666  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:18,666  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:18,666  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5f160f9c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@44a1e5d2 will be shutdown
2024-04-23T17:46:18,667  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:18,667  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -12
2024-04-23T17:46:18,667  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:18,668  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:18,668  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:18,669  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a1eef38, with PersistenceManager: null will be shutdown
2024-04-23T17:46:18,669  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a1eef38, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3ed6d7fb created in the thread with id: 1
2024-04-23T17:46:18,686  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a1eef38 from thread id: 1
2024-04-23T17:46:18,687  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:18,687  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:18,697  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([7]) with min_open_txn: 1
2024-04-23T17:46:18,697  INFO [main] lockmgr.DbTxnManager: Opened txnid:7
2024-04-23T17:46:18,697  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e030680c-e9c9-4220-9841-5a5672508007, clientType=HIVECLI]
2024-04-23T17:46:18,698  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:18,698  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:18,698  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a1eef38, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3ed6d7fb will be shutdown
2024-04-23T17:46:18,698  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:18,698  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -13
2024-04-23T17:46:18,699  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:18,700  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:18,700  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:18,701  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9, with PersistenceManager: null will be shutdown
2024-04-23T17:46:18,701  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39c4ed23 created in the thread with id: 1
2024-04-23T17:46:18,703  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9 from thread id: 1
2024-04-23T17:46:18,703  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:18,703  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:18,709  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:18,709  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:18,710  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39c4ed23 will be shutdown
2024-04-23T17:46:18,710  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f0ea6f8 created in the thread with id: 1
2024-04-23T17:46:18,712  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:18,712  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:18,712  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919578, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:18,715  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:18,750  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919578, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:18,753  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:18,764  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,778  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,779  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:18,785  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:18,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:18,810  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:18,818  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,822  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,822  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where ds = 'today'> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:18,823  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:18,824  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:18,824  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:18,825  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:18,830  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,830  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:18,830  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:18,831  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,835  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,835  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:18,835  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:18,835  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:18,838  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:18,887  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T17:46:18,908  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [2], allowMissingStats: true
2024-04-23T17:46:18,937  INFO [main] stats.BasicStats: Number of partishes : 1
2024-04-23T17:46:18,947  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:18,947  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:18,947  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:18,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:18,951  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,951  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:18,960  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 7
2024-04-23T17:46:18,961  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [7])
2024-04-23T17:46:18,961  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-18_822_7666206612666489235-1
2024-04-23T17:46:18,967  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,971  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:18,972  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:18,991  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:18,995  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:18,995  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:18,996  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:18,996  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:18,996  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:18,996  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:18,997  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:18,998  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:18,998  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:19,012  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-25 is a root stage
  Stage-26 depends on stages: Stage-25
  Stage-24 depends on stages: Stage-26
  Stage-27 depends on stages: Stage-24

STAGE PLANS:
  Stage: Stage-25
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: (ds = 'today') (type: boolean)
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              outputColumnNames: _col0
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), 'today' (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-26
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-24
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: DELETE

  Stage: Stage-27
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:19,013  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:19,017  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,017  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:19,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,108  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,108  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:19,199  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:19,200  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:19,200  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:19,200  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:19,200  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:19,200  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:19,200  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = eec6c271-7c3d-440f-a4c6-0fde7df14203
2024-04-23T17:46:19,203  INFO [main] SessionState: Hive Session ID = eec6c271-7c3d-440f-a4c6-0fde7df14203
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:19,203  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:19,209  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/eec6c271-7c3d-440f-a4c6-0fde7df14203
2024-04-23T17:46:19,211  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/eec6c271-7c3d-440f-a4c6-0fde7df14203
2024-04-23T17:46:19,214  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/eec6c271-7c3d-440f-a4c6-0fde7df14203/_tmp_space.db
2024-04-23T17:46:19,215  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:19,215  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:19,215  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45198fd9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f0ea6f8 will be shutdown
2024-04-23T17:46:19,215  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,215  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-23T17:46:19,216  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,216  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:19,216  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,217  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f4249cb, with PersistenceManager: null will be shutdown
2024-04-23T17:46:19,217  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f4249cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@38f4c6af created in the thread with id: 1
2024-04-23T17:46:19,231  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f4249cb from thread id: 1
2024-04-23T17:46:19,232  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,232  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,240  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([8]) with min_open_txn: 1
2024-04-23T17:46:19,240  INFO [main] lockmgr.DbTxnManager: Opened txnid:8
2024-04-23T17:46:19,241  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=eec6c271-7c3d-440f-a4c6-0fde7df14203, clientType=HIVECLI]
2024-04-23T17:46:19,241  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:19,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:19,241  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6f4249cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@38f4c6af will be shutdown
2024-04-23T17:46:19,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,241  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-23T17:46:19,242  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,242  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:19,243  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,243  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963, with PersistenceManager: null will be shutdown
2024-04-23T17:46:19,243  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5736cc4e created in the thread with id: 1
2024-04-23T17:46:19,246  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963 from thread id: 1
2024-04-23T17:46:19,246  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,246  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,252  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,252  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,253  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5736cc4e will be shutdown
2024-04-23T17:46:19,253  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2e1e46e7 created in the thread with id: 1
2024-04-23T17:46:19,255  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,255  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,255  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:19,257  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:19,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:19,285  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:19,299  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,305  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,306  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:19,314  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:19,334  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:19,338  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:19,346  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,350  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,350  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where ds = 'today'> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:19,352  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:19,353  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,357  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,357  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:19,357  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:19,358  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:19,362  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,362  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:19,362  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:19,362  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,365  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,366  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:19,366  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:19,366  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:19,368  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:19,396  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T17:46:19,396  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T17:46:19,414  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T17:46:19,436  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T17:46:19,437  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:19,444  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T17:46:19,444  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T17:46:19,487  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:19,487  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:19,487  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:19,487  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,491  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,491  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:19,501  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 8
2024-04-23T17:46:19,501  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [8])
2024-04-23T17:46:19,501  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-19_350_7473388443785293632-1
2024-04-23T17:46:19,507  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:19,511  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 oldColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,512  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 newColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,512  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,512  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,512  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,512  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:19,514  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:19,536  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:19,541  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,542  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:19,542  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:19,542  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:19,543  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:19,543  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:19,543  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:19,544  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:19,545  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:19,545  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:19,570  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-29 is a root stage
  Stage-30 depends on stages: Stage-29
  Stage-28 depends on stages: Stage-30
  Stage-31 depends on stages: Stage-28

STAGE PLANS:
  Stage: Stage-29
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: (ds = 'today') (type: boolean)
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string)
              outputColumnNames: _col0, _col1
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string), 'today' (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-30
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), VALUE._col1 (type: string), VALUE._col2 (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-28
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: UPDATE

  Stage: Stage-31
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:19,571  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:19,576  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:19,691  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,696  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,696  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:19,781  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,781  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-23T17:46:19,782  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,782  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-23T17:46:19,816  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:19,816  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:19,817  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:19,818  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 7cbc4504-2e69-47aa-bf8b-6edc8469bb3f
2024-04-23T17:46:19,821  INFO [main] SessionState: Hive Session ID = 7cbc4504-2e69-47aa-bf8b-6edc8469bb3f
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:19,821  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:19,827  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/7cbc4504-2e69-47aa-bf8b-6edc8469bb3f
2024-04-23T17:46:19,830  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/7cbc4504-2e69-47aa-bf8b-6edc8469bb3f
2024-04-23T17:46:19,832  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/7cbc4504-2e69-47aa-bf8b-6edc8469bb3f/_tmp_space.db
2024-04-23T17:46:19,833  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:19,834  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:19,834  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@56dde963, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2e1e46e7 will be shutdown
2024-04-23T17:46:19,834  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,835  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -18
2024-04-23T17:46:19,835  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,836  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:19,836  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,836  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@18e0d125, with PersistenceManager: null will be shutdown
2024-04-23T17:46:19,836  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@18e0d125, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33d3e850 created in the thread with id: 1
2024-04-23T17:46:19,856  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@18e0d125 from thread id: 1
2024-04-23T17:46:19,856  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,856  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,868  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([9]) with min_open_txn: 1
2024-04-23T17:46:19,868  INFO [main] lockmgr.DbTxnManager: Opened txnid:9
2024-04-23T17:46:19,869  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7cbc4504-2e69-47aa-bf8b-6edc8469bb3f, clientType=HIVECLI]
2024-04-23T17:46:19,869  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:19,869  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:19,869  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@18e0d125, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33d3e850 will be shutdown
2024-04-23T17:46:19,869  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:19,869  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -19
2024-04-23T17:46:19,870  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,870  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:19,870  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,871  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c, with PersistenceManager: null will be shutdown
2024-04-23T17:46:19,871  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@247113ce created in the thread with id: 1
2024-04-23T17:46:19,873  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c from thread id: 1
2024-04-23T17:46:19,873  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,873  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,879  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:19,879  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:19,879  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@247113ce will be shutdown
2024-04-23T17:46:19,879  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7590b28f created in the thread with id: 1
2024-04-23T17:46:19,881  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:19,881  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:19,881  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:19,883  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:19,909  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:19,912  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:19,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:19,940  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,941  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:19,947  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:19,967  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:19,970  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:19,977  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:19,980  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,980  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from T where a > 5> as 
<insert into table `default`.`T` select ROW__ID from `default`.`T` sort by ROW__ID >
2024-04-23T17:46:19,981  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:19,982  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:19,982  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:19,982  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:19,985  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,985  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:19,986  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:19,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:19,989  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:19,989  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:19,989  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:19,991  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T17:46:20,031  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.t, projIndxSet: [0], allowMissingStats: true
2024-04-23T17:46:20,032  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table_statistics_req: table=hive.default.t	
2024-04-23T17:46:20,037  WARN [main] calcite.RelOptHiveTable: No Stats for default@t, Columns: a
No Stats for default@t, Columns: a
2024-04-23T17:46:20,037  INFO [main] SessionState: No Stats for default@t, Columns: a
2024-04-23T17:46:20,083  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:20,084  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:20,084  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:20,084  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:20,087  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,095  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 9
2024-04-23T17:46:20,096  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [9])
2024-04-23T17:46:20,096  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-19_980_6109914119540630032-1
2024-04-23T17:46:20,102  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:20,104  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,104  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,104  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,104  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,105  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:20,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:20,133  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,134  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:20,135  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:20,135  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:20,137  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:20,139  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:20,139  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:20,153  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-33 is a root stage
  Stage-34 depends on stages: Stage-33
  Stage-32 depends on stages: Stage-34
  Stage-35 depends on stages: Stage-32

STAGE PLANS:
  Stage: Stage-33
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t
            filterExpr: (UDFToDouble(a) > 5.0D) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(a) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                outputColumnNames: _col0
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: t
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            properties:
              bucket_count 2
              bucket_field_name a
              column.name.delimiter ,
              columns a,b
              columns.types string:string
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.t
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              transactional true
              transactional_properties default
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
            name: default.t
      Truncated Path -> Alias:
        /t [t]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
          outputColumnNames: _col0
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-34
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
          outputColumnNames: _col0
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-32
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: DELETE

  Stage: Stage-35
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:20,154  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:20,157  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,158  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:20,267  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,270  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,270  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:20,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:20,362  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:20,362  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 77ce9530-f182-47df-b804-b79d94a95d17
2024-04-23T17:46:20,365  INFO [main] SessionState: Hive Session ID = 77ce9530-f182-47df-b804-b79d94a95d17
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:20,365  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:20,371  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/77ce9530-f182-47df-b804-b79d94a95d17
2024-04-23T17:46:20,374  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/77ce9530-f182-47df-b804-b79d94a95d17
2024-04-23T17:46:20,377  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/77ce9530-f182-47df-b804-b79d94a95d17/_tmp_space.db
2024-04-23T17:46:20,378  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:20,378  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:20,378  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2203fa4c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7590b28f will be shutdown
2024-04-23T17:46:20,378  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:20,378  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -20
2024-04-23T17:46:20,379  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:20,379  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:20,379  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:20,380  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6191395, with PersistenceManager: null will be shutdown
2024-04-23T17:46:20,380  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6191395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@53f68c97 created in the thread with id: 1
2024-04-23T17:46:20,396  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6191395 from thread id: 1
2024-04-23T17:46:20,396  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:20,397  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:20,405  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([10]) with min_open_txn: 1
2024-04-23T17:46:20,406  INFO [main] lockmgr.DbTxnManager: Opened txnid:10
2024-04-23T17:46:20,406  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=77ce9530-f182-47df-b804-b79d94a95d17, clientType=HIVECLI]
2024-04-23T17:46:20,406  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:20,406  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:20,406  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6191395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@53f68c97 will be shutdown
2024-04-23T17:46:20,406  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:20,407  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -21
2024-04-23T17:46:20,407  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:20,407  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:20,407  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:20,408  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb, with PersistenceManager: null will be shutdown
2024-04-23T17:46:20,408  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4267edd9 created in the thread with id: 1
2024-04-23T17:46:20,409  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb from thread id: 1
2024-04-23T17:46:20,410  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:20,410  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:20,416  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:20,416  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:20,416  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4267edd9 will be shutdown
2024-04-23T17:46:20,416  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39e17209 created in the thread with id: 1
2024-04-23T17:46:20,418  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:20,418  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:20,418  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919580, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:20,420  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:20,569  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:20,569  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -22
2024-04-23T17:46:20,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919580, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:20,579  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:20,588  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,602  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,602  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:20,608  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:20,626  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:20,629  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:20,636  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,639  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,640  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5 where b > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:20,640  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:20,642  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,645  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,645  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:20,645  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:20,646  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:20,649  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,649  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:20,649  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:20,650  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,653  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,653  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:20,653  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:20,653  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:20,655  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:20,680  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T17:46:20,680  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T17:46:20,696  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T17:46:20,717  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1, 2], allowMissingStats: true
2024-04-23T17:46:20,718  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:20,726  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a, b
No Stats for default@u, Columns: a, b
2024-04-23T17:46:20,726  INFO [main] SessionState: No Stats for default@u, Columns: a, b
2024-04-23T17:46:20,769  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:20,769  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:20,769  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:20,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,773  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,773  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:20,782  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 10
2024-04-23T17:46:20,783  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [10])
2024-04-23T17:46:20,783  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-20_640_7730270563882041603-1
2024-04-23T17:46:20,790  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:20,794  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 oldColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,794  INFO [main] optimizer.ColumnPrunerProcFactory: RS 6 newColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,794  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,794  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,794  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,795  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:20,796  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:20,796  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_6 and SEL_7 as it was introduced by enforce bucketing/sorting.
2024-04-23T17:46:20,797  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_10 and SEL_11 as parent of FS_8 and child of SEL_4
2024-04-23T17:46:20,827  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:20,830  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,831  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:20,831  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:20,832  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:20,833  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:20,834  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:20,834  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:20,835  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:20,836  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:20,850  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-37 is a root stage
  Stage-38 depends on stages: Stage-37
  Stage-36 depends on stages: Stage-38
  Stage-39 depends on stages: Stage-36

STAGE PLANS:
  Stage: Stage-37
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: (UDFToDouble(b) > 5.0D) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(b) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string), ds (type: string)
                outputColumnNames: _col0, _col1, _col3
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  value expressions: _col1 (type: string), _col3 (type: string)
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-38
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col3 (type: string), _bucket_number (type: string), _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: aaa
              numBuckets: 2
              sort order: +++
              Map-reduce partition columns: _col3 (type: string)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col1 (type: string), VALUE._col2 (type: string), KEY._col3 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-36
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: UPDATE

  Stage: Stage-39
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:20,850  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:20,854  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,854  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:20,942  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:20,947  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:20,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:21,047  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:21,048  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:21,048  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:21,048  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:21,048  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:21,048  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 0d8a09db-4c56-4558-9611-4bb797fab9fd
2024-04-23T17:46:21,051  INFO [main] SessionState: Hive Session ID = 0d8a09db-4c56-4558-9611-4bb797fab9fd
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:21,052  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:21,057  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/0d8a09db-4c56-4558-9611-4bb797fab9fd
2024-04-23T17:46:21,060  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/0d8a09db-4c56-4558-9611-4bb797fab9fd
2024-04-23T17:46:21,063  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/0d8a09db-4c56-4558-9611-4bb797fab9fd/_tmp_space.db
2024-04-23T17:46:21,063  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:21,064  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:21,064  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6bd02ccb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39e17209 will be shutdown
2024-04-23T17:46:21,064  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:21,064  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -23
2024-04-23T17:46:21,065  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,066  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:21,066  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,066  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a552fe4, with PersistenceManager: null will be shutdown
2024-04-23T17:46:21,066  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a552fe4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@61cb973d created in the thread with id: 1
2024-04-23T17:46:21,083  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a552fe4 from thread id: 1
2024-04-23T17:46:21,083  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,083  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,091  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([11]) with min_open_txn: 1
2024-04-23T17:46:21,092  INFO [main] lockmgr.DbTxnManager: Opened txnid:11
2024-04-23T17:46:21,092  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0d8a09db-4c56-4558-9611-4bb797fab9fd, clientType=HIVECLI]
2024-04-23T17:46:21,092  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:21,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:21,092  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a552fe4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@61cb973d will be shutdown
2024-04-23T17:46:21,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:21,092  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-23T17:46:21,093  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,093  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:21,093  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,094  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5, with PersistenceManager: null will be shutdown
2024-04-23T17:46:21,094  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@31e1a699 created in the thread with id: 1
2024-04-23T17:46:21,095  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5 from thread id: 1
2024-04-23T17:46:21,096  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,096  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,101  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,101  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,101  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@31e1a699 will be shutdown
2024-04-23T17:46:21,102  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@b82b59c created in the thread with id: 1
2024-04-23T17:46:21,103  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,104  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919581, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:21,105  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:21,132  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919581, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:21,136  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:21,145  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,156  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,157  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:21,161  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:21,180  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:21,182  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:21,189  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:21,192  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,193  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from T> as 
<insert into table `default`.`T` select ROW__ID from `default`.`T` sort by ROW__ID >
2024-04-23T17:46:21,193  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:21,194  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:21,194  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:21,195  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:21,198  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,198  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:21,198  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:21,199  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:21,202  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,202  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:21,203  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:21,204  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.t	
2024-04-23T17:46:21,255  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:21,255  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:21,255  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:21,255  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:21,258  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,266  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 11
2024-04-23T17:46:21,266  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [11])
2024-04-23T17:46:21,267  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-21_193_5504698351517612613-1
2024-04-23T17:46:21,272  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:21,273  INFO [main] optimizer.ColumnPrunerProcFactory: RS 4 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,273  INFO [main] optimizer.ColumnPrunerProcFactory: RS 4 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,273  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,274  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,275  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:21,295  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:21,298  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,298  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:21,299  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:21,299  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:21,300  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:21,301  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:21,301  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:21,314  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-41 is a root stage
  Stage-42 depends on stages: Stage-41
  Stage-40 depends on stages: Stage-42
  Stage-43 depends on stages: Stage-40

STAGE PLANS:
  Stage: Stage-41
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              outputColumnNames: _col0
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: t
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            properties:
              bucket_count 2
              bucket_field_name a
              column.name.delimiter ,
              columns a,b
              columns.types string:string
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.t
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              transactional true
              transactional_properties default
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
            name: default.t
      Truncated Path -> Alias:
        /t [t]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
          outputColumnNames: _col0
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-42
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
          outputColumnNames: _col0
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-40
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: DELETE

  Stage: Stage-43
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:21,314  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:21,317  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,318  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:21,414  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,417  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,417  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:21,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:21,498  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:21,498  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:21,498  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = 2116fbe0-17a3-4833-b18b-e5fdfa611f56
2024-04-23T17:46:21,500  INFO [main] SessionState: Hive Session ID = 2116fbe0-17a3-4833-b18b-e5fdfa611f56
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:21,501  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:21,507  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/2116fbe0-17a3-4833-b18b-e5fdfa611f56
2024-04-23T17:46:21,509  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/2116fbe0-17a3-4833-b18b-e5fdfa611f56
2024-04-23T17:46:21,512  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/2116fbe0-17a3-4833-b18b-e5fdfa611f56/_tmp_space.db
2024-04-23T17:46:21,512  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:21,513  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:21,513  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2eaf17c5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@b82b59c will be shutdown
2024-04-23T17:46:21,513  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:21,513  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-23T17:46:21,513  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,514  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:21,514  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,514  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57c09e37, with PersistenceManager: null will be shutdown
2024-04-23T17:46:21,514  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57c09e37, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56f3a810 created in the thread with id: 1
2024-04-23T17:46:21,527  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57c09e37 from thread id: 1
2024-04-23T17:46:21,527  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,527  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,534  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([12]) with min_open_txn: 1
2024-04-23T17:46:21,535  INFO [main] lockmgr.DbTxnManager: Opened txnid:12
2024-04-23T17:46:21,535  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=2116fbe0-17a3-4833-b18b-e5fdfa611f56, clientType=HIVECLI]
2024-04-23T17:46:21,535  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:21,535  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:21,535  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57c09e37, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56f3a810 will be shutdown
2024-04-23T17:46:21,535  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:21,535  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -26
2024-04-23T17:46:21,536  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,536  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:21,536  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,537  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8, with PersistenceManager: null will be shutdown
2024-04-23T17:46:21,537  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@279b5232 created in the thread with id: 1
2024-04-23T17:46:21,539  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8 from thread id: 1
2024-04-23T17:46:21,539  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,539  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,544  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:21,545  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:21,545  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@279b5232 will be shutdown
2024-04-23T17:46:21,545  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2ef77fcc created in the thread with id: 1
2024-04-23T17:46:21,547  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:21,547  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:21,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919581, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:21,551  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:21,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919581, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:21,578  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:21,587  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,598  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,599  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:21,606  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:21,625  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:21,628  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:21,637  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,641  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,641  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <delete from U where ds = 'today' and a > 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:21,642  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:21,643  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:21,643  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:21,644  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:21,648  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,648  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:21,648  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:21,649  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,653  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,653  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:21,653  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:21,653  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:21,656  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:21,706  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_spec_by_expr : tbl=hive.default.u	
2024-04-23T17:46:21,728  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T17:46:21,728  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:21,738  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T17:46:21,738  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T17:46:21,792  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:21,792  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:21,792  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:21,793  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,797  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,797  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:21,809  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 12
2024-04-23T17:46:21,810  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [12])
2024-04-23T17:46:21,810  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-21_641_2541987488840931028-1
2024-04-23T17:46:21,816  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:21,821  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,821  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,821  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,821  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,822  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 oldColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,822  INFO [main] optimizer.ColumnPrunerProcFactory: RS 3 newColExprMap: {KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:21,823  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:21,853  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:21,858  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,859  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:21,860  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:21,860  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:21,861  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:21,863  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:21,863  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:21,878  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-45 is a root stage
  Stage-46 depends on stages: Stage-45
  Stage-44 depends on stages: Stage-46
  Stage-47 depends on stages: Stage-44

STAGE PLANS:
  Stage: Stage-45
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            filterExpr: ((UDFToDouble(a) > 5.0D) and (ds = 'today')) (type: boolean)
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(a) > 5.0D) (type: boolean)
              Select Operator
                expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                outputColumnNames: _col0
                Reduce Output Operator
                  bucketingVersion: 2
                  key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                  null sort order: z
                  numBuckets: -1
                  sort order: +
                  tag: -1
                  auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), 'today' (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-46
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: z
              numBuckets: -1
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              tag: -1
              value expressions: _col1 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: DELETE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-44
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: DELETE

  Stage: Stage-47
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:21,879  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:21,883  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,883  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:21,978  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:21,981  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:21,981  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:22,083  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:22,083  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:22,084  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:22,085  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = efa24823-7f15-479f-b32c-6c55ef6cabfc
2024-04-23T17:46:22,087  INFO [main] SessionState: Hive Session ID = efa24823-7f15-479f-b32c-6c55ef6cabfc
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:22,088  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:22,093  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/efa24823-7f15-479f-b32c-6c55ef6cabfc
2024-04-23T17:46:22,096  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/efa24823-7f15-479f-b32c-6c55ef6cabfc
2024-04-23T17:46:22,099  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/efa24823-7f15-479f-b32c-6c55ef6cabfc/_tmp_space.db
2024-04-23T17:46:22,099  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:22,100  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:22,100  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5a0d2ed8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2ef77fcc will be shutdown
2024-04-23T17:46:22,100  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:22,100  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -27
2024-04-23T17:46:22,101  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,101  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:22,101  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,102  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@14200fed, with PersistenceManager: null will be shutdown
2024-04-23T17:46:22,102  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@14200fed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@45dc01ca created in the thread with id: 1
2024-04-23T17:46:22,119  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@14200fed from thread id: 1
2024-04-23T17:46:22,119  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,119  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,128  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([13]) with min_open_txn: 1
2024-04-23T17:46:22,128  INFO [main] lockmgr.DbTxnManager: Opened txnid:13
2024-04-23T17:46:22,129  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=efa24823-7f15-479f-b32c-6c55ef6cabfc, clientType=HIVECLI]
2024-04-23T17:46:22,129  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:22,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:22,129  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@14200fed, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@45dc01ca will be shutdown
2024-04-23T17:46:22,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:22,129  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -28
2024-04-23T17:46:22,130  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,130  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:22,130  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,131  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626, with PersistenceManager: null will be shutdown
2024-04-23T17:46:22,131  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f45a09c created in the thread with id: 1
2024-04-23T17:46:22,133  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626 from thread id: 1
2024-04-23T17:46:22,133  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,133  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,139  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,140  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f45a09c will be shutdown
2024-04-23T17:46:22,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@320967d4 created in the thread with id: 1
2024-04-23T17:46:22,142  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,142  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919582, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:22,147  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:22,172  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919582, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:22,174  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:22,187  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,191  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:22,200  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:22,220  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:22,222  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:22,229  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,232  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,233  INFO [main] parse.RewriteSemanticAnalyzer: Going to reparse <update U set b = 5> as 
<insert into table `default`.`U` partition (`ds`) select ROW__ID,`a`,`b`, `ds` from `default`.`U` sort by ROW__ID >
2024-04-23T17:46:22,233  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Starting Semantic Analysis
2024-04-23T17:46:22,235  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,238  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,238  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:22,238  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:22,240  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:22,242  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,243  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:22,243  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:22,243  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,246  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,246  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:22,246  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:22,246  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:22,249  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:22,275  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.u	
2024-04-23T17:46:22,276  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.u	
2024-04-23T17:46:22,290  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 2], allowMissingStats: true
2024-04-23T17:46:22,290  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T17:46:22,311  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:22,318  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a
No Stats for default@u, Columns: a
2024-04-23T17:46:22,318  INFO [main] SessionState: No Stats for default@u, Columns: a
2024-04-23T17:46:22,345  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for source tables
2024-04-23T17:46:22,345  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for subqueries
2024-04-23T17:46:22,346  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Get metadata for destination tables
2024-04-23T17:46:22,346  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,348  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,349  WARN [main] parse.BaseSemanticAnalyzer: Dynamic partitioning is used; only validating 0 columns
2024-04-23T17:46:22,356  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 13
2024-04-23T17:46:22,357  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=u (txnIds: [13])
2024-04-23T17:46:22,357  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/.hive-staging_hive_2024-04-23_17-46-22_233_239540729116155133-1
2024-04-23T17:46:22,363  INFO [main] parse.UpdateDeleteSemanticAnalyzer: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:22,364  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,364  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col2=Column[_col3], VALUE._col0=Column[_col1], VALUE._col1=Column[_col2], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,365  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,365  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,365  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,365  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], VALUE._col1=Column[_col3], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,366  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:22,366  INFO [main] optimizer.SortedDynPartitionOptimizer: Removed RS_5 and SEL_6 as it was introduced by enforce bucketing/sorting.
2024-04-23T17:46:22,367  INFO [main] optimizer.SortedDynPartitionOptimizer: Inserted RS_8 and SEL_9 as parent of FS_7 and child of SEL_3
2024-04-23T17:46:22,395  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:22,398  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,399  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:22,400  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:22,400  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:22,401  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:22,402  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:22,403  INFO [main] parse.UpdateDeleteSemanticAnalyzer: Completed plan generation
2024-04-23T17:46:22,416  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-49 is a root stage
  Stage-50 depends on stages: Stage-49
  Stage-48 depends on stages: Stage-50
  Stage-51 depends on stages: Stage-48

STAGE PLANS:
  Stage: Stage-49
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            GatherStats: false
            Select Operator
              expressions: ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), a (type: string), ds (type: string)
              outputColumnNames: _col0, _col1, _col3
              Reduce Output Operator
                bucketingVersion: 2
                key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                null sort order: z
                numBuckets: -1
                sort order: +
                tag: -1
                value expressions: _col1 (type: string), _col3 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: string), '5' (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 0
            directory: file:MASKED-OUT
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3
                  columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-50
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 1
              key expressions: _col3 (type: string), _bucket_number (type: string), _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: aaa
              numBuckets: 2
              sort order: +++
              Map-reduce partition columns: _col3 (type: string)
              tag: -1
              value expressions: _col1 (type: string), _col2 (type: string)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types struct<writeid:bigint,bucketid:int,rowid:bigint>,string,string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY._col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col1 (type: string), VALUE._col2 (type: string), KEY._col3 (type: string), KEY._bucket_number (type: string)
          outputColumnNames: _col0, _col1, _col2, _col3, _bucket_number
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            Dp Sort State: PARTITION_BUCKET_SORTED
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.u
                  partition_columns ds
                  partition_columns.types string
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.u
            TotalFiles: 1
            Write Type: UPDATE
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-48
    Move Operator
      tables:
          partition:
            ds 
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
          Write Type: UPDATE

  Stage: Stage-51
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT


2024-04-23T17:46:22,416  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:22,425  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,426  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:22,512  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,514  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:22,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:22,602  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:22,602  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:22,602  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:22,602  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:22,602  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:22,602  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = b438ea87-7095-419c-93de-09b781f3e1e2
2024-04-23T17:46:22,605  INFO [main] SessionState: Hive Session ID = b438ea87-7095-419c-93de-09b781f3e1e2
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:22,605  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:22,611  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/b438ea87-7095-419c-93de-09b781f3e1e2
2024-04-23T17:46:22,613  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/b438ea87-7095-419c-93de-09b781f3e1e2
2024-04-23T17:46:22,615  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/b438ea87-7095-419c-93de-09b781f3e1e2/_tmp_space.db
2024-04-23T17:46:22,616  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:22,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:22,616  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@43e49626, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@320967d4 will be shutdown
2024-04-23T17:46:22,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:22,616  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -29
2024-04-23T17:46:22,617  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,617  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:22,617  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,618  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c721652, with PersistenceManager: null will be shutdown
2024-04-23T17:46:22,618  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c721652, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2feb9067 created in the thread with id: 1
2024-04-23T17:46:22,632  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c721652 from thread id: 1
2024-04-23T17:46:22,632  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,632  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,639  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([14]) with min_open_txn: 1
2024-04-23T17:46:22,640  INFO [main] lockmgr.DbTxnManager: Opened txnid:14
2024-04-23T17:46:22,640  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b438ea87-7095-419c-93de-09b781f3e1e2, clientType=HIVECLI]
2024-04-23T17:46:22,640  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:22,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:22,640  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c721652, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2feb9067 will be shutdown
2024-04-23T17:46:22,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:22,640  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -30
2024-04-23T17:46:22,641  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,641  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:22,641  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,642  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2, with PersistenceManager: null will be shutdown
2024-04-23T17:46:22,642  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71693a66 created in the thread with id: 1
2024-04-23T17:46:22,644  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2 from thread id: 1
2024-04-23T17:46:22,644  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,644  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,651  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:22,651  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:22,651  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71693a66 will be shutdown
2024-04-23T17:46:22,651  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3ddd5d32 created in the thread with id: 1
2024-04-23T17:46:22,653  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:22,653  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:22,653  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919582, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:22,655  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:22,676  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919582, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:22,678  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:22,690  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:22,700  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,701  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:22,707  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:22,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:22,726  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:22,732  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T17:46:22,734  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:22,735  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:22,735  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.u	
2024-04-23T17:46:22,739  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,739  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:22,739  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:22,739  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:22,743  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,743  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:22,743  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:22,745  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.u	
2024-04-23T17:46:22,771  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.u, projIndxSet: [0, 1], allowMissingStats: true
2024-04-23T17:46:22,771  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.u	
2024-04-23T17:46:22,793  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.u	
2024-04-23T17:46:22,800  WARN [main] calcite.RelOptHiveTable: No Stats for default@u, Columns: a, b
No Stats for default@u, Columns: a, b
2024-04-23T17:46:22,800  INFO [main] SessionState: No Stats for default@u, Columns: a, b
2024-04-23T17:46:22,827  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:22,827  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:22,827  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:22,827  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:22,830  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,831  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T17:46:22,832  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T17:46:22,841  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 14
2024-04-23T17:46:22,841  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [14])
2024-04-23T17:46:22,842  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-22_615_7423700744401164966-1
2024-04-23T17:46:22,847  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table t in query insert into table T select a, b from U
2024-04-23T17:46:22,849  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:22,852  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,857  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:22,860  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:22,864  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,864  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:22,864  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:22,869  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/b438ea87-7095-419c-93de-09b781f3e1e2/hive_2024-04-23_17-46-22_847_7835517480541984312-1/-mr-10000/.hive-staging_hive_2024-04-23_17-46-22_847_7835517480541984312-1
2024-04-23T17:46:22,878  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:22,883  INFO [main] optimizer.ColumnPrunerProcFactory: RS 8 oldColExprMap: {VALUE._col2=Column[_col2], VALUE._col3=Column[_col3], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col0=Column[_col0], VALUE._col1=Column[_col1], VALUE._col6=Column[_col6], VALUE._col7=Column[_col7], VALUE._col8=Column[_col8]}
2024-04-23T17:46:22,883  INFO [main] optimizer.ColumnPrunerProcFactory: RS 8 newColExprMap: {VALUE._col2=Column[_col2], VALUE._col3=Column[_col3], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col0=Column[_col0], VALUE._col1=Column[_col1], VALUE._col6=Column[_col6], VALUE._col7=Column[_col7], VALUE._col8=Column[_col8]}
2024-04-23T17:46:22,883  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,883  INFO [main] optimizer.ColumnPrunerProcFactory: RS 2 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:22,884  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:22,884  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:22,898  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:22,902  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,919  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:22,919  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:22,919  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:22,920  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 2)
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.orc.OrcInputFormat]
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:22,921  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:22,921  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T17:46:22,921  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T17:46:22,921  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T17:46:22,935  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-53 is a root stage
  Stage-52 depends on stages: Stage-53
  Stage-54 depends on stages: Stage-52, Stage-55
  Stage-55 depends on stages: Stage-53

STAGE PLANS:
  Stage: Stage-53
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: u
            GatherStats: false
            Select Operator
              expressions: a (type: string), b (type: string)
              outputColumnNames: _col0, _col1
              Reduce Output Operator
                bucketingVersion: 1
                key expressions: _col0 (type: string)
                null sort order: a
                numBuckets: -1
                sort order: +
                Map-reduce partition columns: _col0 (type: string)
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        pfile:MASKED-OUT
        pfile:MASKED-OUT
      Path -> Partition:
        pfile:MASKED-OUT
          Partition
            base file name: ds=today
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds today
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
        pfile:MASKED-OUT
          Partition
            base file name: ds=yesterday
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              ds yesterday
            properties:
              bucket_count 2
              bucket_field_name a
              file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              location pfile:MASKED-OUT
              name default.u
              partition_columns ds
              partition_columns.types string
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.u
                partition_columns ds
                partition_columns.types string
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.u
            name: default.u
      Truncated Path -> Alias:
        /u/ds=today [u]
        /u/ds=yesterday [u]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false
          Select Operator
            expressions: _col0 (type: string), _col1 (type: string)
            outputColumnNames: a, b
            Group By Operator
              aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
              minReductionHashAggr: 0.99
              mode: hash
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
              File Output Operator
                bucketingVersion: 1
                compressed: false
                GlobalTableId: 0
                directory: file:MASKED-OUT
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      column.name.delimiter ,
                      columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                      columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-52
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: INSERT

  Stage: Stage-54
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.t
          Is Table Level Stats: true

  Stage: Stage-55
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              null sort order: 
              numBuckets: -1
              sort order: 
              tag: -1
              value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
              columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T17:46:22,935  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:22,938  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:22,938  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:23,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:23,009  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,009  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:23,096  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-23T17:46:23,096  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-23T17:46:23,096  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-23T17:46:23,097  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-23T17:46:23,097  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
Hive Session ID = b753f8e4-d18c-4b97-8905-f43edfc48d4b
2024-04-23T17:46:23,100  INFO [main] SessionState: Hive Session ID = b753f8e4-d18c-4b97-8905-f43edfc48d4b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:23,100  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-23T17:46:23,105  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/b753f8e4-d18c-4b97-8905-f43edfc48d4b
2024-04-23T17:46:23,108  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/b753f8e4-d18c-4b97-8905-f43edfc48d4b
2024-04-23T17:46:23,110  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/b753f8e4-d18c-4b97-8905-f43edfc48d4b/_tmp_space.db
2024-04-23T17:46:23,112  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-23T17:46:23,112  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:23,112  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6de70de2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3ddd5d32 will be shutdown
2024-04-23T17:46:23,112  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:23,112  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -31
2024-04-23T17:46:23,113  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:23,113  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:23,113  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:23,114  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4a04e9d4, with PersistenceManager: null will be shutdown
2024-04-23T17:46:23,114  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4a04e9d4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3773cfeb created in the thread with id: 1
2024-04-23T17:46:23,129  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4a04e9d4 from thread id: 1
2024-04-23T17:46:23,129  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:23,129  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:23,137  INFO [main] txn.TxnHandler: Added entries to MIN_HISTORY_LEVEL for current txns: ([15]) with min_open_txn: 1
2024-04-23T17:46:23,138  INFO [main] lockmgr.DbTxnManager: Opened txnid:15
2024-04-23T17:46:23,138  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b753f8e4-d18c-4b97-8905-f43edfc48d4b, clientType=HIVECLI]
2024-04-23T17:46:23,138  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-23T17:46:23,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-23T17:46:23,138  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4a04e9d4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3773cfeb will be shutdown
2024-04-23T17:46:23,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-23T17:46:23,138  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -32
2024-04-23T17:46:23,139  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:23,140  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-23T17:46:23,140  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:23,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@378ec278, with PersistenceManager: null will be shutdown
2024-04-23T17:46:23,140  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@378ec278, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@53c4b0a9 created in the thread with id: 1
2024-04-23T17:46:23,142  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@378ec278 from thread id: 1
2024-04-23T17:46:23,142  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:23,142  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:23,148  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-23T17:46:23,148  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-23T17:46:23,148  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@378ec278, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@53c4b0a9 will be shutdown
2024-04-23T17:46:23,148  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@378ec278, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66e58c35 created in the thread with id: 1
2024-04-23T17:46:23,150  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-23T17:46:23,150  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-23T17:46:23,150  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:T, dbName:default, owner:alex, createTime:1713919583, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:23,152  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t
2024-04-23T17:46:23,179  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:U, dbName:default, owner:alex, createTime:1713919583, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:default), FieldSchema(name:b, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:2, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[a], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transactional_properties=default, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER, writeId:0)	
2024-04-23T17:46:23,181  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u
2024-04-23T17:46:23,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:23,201  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,202  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:23,208  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=yesterday
2024-04-23T17:46:23,226  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition : tbl=hive.default.u	
2024-04-23T17:46:23,228  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/u/ds=today
2024-04-23T17:46:23,235  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-23T17:46:23,237  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:23,241  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,241  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-23T17:46:23,241  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:23,241  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:23,241  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:23,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:23,245  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,245  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-23T17:46:23,245  INFO [main] parse.CalcitePlanner: Disabling LLAP IO encode as ETL query is detected
2024-04-23T17:46:23,258  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-23T17:46:23,310  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:23,310  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:23,310  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:23,311  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-23T17:46:23,312  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:23,312  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:23,312  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:23,312  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:23,315  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,317  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_not_null_constraints : tbl=hive.default.t	
2024-04-23T17:46:23,318  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_check_constraints : tbl=hive.default.t	
2024-04-23T17:46:23,326  INFO [main] txn.TxnHandler: Allocated writeId: 1 for txnId: 15
2024-04-23T17:46:23,326  INFO [main] txn.TxnHandler: Allocated write ids for dbName=default, tblName=t (txnIds: [15])
2024-04-23T17:46:23,327  INFO [main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/t/.hive-staging_hive_2024-04-23_17-46-23_111_8139868915923012751-1
2024-04-23T17:46:23,340  INFO [main] parse.CalcitePlanner: Generate an operator pipeline to autogather column stats for table t in query insert into table T values ('abc', 3), ('ghi', null)
2024-04-23T17:46:23,341  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:23,345  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,350  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-23T17:46:23,353  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:23,356  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,356  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-23T17:46:23,356  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-23T17:46:23,361  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/ql/target/tmp/localscratchdir/b753f8e4-d18c-4b97-8905-f43edfc48d4b/hive_2024-04-23_17-46-23_340_7473477320721408574-1/-mr-10000/.hive-staging_hive_2024-04-23_17-46-23_340_7473477320721408574-1
2024-04-23T17:46:23,369  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-23T17:46:23,374  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 oldColExprMap: {VALUE._col2=Column[_col2], VALUE._col3=Column[_col3], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col0=Column[_col0], VALUE._col1=Column[_col1], VALUE._col6=Column[_col6], VALUE._col7=Column[_col7], VALUE._col8=Column[_col8]}
2024-04-23T17:46:23,374  INFO [main] optimizer.ColumnPrunerProcFactory: RS 11 newColExprMap: {VALUE._col2=Column[_col2], VALUE._col3=Column[_col3], VALUE._col4=Column[_col4], VALUE._col5=Column[_col5], VALUE._col0=Column[_col0], VALUE._col1=Column[_col1], VALUE._col6=Column[_col6], VALUE._col7=Column[_col7], VALUE._col8=Column[_col8]}
2024-04-23T17:46:23,375  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 oldColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:23,375  INFO [main] optimizer.ColumnPrunerProcFactory: RS 5 newColExprMap: {VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
2024-04-23T17:46:23,375  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-23T17:46:23,376  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:23,376  INFO [main] optimizer.SortedDynPartitionOptimizer: Sorted dynamic partitioning optimization kicked in..
2024-04-23T17:46:23,389  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.t	
2024-04-23T17:46:23,393  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,406  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:23,407  INFO [main] physical.Vectorizer: Vectorization is enabled for input format(s) [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:23,407  INFO [main] physical.Vectorizer: Validating and vectorizing MapWork... (vectorizedVertexNum 0)
2024-04-23T17:46:23,407  INFO [main] physical.Vectorizer: Map vectorization enabled: true
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map vectorized: true
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 0
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vector.serde.deserialize IS true]
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.mapred.SequenceFileInputFormat]
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 1
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Examining input format to see if vectorization is enabled.
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map vectorization enabled: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map vectorized: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map vectorizedVertexNum: 2
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map enabledConditionsMet: [hive.vectorized.use.vectorized.input.format IS true]
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map enabledConditionsNotMet: [Could not enable vectorization due to partition column names size 1 is greater than the number of table column names size 0 IS false]
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Map inputFileFormatClassNameSet: [org.apache.hadoop.hive.ql.io.NullRowsInputFormat]
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorization enabled: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorized: false
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reduce vectorizedVertexNum: 3
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reducer hive.vectorized.execution.reduce.enabled: true
2024-04-23T17:46:23,408  INFO [main] physical.Vectorizer: Reducer engine: mr
2024-04-23T17:46:23,408  INFO [main] common.HiveStatsUtils: Error requested is 20.0%
2024-04-23T17:46:23,408  INFO [main] common.HiveStatsUtils: Choosing 16 bit vectors..
2024-04-23T17:46:23,408  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-23T17:46:23,423  INFO [main] parse.TestUpdateDeleteSemanticAnalyzer: STAGE DEPENDENCIES:
  Stage-57 is a root stage
  Stage-56 depends on stages: Stage-57
  Stage-58 depends on stages: Stage-56, Stage-59
  Stage-59 depends on stages: Stage-57

STAGE PLANS:
  Stage: Stage-57
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: _dummy_table
            Row Limit Per Split: 1
            GatherStats: false
            Select Operator
              expressions: array(const struct('abc',3),const struct('ghi',null)) (type: array<struct<col1:string,col2:int>>)
              outputColumnNames: _col0
              UDTF Operator
                function name: inline
                Select Operator
                  expressions: col1 (type: string), CAST( col2 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Reduce Output Operator
                    bucketingVersion: 1
                    key expressions: _col0 (type: string)
                    null sort order: a
                    numBuckets: -1
                    sort order: +
                    Map-reduce partition columns: _col0 (type: string)
                    tag: -1
                    value expressions: _col1 (type: string)
                    auto parallelism: false
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: dummy_path
            input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns 
              columns.types 
              file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              location file:MASKED-OUT
              name _dummy_database._dummy_table
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
            serde: org.apache.hadoop.hive.serde2.NullStructSerDe
          
              input format: org.apache.hadoop.hive.ql.io.NullRowsInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucketing_version 2
                column.name.delimiter ,
                columns 
                columns.comments 
                columns.types 
                file.inputformat org.apache.hadoop.hive.ql.io.NullRowsInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                location file:MASKED-OUT
                name _dummy_database._dummy_table
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
              serde: org.apache.hadoop.hive.serde2.NullStructSerDe
              name: _dummy_database._dummy_table
            name: _dummy_database._dummy_table
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          File Output Operator
            bucketingVersion: 1
            compressed: false
            GlobalTableId: 1
            directory: pfile:MASKED-OUT
            NumFilesPerFileSink: 1
            Stats Publishing Key Prefix: pfile:MASKED-OUT
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name a
                  column.name.delimiter ,
                  columns a,b
                  columns.comments 'default','default'
                  columns.types string:string
                  file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  location pfile:MASKED-OUT
                  name default.t
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  transactional true
                  transactional_properties default
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.t
            TotalFiles: 1
            Write Type: INSERT
            GatherStats: true
            MultiFileSpray: false
          Select Operator
            expressions: _col0 (type: string), _col1 (type: string)
            outputColumnNames: a, b
            Group By Operator
              aggregations: max(length(a)), avg(COALESCE(length(a),0)), count(1), count(a), compute_bit_vector_hll(a), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b)
              minReductionHashAggr: 0.99
              mode: hash
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
              File Output Operator
                bucketingVersion: 1
                compressed: false
                GlobalTableId: 0
                directory: file:MASKED-OUT
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      column.name.delimiter ,
                      columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                      columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-56
    Move Operator
      tables:
          replace: false
          source: pfile:MASKED-OUT
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name a
                column.name.delimiter ,
                columns a,b
                columns.comments 'default','default'
                columns.types string:string
                file.inputformat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                file.outputformat org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                location pfile:MASKED-OUT
                name default.t
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                transactional true
                transactional_properties default
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t
          Write Type: INSERT

  Stage: Stage-58
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: pfile:MASKED-OUT
      Column Stats Desc:
          Columns: a, b
          Column Types: string, string
          Table: default.t
          Is Table Level Stats: true

  Stage: Stage-59
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              bucketingVersion: 2
              null sort order: 
              numBuckets: -1
              sort order: 
              tag: -1
              value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
              auto parallelism: false
      Execution mode: vectorized
      Path -> Alias:
        file:MASKED-OUT
      Path -> Partition:
        file:MASKED-OUT
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
              columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                columns.types int,struct<count:bigint,sum:double,input:int>,bigint,bigint,binary,int,struct<count:bigint,sum:double,input:int>,bigint,binary
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
        file:MASKED-OUT
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
          Select Operator
            expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
            File Output Operator
              bucketingVersion: 2
              compressed: false
              GlobalTableId: 0
              directory: file:MASKED-OUT
              NumFilesPerFileSink: 1
              Stats Publishing Key Prefix: file:MASKED-OUT
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    bucketing_version -1
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11
                    columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false


2024-04-23T17:46:23,423  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.T	
2024-04-23T17:46:23,427  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,427  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.T	
2024-04-23T17:46:23,509  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.U	
2024-04-23T17:46:23,511  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-23T17:46:23,512  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.U	
2024-04-23T17:46:23,588  INFO [pool-2-thread-1] lockmgr.DbTxnManager: Shutting down Heartbeater thread pool.
