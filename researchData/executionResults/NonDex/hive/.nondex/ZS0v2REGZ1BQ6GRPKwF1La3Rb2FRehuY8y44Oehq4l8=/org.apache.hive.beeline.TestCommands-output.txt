SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,104259 seconds to load 250 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@6ca18a14]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@6ca18a14) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@205d38da
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,031992 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 132 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/beeline/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/beeline/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={DRFA, console}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(disableAnsi="null", Configuration(HiveLog4j2Test), alwaysWriteExceptions="null", charset="null", footer="null", noConsoleNoAnsi="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Replace=null, header="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(follow="null", direct="null", target="SYSTEM_ERR", bufferSize="null", bufferedIo="null", immediateFlush="null", name="console", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ignoreExceptions="null", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(PatternSelector=null, pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", Configuration(HiveLog4j2Test), noConsoleNoAnsi="null", Replace=null, header="null", alwaysWriteExceptions="null", disableAnsi="null", charset="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(modulate="true", interval="1", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(compressionLevel="null", Configuration(HiveLog4j2Test), max="30", stopCustomActionsOnError="null", ={}, min="null", fileIndex="null", tempCompressedFilePattern="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileOwner="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), filePermissions="null", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), fileName="/home/alex/Repositories/hive/beeline/target/tmp/log/hive.log", advertise="null", append="null", advertiseURI="null", fileGroup="null", filePattern="/home/alex/Repositories/hive/beeline/target/tmp/log/hive.log.%d{yyyy-MM-dd}", bufferSize="null", bufferedIo="null", immediateFlush="null", Configuration(HiveLog4j2Test), ignoreExceptions="null", name="DRFA", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ={}, Filter=null)
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/beeline/target/tmp/log/hive.log seek to 5750599
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T08:10:12.044-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-08:10:20.198, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-08:10:20.199, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@479cbee5 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@342c38f8...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@342c38f8 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@1f2f9244
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@6ca18a14
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/beeline/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@6ca18a14) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@6ca18a14] started OK.
Connecting to jdbc:hive2://
2024-04-24T08:10:20,571  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/beeline/target/test-classes/hive-site.xml
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-24T08:10:20,920  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T08:10:20,988  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:10:20,988  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:10:20,989  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
Hive Session ID = 8abea6a1-4683-4b96-b9a6-1999fa0e7f31
2024-04-24T08:10:21,028  INFO [main] SessionState: Hive Session ID = 8abea6a1-4683-4b96-b9a6-1999fa0e7f31
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:21,043  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:21,335  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/8abea6a1-4683-4b96-b9a6-1999fa0e7f31
2024-04-24T08:10:21,339  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/8abea6a1-4683-4b96-b9a6-1999fa0e7f31
2024-04-24T08:10:21,342  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/8abea6a1-4683-4b96-b9a6-1999fa0e7f31/_tmp_space.db
2024-04-24T08:10:21,366  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=8abea6a1-4683-4b96-b9a6-1999fa0e7f31, clientType=HIVESERVER2]
2024-04-24T08:10:21,516  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:10:21,666  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:21,701  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:10:21,708  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T08:10:21,708  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T08:10:21,728  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:10:21,733  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T08:10:22,015  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:10:22,018  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T08:10:22,537  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T08:10:22,538  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: null will be shutdown
2024-04-24T08:10:22,560  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56637cff created in the thread with id: 1
2024-04-24T08:10:24,460  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T08:10:24,460  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T08:10:24,460  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae from thread id: 1
2024-04-24T08:10:24,538  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T08:10:24,541  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T08:10:24,567  INFO [main] metastore.HMSHandler: No user is added in admin role, since config is empty
2024-04-24T08:10:24,574  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T08:10:24,575  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T08:10:24,576  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T08:10:24,577  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T08:10:24,598  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:10:24,602  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T08:10:24,604  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:10:24,605  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T08:10:24,608  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T08:10:24,611  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T08:10:24,613  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T08:10:24,614  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T08:10:24,619  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T08:10:24,620  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T08:10:24,624  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:10:24,772  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:10:25,617  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,622  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,628  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,634  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,634  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,634  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,636  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T08:10:25,720  INFO [main] service.CompositeService: Operation log root directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs
2024-04-24T08:10:25,721  INFO [main] service.CompositeService: HiveServer2: Background operation thread pool size: 100
2024-04-24T08:10:25,721  INFO [main] service.CompositeService: HiveServer2: Background operation thread wait queue size: 100
2024-04-24T08:10:25,722  INFO [main] service.CompositeService: HiveServer2: Background operation thread keepalive time: 10 seconds
2024-04-24T08:10:25,724  INFO [main] service.CompositeService: Connections limit are user: 0 ipaddress: 0 user-ipaddress: 0
2024-04-24T08:10:25,726  INFO [main] cleanup.EventualCleanupService: EventualCleanupService started with 10 threads and queue of size 10000
2024-04-24T08:10:25,734  INFO [main] service.AbstractService: Service:OperationManager is inited.
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.NullAppender].
DEBUG StatusLogger createNullAppender()
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-2
DEBUG StatusLogger Log4j2 ConfigurationScheduler starting 2 threads
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.NullAppender].
DEBUG StatusLogger createNullAppender()
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-3
2024-04-24T08:10:25,748  INFO [main] service.AbstractService: Service:SessionManager is inited.
2024-04-24T08:10:25,748  INFO [main] service.AbstractService: Service:CLIService is inited.
2024-04-24T08:10:25,749  INFO [main] service.AbstractService: Service:OperationManager is started.
2024-04-24T08:10:25,749  INFO [main] service.AbstractService: Service:SessionManager is started.
2024-04-24T08:10:25,750  INFO [main] service.AbstractService: Service:CLIService is started.
2024-04-24T08:10:25,751  INFO [main] service.AbstractService: Service:ThriftBinaryCLIService is inited.
2024-04-24T08:10:25,753  INFO [main] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V10
2024-04-24T08:10:25,758  INFO [main] thrift.ThriftCLIService: Creating Hive session handle for user [null] from IP null
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:25,765  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:25,775  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:25,778  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:25,782  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b/_tmp_space.db
2024-04-24T08:10:25,783  INFO [main] session.SessionState: Reloading auxiliary JAR files
2024-04-24T08:10:25,783  WARN [main] session.SessionState: Configuration hive.reloadable.aux.jars.path not specified
2024-04-24T08:10:25,787  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:10:25,788  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:10:25,790  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56637cff will be shutdown
2024-04-24T08:10:25,791  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35e98af created in the thread with id: 1
2024-04-24T08:10:25,796  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:10:25,796  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:10:25,798  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T08:10:25,826  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:10:25,826  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@252a8aae, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35e98af will be shutdown
2024-04-24T08:10:25,826  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:10:25,826  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T08:10:25,828  INFO [main] session.HiveSessionImpl: Operation log session directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:25,833  INFO [main] service.CompositeService: Session opened, SessionHandle [bb7f7dcf-2cef-4a11-9069-957efd549c6b], current sessions:1
2024-04-24T08:10:25,837  INFO [main] thrift.ThriftCLIService: Login attempt is successful for user : null
Connected to: Apache Hive (version 4.0.0-SNAPSHOT)
Error: Couldn't load manifest attributes. (state=,code=0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
2024-04-24T08:10:25,923  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] session.HiveSessionImpl: executing select 3
2024-04-24T08:10:25,936  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] operation.OperationManager: Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bb0a704f-f103-4094-8a7f-968e266bcef4] SessionHandle [bb7f7dcf-2cef-4a11-9069-957efd549c6b]
2024-04-24T08:10:25,941  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] common.LogUtils: Thread context registration is done.
DEBUG StatusLogger Building Plugin[name=filter, class=org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter].
DEBUG StatusLogger createFilter(loggingLevel="EXECUTION")
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(charset="null", Replace=null, noConsoleNoAnsi="null", Configuration(HiveLog4j2Test), disableAnsi="null", PatternSelector=null, header="null", alwaysWriteExceptions="null", footer="null", pattern="%-5p : %m%n")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b/alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328", append="null", name="query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), NameFilter(org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter with name NameFilter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b/alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328
DEBUG StatusLogger Building Plugin[name=, class=org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter].
DEBUG StatusLogger createFilter()
DEBUG StatusLogger Building Plugin[name=, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(noConsoleNoAnsi="null", disableAnsi="null", Configuration(HiveLog4j2Test), Replace=null, footer="null", alwaysWriteExceptions="null", PatternSelector=null, header="null", pattern="%-5p : %m%n", charset="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b/alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328.test", append="null", name="test-query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), test-filter(org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter with name test-filter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b/alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328.test
2024-04-24T08:10:25,957  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] operation.SQLOperation: [opType=EXECUTE_STATEMENT, queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328, startTime=1713971425932, sessionId=bb7f7dcf-2cef-4a11-9069-957efd549c6b, createTime=1713971425761, userName=null, ipAddress=null]
2024-04-24T08:10:26,043  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] ql.Driver: Compiling command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328): select 3
2024-04-24T08:10:26,073  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
2024-04-24T08:10:26,073  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:10:26,074  WARN [Metastore-RuntimeStats-Loader-1] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T08:10:26,076  INFO [Metastore-RuntimeStats-Loader-1] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:10:26,078  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:26,078  INFO [Metastore-RuntimeStats-Loader-1] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:10:26,080  INFO [Metastore-RuntimeStats-Loader-1] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2796c897, with PersistenceManager: null will be shutdown
2024-04-24T08:10:26,081  INFO [Metastore-RuntimeStats-Loader-1] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2796c897, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1e556514 created in the thread with id: 86
2024-04-24T08:10:26,089  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2796c897 from thread id: 86
2024-04-24T08:10:26,089  INFO [Metastore-RuntimeStats-Loader-1] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:10:26,090  INFO [Metastore-RuntimeStats-Loader-1] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:10:26,091  INFO [Metastore-RuntimeStats-Loader-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_runtime_stats	
2024-04-24T08:10:26,817  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:26,818  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: null will be shutdown
2024-04-24T08:10:26,819  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@361cd35c created in the thread with id: 1
2024-04-24T08:10:26,823  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211 from thread id: 1
2024-04-24T08:10:27,114  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] reflections.Reflections: Reflections took 254 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:10:27,359  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] reflections.Reflections: Reflections took 173 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:10:27,526  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] reflections.Reflections: Reflections took 159 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:10:27,635  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Starting caching scope for: alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328
2024-04-24T08:10:27,638  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:10:27,638  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=bb7f7dcf-2cef-4a11-9069-957efd549c6b, clientType=HIVESERVER2]
2024-04-24T08:10:27,641  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-24T08:10:27,642  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T08:10:27,642  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T08:10:27,647  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T08:10:27,658  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-24T08:10:29,035  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive._dummy_database._dummy_table	
2024-04-24T08:10:29,772  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T08:10:29,776  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive._dummy_database._dummy_table	
2024-04-24T08:10:29,791  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T08:10:29,791  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T08:10:29,850  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b/hive_2024-04-24_08-10-25_987_8953085987921210718-1/-mr-10001/.hive-staging_hive_2024-04-24_08-10-25_987_8953085987921210718-1
2024-04-24T08:10:29,889  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-24T08:10:29,991  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-24T08:10:30,020  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Completed plan generation
2024-04-24T08:10:30,020  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] parse.CalcitePlanner: Ending caching scope for: alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328
2024-04-24T08:10:30,020  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:10:30,022  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
2024-04-24T08:10:30,045  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-04-24T08:10:30,048  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-04-24T08:10:30,052  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.SelectOperator: SELECT null
2024-04-24T08:10:30,052  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-04-24T08:10:30,057  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:10:30,058  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=9, isCompatibleWith_(Configuration)=0, getAllFunctions_()=27, getAllTableConstraints_(AllTableConstraintsRequest)=103}
2024-04-24T08:10:30,059  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] ql.Driver: Completed compiling command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328); Time taken: 4.017 seconds
2024-04-24T08:10:30,062  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] common.LogUtils: Unregistered logging context.
2024-04-24T08:10:30,064  INFO [HiveServer2-Background-Pool: Thread-93] common.LogUtils: Thread context registration is done.
2024-04-24T08:10:30,065  INFO [HiveServer2-Background-Pool: Thread-93] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:10:30,066  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:10:30,071  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Executing command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328): select 3
2024-04-24T08:10:30,076  INFO [HiveServer2-Background-Pool: Thread-93] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:10:30,076  INFO [HiveServer2-Background-Pool: Thread-93] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T08:10:30,076  INFO [HiveServer2-Background-Pool: Thread-93] ql.Driver: Completed executing command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328); Time taken: 0.005 seconds
2024-04-24T08:10:30,077  INFO [HiveServer2-Background-Pool: Thread-93] common.LogUtils: Unregistered logging context.
INFO  : Compiling command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328): select 3
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328); Time taken: 4.017 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328): select 3
INFO  : Completed executing command(queryId=alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328); Time taken: 0.005 seconds
DEBUG : Shutting down query select 3
2024-04-24T08:10:30,099  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] operation.OperationManager: Closing operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bb0a704f-f103-4094-8a7f-968e266bcef4]
2024-04-24T08:10:30,099  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] operation.OperationManager: Removed queryId: alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328 corresponding to operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=bb0a704f-f103-4094-8a7f-968e266bcef4] with tag: null
2024-04-24T08:10:30,100  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] ql.Context: Deleting scratch dir: file:/home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b/hive_2024-04-24_08-10-25_987_8953085987921210718-1
2024-04-24T08:10:30,100  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] cleanup.EventualCleanupService: Delete file:/home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b/hive_2024-04-24_08-10-25_987_8953085987921210718-1 operation was queued
2024-04-24T08:10:30,101  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.TableScanOperator: Closing Operator: TS[0]
2024-04-24T08:10:30,101  INFO [EventualCleanupService thread 1] cleanup.EventualCleanupService: Deleted file:/home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b/hive_2024-04-24_08-10-25_987_8953085987921210718-1
2024-04-24T08:10:30,102  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.TableScanOperator: RECORDS_OUT_OPERATOR_TS_0:0, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T08:10:30,102  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.SelectOperator: Closing Operator: SEL[1]
2024-04-24T08:10:30,102  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.SelectOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_SEL_1:0, 
2024-04-24T08:10:30,102  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-04-24T08:10:30,102  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] exec.ListSinkOperator: RECORDS_OUT_OPERATOR_LIST_SINK_3:0, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T08:10:30,103  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] operation.SQLOperation: Closing operation log /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b/alex_20240424081025_8029d90b-cb70-4c8f-82e4-135db45af328 without delay
Error: Failed to wait for operation to complete (state=08S01,code=0)
Closing: 0: jdbc:hive2://
2024-04-24T08:10:30,105  INFO [main] service.CompositeService: Session closed, SessionHandle [bb7f7dcf-2cef-4a11-9069-957efd549c6b], current sessions:0
2024-04-24T08:10:30,105  INFO [bb7f7dcf-2cef-4a11-9069-957efd549c6b main] session.HiveSessionImpl: Operation log session directory is deleted: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:30,106  INFO [main] cleanup.EventualCleanupService: Delete /tmp/hive/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b operation was queued
2024-04-24T08:10:30,106  INFO [main] cleanup.EventualCleanupService: Delete /home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b operation was queued
2024-04-24T08:10:30,106  INFO [EventualCleanupService thread 0] cleanup.EventualCleanupService: Deleted /tmp/hive/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:30,107  INFO [EventualCleanupService thread 2] cleanup.EventualCleanupService: Deleted /home/alex/Repositories/hive/beeline/target/tmp/alex/bb7f7dcf-2cef-4a11-9069-957efd549c6b
2024-04-24T08:10:30,113  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:10:30,113  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3b435211, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@361cd35c will be shutdown
2024-04-24T08:10:30,114  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:10:30,114  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
Connecting to jdbc:hive2://
2024-04-24T08:10:30,181  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T08:10:30,181  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hiveserver2site does not exist
2024-04-24T08:10:30,182  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
Hive Session ID = 841fd929-60af-4b94-a72e-83649547679d
2024-04-24T08:10:30,182  INFO [main] SessionState: Hive Session ID = 841fd929-60af-4b94-a72e-83649547679d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:30,183  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:30,191  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/841fd929-60af-4b94-a72e-83649547679d
2024-04-24T08:10:30,195  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/841fd929-60af-4b94-a72e-83649547679d
2024-04-24T08:10:30,199  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/841fd929-60af-4b94-a72e-83649547679d/_tmp_space.db
2024-04-24T08:10:30,199  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=841fd929-60af-4b94-a72e-83649547679d, clientType=HIVESERVER2]
2024-04-24T08:10:30,201  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:10:30,202  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:30,202  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:10:30,203  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: null will be shutdown
2024-04-24T08:10:30,203  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5a8b42a3 created in the thread with id: 1
2024-04-24T08:10:30,207  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2 from thread id: 1
2024-04-24T08:10:30,207  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:10:30,208  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:10:30,208  INFO [main] service.CompositeService: Operation log root directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs
2024-04-24T08:10:30,208  INFO [main] service.CompositeService: HiveServer2: Background operation thread pool size: 100
2024-04-24T08:10:30,208  INFO [main] service.CompositeService: HiveServer2: Background operation thread wait queue size: 100
2024-04-24T08:10:30,208  INFO [main] service.CompositeService: HiveServer2: Background operation thread keepalive time: 10 seconds
2024-04-24T08:10:30,209  INFO [main] service.CompositeService: Connections limit are user: 0 ipaddress: 0 user-ipaddress: 0
2024-04-24T08:10:30,210  INFO [main] cleanup.EventualCleanupService: EventualCleanupService started with 10 threads and queue of size 10000
ERROR StatusLogger Log4j2 ConfigurationScheduler attempted to increment scheduled items after start
ERROR StatusLogger Log4j2 ConfigurationScheduler attempted to increment scheduled items after start
2024-04-24T08:10:30,212  INFO [main] service.AbstractService: Service:OperationManager is inited.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:SessionManager is inited.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:CLIService is inited.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:OperationManager is started.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:SessionManager is started.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:CLIService is started.
2024-04-24T08:10:30,213  INFO [main] service.AbstractService: Service:ThriftBinaryCLIService is inited.
2024-04-24T08:10:30,213  INFO [main] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V10
2024-04-24T08:10:30,213  INFO [main] thrift.ThriftCLIService: Creating Hive session handle for user [null] from IP null
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:30,215  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T08:10:30,224  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,228  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/beeline/target/tmp/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,232  INFO [main] session.SessionState: Created HDFS directory: /tmp/hive/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee/_tmp_space.db
2024-04-24T08:10:30,232  INFO [main] session.SessionState: Reloading auxiliary JAR files
2024-04-24T08:10:30,232  WARN [main] session.SessionState: Configuration hive.reloadable.aux.jars.path not specified
2024-04-24T08:10:30,232  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:10:30,232  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@351e86b2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5a8b42a3 will be shutdown
2024-04-24T08:10:30,233  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:10:30,233  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T08:10:30,233  INFO [main] session.HiveSessionImpl: Operation log session directory is created: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,233  INFO [main] service.CompositeService: Session opened, SessionHandle [e47686f2-56ed-48b3-9045-45515ad4a2ee], current sessions:1
2024-04-24T08:10:30,233  INFO [main] thrift.ThriftCLIService: Login attempt is successful for user : null
Connected to: Apache Hive (version 4.0.0-SNAPSHOT)
Error: Couldn't load manifest attributes. (state=,code=0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
2024-04-24T08:10:30,236  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] session.HiveSessionImpl: executing create table t1(x int)
2024-04-24T08:10:30,237  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] operation.OperationManager: Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4efa6525-b585-42b4-834e-67e4df95e8f0] SessionHandle [e47686f2-56ed-48b3-9045-45515ad4a2ee]
2024-04-24T08:10:30,238  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] common.LogUtils: Thread context registration is done.
DEBUG StatusLogger Building Plugin[name=filter, class=org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter].
DEBUG StatusLogger createFilter(loggingLevel="EXECUTION")
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%-5p : %m%n", header="null", noConsoleNoAnsi="null", footer="null", disableAnsi="null", PatternSelector=null, alwaysWriteExceptions="null", Configuration(HiveLog4j2Test), Replace=null, charset="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee/alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e", append="null", name="query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), NameFilter(org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter with name NameFilter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee/alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e
DEBUG StatusLogger Building Plugin[name=, class=org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter].
DEBUG StatusLogger createFilter()
DEBUG StatusLogger Building Plugin[name=, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(Configuration(HiveLog4j2Test), disableAnsi="null", header="null", alwaysWriteExceptions="null", pattern="%-5p : %m%n", Replace=null, footer="null", noConsoleNoAnsi="null", PatternSelector=null, charset="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender].
DEBUG StatusLogger createAppender(fileName="/home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee/alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e.test", append="null", name="test-query-file-appender", immediateFlush="null", bufferSize="null", ignoreExceptions="null", PatternLayout(org.apache.logging.log4j.core.layout.PatternLayout with name PatternLayout), test-filter(org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter with name test-filter), advertise="null", advertiseURI="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Starting RandomAccessFileManager /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee/alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e.test
2024-04-24T08:10:30,246  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] operation.SQLOperation: [opType=EXECUTE_STATEMENT, queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e, startTime=1713971430236, sessionId=e47686f2-56ed-48b3-9045-45515ad4a2ee, createTime=1713971430213, userName=null, ipAddress=null]
2024-04-24T08:10:30,247  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] ql.Driver: Compiling command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e): create table t1(x int)
2024-04-24T08:10:30,250  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T08:10:30,252  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:30,252  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T08:10:30,253  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: null will be shutdown
2024-04-24T08:10:30,253  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a69014e created in the thread with id: 1
2024-04-24T08:10:30,257  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa from thread id: 1
2024-04-24T08:10:30,257  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T08:10:30,258  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T08:10:30,259  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] parse.CalcitePlanner: Starting caching scope for: alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e
2024-04-24T08:10:30,259  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T08:10:30,259  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e47686f2-56ed-48b3-9045-45515ad4a2ee, clientType=HIVESERVER2]
2024-04-24T08:10:30,260  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] parse.CalcitePlanner: Creating table default.t1 position=13
2024-04-24T08:10:30,265  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T08:10:30,433  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] reflections.Reflections: Reflections took 161 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T08:10:30,560  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] parse.CalcitePlanner: Ending caching scope for: alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e
2024-04-24T08:10:30,560  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T08:10:30,561  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T08:10:30,561  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T08:10:30,561  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getDatabase_(String)=5, flushCache_()=0}
2024-04-24T08:10:30,561  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] ql.Driver: Completed compiling command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e); Time taken: 0.314 seconds
2024-04-24T08:10:30,562  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] common.LogUtils: Unregistered logging context.
2024-04-24T08:10:30,563  INFO [HiveServer2-Background-Pool: Thread-115] common.LogUtils: Thread context registration is done.
2024-04-24T08:10:30,563  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecDriver: Execution #1 of query
2024-04-24T08:10:30,564  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T08:10:30,564  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Executing command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e): create table t1(x int)
2024-04-24T08:10:30,567  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T08:10:30,652  INFO [HiveServer2-Background-Pool: Thread-115] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:t1, dbName:default, owner:alex, createTime:1713971430, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:x, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numFiles=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"x":"true"}}, bucketing_version=2, rawDataSize=0, numRows=0, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T08:10:30,654  INFO [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T08:10:30,655  INFO [HiveServer2-Background-Pool: Thread-115] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658aa008, with PersistenceManager: null will be shutdown
2024-04-24T08:10:30,656  INFO [HiveServer2-Background-Pool: Thread-115] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658aa008, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@a45ad92 created in the thread with id: 115
2024-04-24T08:10:30,660  INFO [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658aa008 from thread id: 115
2024-04-24T08:10:30,825  WARN [HiveServer2-Background-Pool: Thread-115] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
2024-04-24T08:10:30,827 ERROR [HiveServer2-Background-Pool: Thread-115] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy38.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233)
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

2024-04-24T08:10:30,848 ERROR [HiveServer2-Background-Pool: Thread-115] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
2024-04-24T08:10:30,849 ERROR [HiveServer2-Background-Pool: Thread-115] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
2024-04-24T08:10:30,853  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T08:10:30,853  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.t1 already exists) retryPossible: false
2024-04-24T08:10:30,853  INFO [HiveServer2-Background-Pool: Thread-115] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.t1 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
2024-04-24T08:10:30,853 ERROR [HiveServer2-Background-Pool: Thread-115] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
2024-04-24T08:10:30,854  INFO [HiveServer2-Background-Pool: Thread-115] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T08:10:30,854  INFO [HiveServer2-Background-Pool: Thread-115] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T08:10:30,854  INFO [HiveServer2-Background-Pool: Thread-115] ql.Driver: Completed executing command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e); Time taken: 0.29 seconds
2024-04-24T08:10:30,858 ERROR [HiveServer2-Background-Pool: Thread-115] operation.SQLOperation: Error running hive query
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:246) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) ~[classes/:?]
	... 11 more
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) ~[classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) ~[classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) ~[classes/:?]
	... 11 more
2024-04-24T08:10:30,859  INFO [HiveServer2-Background-Pool: Thread-115] common.LogUtils: Unregistered logging context.
INFO  : Compiling command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e): create table t1(x int)
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e); Time taken: 0.314 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e): create table t1(x int)
INFO  : Starting task [Stage-0:DDL] in serial mode
DEBUG : Task getting executed using mapred tag : alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e,userid=null
ERROR : Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
ERROR : DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.t1 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:88) [classes/:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [classes/:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_402]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_402]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) [hadoop-common-3.1.0.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) [classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.t1 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy37.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy38.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 27 more
ERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.t1 already exists)
DEBUG : Shutting down query create table t1(x int)
INFO  : Completed executing command(queryId=alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e); Time taken: 0.29 seconds
DEBUG : Shutting down query create table t1(x int)
2024-04-24T08:10:30,867  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] operation.OperationManager: Closing operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4efa6525-b585-42b4-834e-67e4df95e8f0]
2024-04-24T08:10:30,867  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] operation.OperationManager: Removed queryId: alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e corresponding to operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=4efa6525-b585-42b4-834e-67e4df95e8f0] with tag: null
2024-04-24T08:10:30,867  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] operation.SQLOperation: Closing operation log /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee/alex_20240424081030_03f4914b-2fef-401a-93fe-0e49dc44dd3e without delay
Error: Failed to wait for operation to complete (state=08S01,code=0)
Closing: 0: jdbc:hive2://
2024-04-24T08:10:30,868  INFO [main] service.CompositeService: Session closed, SessionHandle [e47686f2-56ed-48b3-9045-45515ad4a2ee], current sessions:0
2024-04-24T08:10:30,869  INFO [e47686f2-56ed-48b3-9045-45515ad4a2ee main] session.HiveSessionImpl: Operation log session directory is deleted: /home/alex/Repositories/hive/beeline/target/tmp/alex/operation_logs/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,869  INFO [main] cleanup.EventualCleanupService: Delete /tmp/hive/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee operation was queued
2024-04-24T08:10:30,869  INFO [main] cleanup.EventualCleanupService: Delete /home/alex/Repositories/hive/beeline/target/tmp/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee operation was queued
2024-04-24T08:10:30,869  INFO [EventualCleanupService thread 0] cleanup.EventualCleanupService: Deleted /tmp/hive/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,869  INFO [EventualCleanupService thread 1] cleanup.EventualCleanupService: Deleted /home/alex/Repositories/hive/beeline/target/tmp/alex/e47686f2-56ed-48b3-9045-45515ad4a2ee
2024-04-24T08:10:30,871  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T08:10:30,871  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7831d1aa, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a69014e will be shutdown
2024-04-24T08:10:30,871  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T08:10:30,871  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
