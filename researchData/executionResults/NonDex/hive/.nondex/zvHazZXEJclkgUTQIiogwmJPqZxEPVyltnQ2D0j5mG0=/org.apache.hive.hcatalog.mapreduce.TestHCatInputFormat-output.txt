2024-04-24T09:08:28,828  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/hcatalog/core/target/testconf/hive-site.xml
2024-04-24T09:08:29,183  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T09:08:29,251  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:08:29,252  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:08:29,252  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:08:29,252  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:08:29,253  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:08:29,253  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:08:29,253  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:08:29,254  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:08:29,254  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:08:29,254  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:08:29,254  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:08:29,503  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:29,645  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:08:29,677  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:29,685  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T09:08:29,685  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T09:08:29,710  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:08:29,717  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T09:08:30,364  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:08:30,369  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
2024-04-24T09:08:31,026  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T09:08:31,026  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: null will be shutdown
2024-04-24T09:08:31,049  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 created in the thread with id: 1
2024-04-24T09:08:33,378  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T09:08:33,378  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T09:08:33,378  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9 from thread id: 1
2024-04-24T09:08:33,648  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T09:08:33,681  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T09:08:33,712  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T09:08:33,714  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T09:08:33,831  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T09:08:33,837  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T09:08:33,838  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T09:08:33,839  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T09:08:33,840  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T09:08:33,842  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T09:08:33,865  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:08:33,868  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T09:08:33,869  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:08:33,870  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T09:08:33,872  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T09:08:33,874  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T09:08:33,876  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T09:08:33,876  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T09:08:33,881  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T09:08:33,883  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 970eb033-1ea8-4fd9-870b-cd378d82dcad
2024-04-24T09:08:34,033  INFO [main] SessionState: Hive Session ID = 970eb033-1ea8-4fd9-870b-cd378d82dcad
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:08:34,046  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:08:34,123  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/970eb033-1ea8-4fd9-870b-cd378d82dcad
2024-04-24T09:08:34,128  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/970eb033-1ea8-4fd9-870b-cd378d82dcad
2024-04-24T09:08:34,131  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/970eb033-1ea8-4fd9-870b-cd378d82dcad/_tmp_space.db
2024-04-24T09:08:34,133  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data/intString.seq
2024-04-24T09:08:34,175  INFO [main] compress.CodecPool: Got brand-new compressor [.deflate]
2024-04-24T09:08:34,243  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): drop table if exists test_bad_records
2024-04-24T09:08:35,507  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,511  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,516  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,517  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,517  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,517  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,520  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T09:08:35,587  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:35,587  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:35,589  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3659d7b1 will be shutdown
2024-04-24T09:08:35,589  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 created in the thread with id: 1
2024-04-24T09:08:35,604  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:35,605  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:35,644  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T09:08:35,929  INFO [main] reflections.Reflections: Reflections took 202 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:08:36,109  INFO [main] reflections.Reflections: Reflections took 120 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:08:36,243  INFO [main] reflections.Reflections: Reflections took 127 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:08:36,255  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:36,404  INFO [main] reflections.Reflections: Reflections took 116 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T09:08:36,467  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:36,469  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:36,473  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:36,473  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getAllFunctions_()=48, flushCache_()=0, isCompatibleWith_(Configuration)=1}
2024-04-24T09:08:36,473  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 2.231 seconds
2024-04-24T09:08:36,474  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:36,475  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:36,478  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): drop table if exists test_bad_records
2024-04-24T09:08:36,482  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:08:36,483  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:36,499  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:36,500  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T09:08:36,500  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 0.021 seconds
2024-04-24T09:08:36,501  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:08:36,574  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89
2024-04-24T09:08:36,577  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T09:08:36,592  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=970eb033-1ea8-4fd9-870b-cd378d82dcad, clientType=HIVECLI]
2024-04-24T09:08:36,594  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T09:08:36,596  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:08:36,596  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c20aab9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@16cf8438 will be shutdown
2024-04-24T09:08:36,596  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:36,596  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T09:08:36,597  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:36,599  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:08:36,599  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:36,600  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3, with PersistenceManager: null will be shutdown
2024-04-24T09:08:36,600  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6c9a3661 created in the thread with id: 1
2024-04-24T09:08:36,611  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3 from thread id: 1
2024-04-24T09:08:36,611  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:36,611  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:36,618  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T09:08:36,624  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:36,624  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:36,625  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6c9a3661 will be shutdown
2024-04-24T09:08:36,625  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@48ccbb32 created in the thread with id: 1
2024-04-24T09:08:36,632  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:36,633  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:36,637  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T09:08:36,658  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89
2024-04-24T09:08:36,658  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:36,659  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:36,659  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:36,659  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T09:08:36,661  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 0.158 seconds
2024-04-24T09:08:36,661  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:36,662  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:36,662  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:08:36,662  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:08:36,663  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T09:08:36,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:08:36,663  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@41f785e3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@48ccbb32 will be shutdown
2024-04-24T09:08:36,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:36,664  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-24T09:08:36,818  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:36,819  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:08:36,819  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:36,820  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: null will be shutdown
2024-04-24T09:08:36,820  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4438938e created in the thread with id: 1
2024-04-24T09:08:36,824  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac from thread id: 1
2024-04-24T09:08:36,824  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:36,825  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:36,825  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713974916, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:underscore_int, type:int, comment:from deserializer), FieldSchema(name:myint, type:string, comment:from deserializer), FieldSchema(name:mystring, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, numFiles=0, bucketing_version=2, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, rawDataSize=0, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T09:08:36,836  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/warehouse/test_bad_records
2024-04-24T09:08:36,985  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:36,985  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=159}
2024-04-24T09:08:36,985  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 0.323 seconds
2024-04-24T09:08:36,987  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data' into table test_bad_records
2024-04-24T09:08:36,989  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89
2024-04-24T09:08:36,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,098  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,141  INFO [main] compress.CodecPool: Got brand-new decompressor [.deflate]
2024-04-24T09:08:37,150  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89
2024-04-24T09:08:37,151  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:37,151  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:37,151  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:37,151  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=106, flushCache_()=0}
2024-04-24T09:08:37,151  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 0.165 seconds
2024-04-24T09:08:37,151  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:37,151  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:37,152  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data' into table test_bad_records
2024-04-24T09:08:37,152  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T09:08:37,153  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data
2024-04-24T09:08:37,154  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,170  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,174  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,189  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,208  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:08:37,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:08:37,274  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T09:08:37,275  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,291  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,291  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T09:08:37,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,307  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,310  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:08:37,310  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:08:37,358  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T09:08:37,358  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:37,358  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=64, alter_table_(String, String, String, Table, EnvironmentContext, String)=110}
2024-04-24T09:08:37,358  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090829_fea0773f-4b4e-4921-a0e4-6451153e8c89); Time taken: 0.206 seconds
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:08:37,410  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:08:37,411  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:08:37,411  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:08:37,411  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:08:37,411  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T09:08:37,412  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T09:08:37,425  INFO [main] common.HiveClientCache: Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
2024-04-24T09:08:37,434  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:37,434  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:37,435  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4438938e will be shutdown
2024-04-24T09:08:37,435  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@10bdfbcc created in the thread with id: 1
2024-04-24T09:08:37,445  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:37,445  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:37,479  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T09:08:37,489  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:37,503  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:37,626  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:37,626  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T09:08:37,654  INFO [main] beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2024-04-24T09:08:37,666  WARN [main] impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
2024-04-24T09:08:37,682  INFO [main] impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-04-24T09:08:37,682  INFO [main] impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-24T09:08:37,727  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T09:08:37,736  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T09:08:37,746  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T09:08:37,758  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T09:08:37,797  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T09:08:37,827  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local345244930_0001
2024-04-24T09:08:37,827  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T09:08:37,937  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T09:08:37,938  INFO [main] mapreduce.Job: Running job: job_local345244930_0001
2024-04-24T09:08:37,939  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T09:08:37,949  INFO [Thread-52] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:08:37,949  INFO [Thread-52] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:08:37,949  INFO [Thread-52] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T09:08:37,967  INFO [Thread-52] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T09:08:37,967  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local345244930_0001_m_000000_0
2024-04-24T09:08:37,992  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:08:37,992  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:08:38,007  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T09:08:38,009  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@3efdf5e8
2024-04-24T09:08:38,049  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=underscore_int,myint,mystring, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713974917}
2024-04-24T09:08:38,052  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T09:08:38,053  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T09:08:38,053  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T09:08:38,053  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T09:08:38,053  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T09:08:38,053  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T09:08:38,054  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T09:08:38,054  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T09:08:38,054  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T09:08:38,055  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,056  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T09:08:38,056  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T09:08:38,056  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T09:08:38,056  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T09:08:38,057  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T09:08:38,057  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T09:08:38,057  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T09:08:38,057  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T09:08:38,058  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T09:08:38,058  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,058  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 21	21	21	
2024-04-24T09:08:38,059  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 22	22	22	
2024-04-24T09:08:38,059  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 23	23	23	
2024-04-24T09:08:38,059  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 24	24	24	
2024-04-24T09:08:38,059  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 25	25	25	
2024-04-24T09:08:38,060  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 26	26	26	
2024-04-24T09:08:38,060  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 27	27	27	
2024-04-24T09:08:38,060  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 28	28	28	
2024-04-24T09:08:38,060  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 29	29	29	
2024-04-24T09:08:38,061  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (3 out of 30 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,061  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 31	31	31	
2024-04-24T09:08:38,061  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 32	32	32	
2024-04-24T09:08:38,061  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 33	33	33	
2024-04-24T09:08:38,062  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 34	34	34	
2024-04-24T09:08:38,062  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 35	35	35	
2024-04-24T09:08:38,062  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 36	36	36	
2024-04-24T09:08:38,062  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 37	37	37	
2024-04-24T09:08:38,062  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 38	38	38	
2024-04-24T09:08:38,063  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 39	39	39	
2024-04-24T09:08:38,063  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (4 out of 40 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,063  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 41	41	41	
2024-04-24T09:08:38,064  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 42	42	42	
2024-04-24T09:08:38,064  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 43	43	43	
2024-04-24T09:08:38,064  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 44	44	44	
2024-04-24T09:08:38,064  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 45	45	45	
2024-04-24T09:08:38,064  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 46	46	46	
2024-04-24T09:08:38,065  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 47	47	47	
2024-04-24T09:08:38,065  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 48	48	48	
2024-04-24T09:08:38,065  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 49	49	49	
2024-04-24T09:08:38,065  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (5 out of 50 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,065  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 51	51	51	
2024-04-24T09:08:38,066  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 52	52	52	
2024-04-24T09:08:38,066  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 53	53	53	
2024-04-24T09:08:38,066  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 54	54	54	
2024-04-24T09:08:38,066  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 55	55	55	
2024-04-24T09:08:38,066  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 56	56	56	
2024-04-24T09:08:38,067  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 57	57	57	
2024-04-24T09:08:38,067  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 58	58	58	
2024-04-24T09:08:38,067  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 59	59	59	
2024-04-24T09:08:38,067  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (6 out of 60 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,068  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 61	61	61	
2024-04-24T09:08:38,068  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 62	62	62	
2024-04-24T09:08:38,068  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 63	63	63	
2024-04-24T09:08:38,068  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 64	64	64	
2024-04-24T09:08:38,068  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 65	65	65	
2024-04-24T09:08:38,069  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 66	66	66	
2024-04-24T09:08:38,069  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 67	67	67	
2024-04-24T09:08:38,069  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 68	68	68	
2024-04-24T09:08:38,069  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 69	69	69	
2024-04-24T09:08:38,069  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (7 out of 70 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,070  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 71	71	71	
2024-04-24T09:08:38,070  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 72	72	72	
2024-04-24T09:08:38,070  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 73	73	73	
2024-04-24T09:08:38,071  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 74	74	74	
2024-04-24T09:08:38,071  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 75	75	75	
2024-04-24T09:08:38,071  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 76	76	76	
2024-04-24T09:08:38,071  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 77	77	77	
2024-04-24T09:08:38,071  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 78	78	78	
2024-04-24T09:08:38,072  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 79	79	79	
2024-04-24T09:08:38,072  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (8 out of 80 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,072  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 81	81	81	
2024-04-24T09:08:38,072  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 82	82	82	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 83	83	83	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 84	84	84	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 85	85	85	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 86	86	86	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 87	87	87	
2024-04-24T09:08:38,073  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 88	88	88	
2024-04-24T09:08:38,074  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 89	89	89	
2024-04-24T09:08:38,074  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (9 out of 90 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,074  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 91	91	91	
2024-04-24T09:08:38,075  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 92	92	92	
2024-04-24T09:08:38,075  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 93	93	93	
2024-04-24T09:08:38,075  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 94	94	94	
2024-04-24T09:08:38,075  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 95	95	95	
2024-04-24T09:08:38,075  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 96	96	96	
2024-04-24T09:08:38,076  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 97	97	97	
2024-04-24T09:08:38,076  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 98	98	98	
2024-04-24T09:08:38,076  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 99	99	99	
2024-04-24T09:08:38,076  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (10 out of 100 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:38,079  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T09:08:38,085  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local345244930_0001_m_000000_0 is done. And is in the process of committing
2024-04-24T09:08:38,086  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T09:08:38,086  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local345244930_0001_m_000000_0 is allowed to commit now
2024-04-24T09:08:38,088  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local345244930_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/test_bad_record_handling_output
2024-04-24T09:08:38,088  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/warehouse/test_bad_records/intString.seq:0+4027
2024-04-24T09:08:38,088  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local345244930_0001_m_000000_0' done.
2024-04-24T09:08:38,090  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local345244930_0001_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=521389
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=750256128
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T09:08:38,090  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local345244930_0001_m_000000_0
2024-04-24T09:08:38,091  INFO [Thread-52] mapred.LocalJobRunner: map task executor complete.
2024-04-24T09:08:38,946  INFO [main] mapreduce.Job: Job job_local345244930_0001 running in uber mode : false
2024-04-24T09:08:38,948  INFO [main] mapreduce.Job:  map 100% reduce 0%
2024-04-24T09:08:38,953  INFO [main] mapreduce.Job: Job job_local345244930_0001 completed successfully
2024-04-24T09:08:38,965  INFO [main] mapreduce.Job: Counters: 15
	File System Counters
		FILE: Number of bytes read=18191
		FILE: Number of bytes written=521389
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=90
		Map output records=90
		Input split bytes=1847
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=750256128
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=889
2024-04-24T09:08:39,021  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:08:39,021  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:08:39,022  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:08:39,025  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:39,025  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:39,025  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@10bdfbcc will be shutdown
2024-04-24T09:08:39,026  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4489f60f created in the thread with id: 1
2024-04-24T09:08:39,029  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 87873cd0-1635-40f2-9c3c-75fc0c981e63
2024-04-24T09:08:39,029  INFO [main] SessionState: Hive Session ID = 87873cd0-1635-40f2-9c3c-75fc0c981e63
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:08:39,030  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T09:08:39,036  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/87873cd0-1635-40f2-9c3c-75fc0c981e63
2024-04-24T09:08:39,038  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/localscratchdir/87873cd0-1635-40f2-9c3c-75fc0c981e63
2024-04-24T09:08:39,041  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/core/target/tmp/scratchdir/alex/87873cd0-1635-40f2-9c3c-75fc0c981e63/_tmp_space.db
2024-04-24T09:08:39,041  INFO [main] mapreduce.HCatBaseTest: Creating data file: /home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data/intString.seq
2024-04-24T09:08:39,049  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): drop table if exists test_bad_records
2024-04-24T09:08:39,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,066  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,066  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:39,067  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:39,067  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:39,067  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=14}
2024-04-24T09:08:39,067  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.018 seconds
2024-04-24T09:08:39,067  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:39,067  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:39,067  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): drop table if exists test_bad_records
2024-04-24T09:08:39,068  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:08:39,068  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,082  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,096  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,096  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,463  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:39,463  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=14, dropTable_(String, String, boolean, boolean, boolean)=380, isCompatibleWith_(Configuration)=0}
2024-04-24T09:08:39,464  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.396 seconds
2024-04-24T09:08:39,464  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:08:39,466  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf
2024-04-24T09:08:39,466  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T09:08:39,466  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=87873cd0-1635-40f2-9c3c-75fc0c981e63, clientType=HIVECLI]
2024-04-24T09:08:39,467  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T09:08:39,467  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:08:39,467  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@615e83ac, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4489f60f will be shutdown
2024-04-24T09:08:39,468  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:39,468  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-24T09:08:39,469  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:39,470  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:08:39,470  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:39,471  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06, with PersistenceManager: null will be shutdown
2024-04-24T09:08:39,472  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2676d96a created in the thread with id: 1
2024-04-24T09:08:39,480  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06 from thread id: 1
2024-04-24T09:08:39,480  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:39,481  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:39,481  INFO [main] parse.CalcitePlanner: Creating table default.test_bad_records position=13
2024-04-24T09:08:39,482  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:39,482  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:39,483  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2676d96a will be shutdown
2024-04-24T09:08:39,483  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ca33187 created in the thread with id: 1
2024-04-24T09:08:39,487  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:39,487  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:39,487  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T09:08:39,492  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf
2024-04-24T09:08:39,492  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:39,492  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:39,493  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:39,493  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T09:08:39,493  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.029 seconds
2024-04-24T09:08:39,493  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:39,493  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:39,493  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): create table test_bad_records row format serde 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer' with serdeproperties (   'serialization.class'='org.apache.hadoop.hive.serde2.thrift.test.IntString',   'serialization.format'='org.apache.thrift.protocol.TBinaryProtocol') stored as  inputformat 'org.apache.hadoop.mapred.SequenceFileInputFormat'  outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
2024-04-24T09:08:39,493  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T09:08:39,493  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T09:08:39,494  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T09:08:39,494  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27ec0d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ca33187 will be shutdown
2024-04-24T09:08:39,494  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:39,494  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-24T09:08:39,496  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T09:08:39,497  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T09:08:39,497  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T09:08:39,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@16f4a3c0, with PersistenceManager: null will be shutdown
2024-04-24T09:08:39,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@16f4a3c0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@26495639 created in the thread with id: 1
2024-04-24T09:08:39,501  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@16f4a3c0 from thread id: 1
2024-04-24T09:08:39,501  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T09:08:39,502  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T09:08:39,502  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:test_bad_records, dbName:default, owner:alex, createTime:1713974919, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:underscore_int, type:int, comment:from deserializer), FieldSchema(name:myint, type:string, comment:from deserializer), FieldSchema(name:mystring, type:int, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numRows=0, bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"myint":"true","mystring":"true","underscore_int":"true"}}, numFiles=0, rawDataSize=0, totalSize=0, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T09:08:39,513  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/warehouse/test_bad_records
2024-04-24T09:08:39,561  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:39,561  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=59}
2024-04-24T09:08:39,562  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.068 seconds
2024-04-24T09:08:39,562  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data' into table test_bad_records
2024-04-24T09:08:39,564  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf
2024-04-24T09:08:39,564  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,618  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,621  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf
2024-04-24T09:08:39,621  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T09:08:39,621  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T09:08:39,622  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T09:08:39,622  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=54, isCompatibleWith_(Configuration)=0}
2024-04-24T09:08:39,622  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.06 seconds
2024-04-24T09:08:39,622  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T09:08:39,622  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T09:08:39,622  INFO [main] ql.Driver: Executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf): load data local inpath '/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data' into table test_bad_records
2024-04-24T09:08:39,623  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.test_bad_records
2024-04-24T09:08:39,623  INFO [main] exec.Task: Loading data to table default.test_bad_records from file:/home/alex/Repositories/hive/hcatalog/core/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713974908691/data
2024-04-24T09:08:39,623  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,637  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,655  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,663  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:08:39,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:08:39,698  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T09:08:39,699  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,712  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,712  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T09:08:39,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,725  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,727  WARN [main] metadata.Hive: Cannot get a table snapshot for test_bad_records
2024-04-24T09:08:39,727  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.test_bad_records newtbl=test_bad_records	
2024-04-24T09:08:39,770  INFO [main] stats.BasicStatsTask: Table default.test_bad_records stats: [numFiles=1, numRows=0, totalSize=4027, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T09:08:39,770  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T09:08:39,771  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {alter_table_(String, String, String, Table, EnvironmentContext, String)=77, getTable_(GetTableRequest)=58, isCompatibleWith_(Configuration)=1}
2024-04-24T09:08:39,771  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424090839_0268bccc-e85b-43e2-84f1-fc05197496cf); Time taken: 0.148 seconds
2024-04-24T09:08:39,812  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T09:08:39,813  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T09:08:39,814  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T09:08:39,814  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T09:08:39,817  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T09:08:39,822  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.test_bad_records	
2024-04-24T09:08:39,834  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T09:08:39,842  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T09:08:39,849  WARN [main] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T09:08:39,854  WARN [main] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T09:08:39,858  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.client.cache.enabled=true, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.warehouse.dir=${test.warehouse.dir}, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.min=32768, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, datanucleus.connectionPool.maxPoolSize=4, hive.metastore.schema.verification=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.stats.column.autogather=true, hive.in.test=true, hive.scheduled.queries.executor.enabled=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.llap.io.use.lrfu=true, hive.metastore.client.cache.maxSize=10Mb, hive.stats.key.prefix.reserve.length=0, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.query.reexecution.stats.persist.scope=query, hive.ignore.mapjoin.hint=false, test.log.dir=${test.tmp.dir}/log/, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.querylog.location=${test.tmp.dir}/tmp, test.data.files=${hive.root}/data/files, hive.users.in.admin.role=hive_admin_user, hive.support.concurrency=true, hive.auto.convert.join=false, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.allocator.direct=false, hive.llap.cache.allow.synthetic.fileid=true, test.data.scripts=${hive.root}/data/scripts, hive.strict.timestamp.conversion=false, hive.test.dummystats.aggregator=value2, hive.llap.io.cache.orc.size=8388608, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, hive.materializedview.rewriting=true, test.property1=value1, hive.mapjoin.max.gc.time.percentage=0.99, hive.exec.submit.local.task.via.child=false, hive.query.results.cache.enabled=false, hive.conf.restricted.list=dummy.config.value, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.cbo.fallback.strategy=TEST, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, hive.llap.io.cache.orc.arena.size=8388608, iceberg.hive.keep.stats=true, hive.stats.fetch.bitvector=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.client.cache.recordStats=true, hive.exec.mode.local.auto=false, javax.jdo.option.ConnectionPassword=mine, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat}
2024-04-24T09:08:39,863  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T09:08:39,883  INFO [main] mapreduce.JobSubmitter: number of splits:1
2024-04-24T09:08:39,898  INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_local1661237917_0002
2024-04-24T09:08:39,898  INFO [main] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T09:08:39,952  INFO [main] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T09:08:39,953  INFO [main] mapreduce.Job: Running job: job_local1661237917_0002
2024-04-24T09:08:39,953  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T09:08:39,954  INFO [Thread-97] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:08:39,954  INFO [Thread-97] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:08:39,954  INFO [Thread-97] mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-24T09:08:39,963  INFO [Thread-97] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T09:08:39,963  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1661237917_0002_m_000000_0
2024-04-24T09:08:39,967  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T09:08:39,967  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T09:08:39,967  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T09:08:39,969  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@642b6f6f
2024-04-24T09:08:39,993  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer with properties {name=default.test_bad_records, numFiles=1, columns.types=int,string,int, numFilesErasureCoded=0, serialization.format=org.apache.thrift.protocol.TBinaryProtocol, columns=underscore_int,myint,mystring, rawDataSize=0, columns.comments=from deserializer from deserializer from deserializer, numRows=0, serialization.class=org.apache.hadoop.hive.serde2.thrift.test.IntString, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, totalSize=4027, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713974919}
2024-04-24T09:08:39,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 1	1	1	
2024-04-24T09:08:39,994  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 2	2	2	
2024-04-24T09:08:39,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 3	3	3	
2024-04-24T09:08:39,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 4	4	4	
2024-04-24T09:08:39,995  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 5	5	5	
2024-04-24T09:08:39,996  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 6	6	6	
2024-04-24T09:08:39,996  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 7	7	7	
2024-04-24T09:08:39,996  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 8	8	8	
2024-04-24T09:08:39,996  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 9	9	9	
2024-04-24T09:08:39,997  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (1 out of 10 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:39,997  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 11	11	11	
2024-04-24T09:08:39,997  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 12	12	12	
2024-04-24T09:08:39,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 13	13	13	
2024-04-24T09:08:39,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 14	14	14	
2024-04-24T09:08:39,998  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 15	15	15	
2024-04-24T09:08:39,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 16	16	16	
2024-04-24T09:08:39,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 17	17	17	
2024-04-24T09:08:39,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 18	18	18	
2024-04-24T09:08:39,999  INFO [LocalJobRunner Map Task Executor #0] mapreduce.HCatBaseTest: HCatRecord: 19	19	19	
2024-04-24T09:08:39,999  WARN [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: Error while reading an input record (2 out of 20 so far ): 
org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) [classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) [hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	... 14 more
2024-04-24T09:08:40,000 ERROR [LocalJobRunner Map Task Executor #0] mapreduce.HCatRecordReader: 2 out of 20 crosses configured threshold (0.009999999776482582)
2024-04-24T09:08:40,000  INFO [Thread-97] mapred.LocalJobRunner: map task executor complete.
2024-04-24T09:08:40,001  WARN [Thread-97] mapred.LocalJobRunner: job_local1661237917_0002
java.lang.Exception: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552) [hadoop-mapreduce-client-common-3.1.0.jar:?]
Caused by: java.lang.RuntimeException: error rate while reading input records crossed threshold
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader$InputErrorTracker.incErrors(HCatRecordReader.java:282) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:196) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:79) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: org.apache.thrift.protocol.TProtocolException: Unrecognized type 98
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:144) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.thrift.protocol.TProtocolUtil.skip(TProtocolUtil.java:60) ~[libthrift-0.14.1.jar:0.14.1]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:481) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString$IntStringStandardScheme.read(IntString.java:444) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.test.IntString.read(IntString.java:384) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(ThriftByteStreamTypedSerDe.java:77) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(ThriftDeserializer.java:75) ~[classes/:?]
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:189) ~[classes/:?]
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:568) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.0.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_402]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
2024-04-24T09:08:40,954  INFO [main] mapreduce.Job: Job job_local1661237917_0002 running in uber mode : false
2024-04-24T09:08:40,955  INFO [main] mapreduce.Job:  map 0% reduce 0%
2024-04-24T09:08:40,955  INFO [main] mapreduce.Job: Job job_local1661237917_0002 failed with state FAILED due to: NA
2024-04-24T09:08:40,956  INFO [main] mapreduce.Job: Counters: 0
2024-04-24T09:08:40,972  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T09:08:40,972  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
