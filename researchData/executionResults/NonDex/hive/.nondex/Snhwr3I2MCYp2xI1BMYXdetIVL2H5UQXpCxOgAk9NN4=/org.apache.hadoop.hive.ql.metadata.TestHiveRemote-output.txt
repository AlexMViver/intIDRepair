SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,075709 seconds to load 250 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@609e8838]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@609e8838) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@2de56eb2
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@2657d4dd
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,021677 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 132 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/ql/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/ql/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(PatternSelector=null, header="null", disableAnsi="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", footer="null", alwaysWriteExceptions="null", noConsoleNoAnsi="null", Configuration(HiveLog4j2Test), charset="null", Replace=null)
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", direct="null", follow="null", bufferSize="null", immediateFlush="null", bufferedIo="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), Configuration(HiveLog4j2Test), name="console", Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(alwaysWriteExceptions="null", header="null", Replace=null, disableAnsi="null", footer="null", noConsoleNoAnsi="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, charset="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(modulate="true", interval="1", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(stopCustomActionsOnError="null", fileIndex="null", ={}, max="30", compressionLevel="null", min="null", Configuration(HiveLog4j2Test), tempCompressedFilePattern="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(advertise="null", filePermissions="null", advertiseURI="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), filePattern="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log.%d{yyyy-MM-dd}", fileOwner="null", fileName="/home/alex/Repositories/hive/ql/target/tmp/log/hive.log", append="null", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), fileGroup="null", bufferSize="null", immediateFlush="null", bufferedIo="null", ignoreExceptions="null", name="DRFA", Configuration(HiveLog4j2Test), PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ={}, Filter=null)
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/ql/target/tmp/log/hive.log seek to 2576881753
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/ql/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T06:09:43.538-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-06:09:45.256, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-06:09:45.257, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@2657d4dd initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@2657d4dd
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@2657d4dd OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@6a2b953e...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@6a2b953e OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@4097cac
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@609e8838
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/ql/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@609e8838) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@609e8838] started OK.
2024-04-24T06:09:45,359  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/ql/target/testconf/hive-site.xml
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
2024-04-24T06:09:45,753  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T06:09:45,815  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T06:09:45,816  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T06:09:45,816  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T06:09:45,816  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T06:09:45,816  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T06:09:45,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T06:09:45,817  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T06:09:45,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T06:09:45,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T06:09:45,817  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T06:09:45,818  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T06:09:45,842  INFO [main] utils.TestTxnDbUtil: Creating transactional tables
2024-04-24T06:09:46,462  INFO [main] utils.TestTxnDbUtil: Reinitializing the metastore db with hive-schema-4.0.0.derby.sql on the database jdbc:derby:memory:/home/alex/Repositories/hive/ql/target/tmp/junit_metastore_db_37927;create=true
2024-04-24T06:09:47,405  INFO [main] metastore.MetaStoreTestUtils: Waiting the HMS to start.
2024-04-24T06:09:47,466  INFO [MetaStoreThread-37927] metastore.AuthFactory: Using authentication NOSASL with kerberos authentication disabled
2024-04-24T06:09:47,567  INFO [MetaStoreThread-37927] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:47,601  INFO [MetaStoreThread-37927] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:47,605  INFO [MetaStoreThread-37927] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T06:09:47,605  INFO [MetaStoreThread-37927] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T06:09:47,628  WARN [MetaStoreThread-37927] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T06:09:47,633  INFO [MetaStoreThread-37927] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T06:09:47,650  INFO [MetaStoreThread-37927] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T06:09:47,653  INFO [MetaStoreThread-37927] hikari.HikariDataSource: HikariPool-1 - Start completed.
log4j: Trying to find [log4j.xml] using context classloader sun.misc.Launcher$AppClassLoader@330bedb4.
log4j: Trying to find [log4j.xml] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.xml] using ClassLoader.getSystemResource().
log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@330bedb4.
log4j: Trying to find [log4j.properties] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.properties] using ClassLoader.getSystemResource().
log4j: Could not find resource: [null].
log4j:WARN No appenders could be found for logger (DataNucleus.General).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2024-04-24T06:09:48,100  INFO [MetaStoreThread-37927] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T06:09:48,101  INFO [MetaStoreThread-37927] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@63d9980c, with PersistenceManager: null will be shutdown
2024-04-24T06:09:48,126  INFO [MetaStoreThread-37927] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@63d9980c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72b31c33 created in the thread with id: 18
2024-04-24T06:09:48,405  INFO [main] metastore.MetaStoreTestUtils: Waiting the HMS to start.
2024-04-24T06:09:49,406  INFO [main] metastore.MetaStoreTestUtils: Waiting the HMS to start.
2024-04-24T06:09:50,155  INFO [MetaStoreThread-37927] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@63d9980c from thread id: 18
2024-04-24T06:09:50,168  INFO [MetaStoreThread-37927] metastore.HMSHandler: Setting location of default catalog, as it hasn't been done after upgrade
2024-04-24T06:09:50,406  INFO [main] metastore.MetaStoreTestUtils: Waiting the HMS to start.
2024-04-24T06:09:50,456  INFO [MetaStoreThread-37927] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T06:09:50,503  INFO [MetaStoreThread-37927] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T06:09:50,520  INFO [MetaStoreThread-37927] metastore.HMSHandler: Added admin role in metastore
2024-04-24T06:09:50,522  INFO [MetaStoreThread-37927] metastore.HMSHandler: Added public role in metastore
2024-04-24T06:09:50,585  INFO [MetaStoreThread-37927] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T06:09:50,592  INFO [MetaStoreThread-37927] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T06:09:50,756  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Starting DB backed MetaStore Server with SetUGI enabled
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Started the new metaserver on port [37927]...
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Options.minWorkerThreads = 200
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Options.maxWorkerThreads = 1000
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: TCP keepalive = true
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Enable SSL = false
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Compaction HMS parameters:
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.initiator.on = false
2024-04-24T06:09:50,760  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.worker.threads = 0
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: hive.metastore.runworker.in = metastore
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.history.retention.attempted = 2
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.history.retention.failed = 3
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.history.retention.succeeded = 3
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.initiator.failed.compacts.threshold = 2
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: metastore.compactor.enable.stats.compression
2024-04-24T06:09:50,761  WARN [MetaStoreThread-37927] metastore.HiveMetaStore: Compactor Initiator is turned Off. Automatic compaction will not be triggered.
2024-04-24T06:09:50,761  WARN [MetaStoreThread-37927] metastore.HiveMetaStore: Invalid number of Compactor Worker threads(0) on HMS
2024-04-24T06:09:50,761  INFO [MetaStoreThread-37927] metastore.HiveMetaStore: Direct SQL optimization = true
2024-04-24T06:09:51,417  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:51,427  INFO [main] metastore.MetaStoreTestUtils: MetaStore warehouse root dir (pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927) is created
2024-04-24T06:09:51,427  INFO [main] metastore.MetaStoreTestUtils: MetaStore Thrift Server started on port: 37927 with warehouse dir: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927 with jdbcUrl: jdbc:derby:memory:/home/alex/Repositories/hive/ql/target/tmp/junit_metastore_db_37927;create=true
Hive Session ID = a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:51,443  INFO [main] SessionState: Hive Session ID = a45ea97c-e210-4b49-9e1d-f46322a15ee3
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:51,453  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:51,498  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:51,502  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:51,506  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:51,939  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,946  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,947  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,948  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,949  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,949  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:51,952  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T06:09:52,028  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:52,029  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:52,030  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:52,043  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:52,061  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:52,102  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_all_functions	
2024-04-24T06:09:52,104  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:52,104  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:52,106  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4d77cbe, with PersistenceManager: null will be shutdown
2024-04-24T06:09:52,106  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4d77cbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7086e422 created in the thread with id: 34
2024-04-24T06:09:52,114  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4d77cbe from thread id: 34
2024-04-24T06:09:52,142  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testtable	
2024-04-24T06:09:52,178  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:52,180  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T06:09:52,184  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:52,184  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:52,184  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4d77cbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7086e422 will be shutdown
2024-04-24T06:09:52,185  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:52,185  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:52,186  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:52,186  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:52,186  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:52,187  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:52,188  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:52,231  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:52,231  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:52,231  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:52,231  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 2
2024-04-24T06:09:52,234  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:52,243  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table_for_testtable, dbName:default, owner:pchakka, createTime:1713964192, lastAccessTime:0, retention:10, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:int, comment:int -- first column), FieldSchema(name:col2, type:string, comment:string -- second column), FieldSchema(name:col3, type:double, comment:double -- thrift column)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:512, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{collection.delim=2, serialization.format=1, mapkey.delim=3, field.delim=1, line.delim=
}), bucketCols:[col1], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:ds, type:string, comment:partition column, date but in string format as date type is not yet supported in QL)], parameters:{bucketing_version=2, comment=this is a test table created as part junit tests}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false, catName:hive, ownerType:USER)	
2024-04-24T06:09:52,244  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:52,245  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:52,246  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6267821b, with PersistenceManager: null will be shutdown
2024-04-24T06:09:52,246  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6267821b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@38c32885 created in the thread with id: 37
2024-04-24T06:09:52,252  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6267821b from thread id: 37
2024-04-24T06:09:52,264  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testtable
2024-04-24T06:09:52,354  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testtable	
2024-04-24T06:09:52,427  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:52,440  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_database: default	
2024-04-24T06:09:52,445  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testtable	
2024-04-24T06:09:52,460  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:52,467  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_table : tbl=hive.default.table_for_testtable	
2024-04-24T06:09:52,675  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testtable	
2024-04-24T06:09:52,680  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:52,680  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:52,682  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 1
2024-04-24T06:09:52,682  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:52,682  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6267821b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@38c32885 will be shutdown
2024-04-24T06:09:52,682  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:52,683  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:52,687  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:52,694  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:52,698  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:52,702  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:52,704  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,706  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:52,706  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:52,707  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@60488dff, with PersistenceManager: null will be shutdown
2024-04-24T06:09:52,707  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@60488dff, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@51e24a2f created in the thread with id: 36
2024-04-24T06:09:52,713  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@60488dff from thread id: 36
2024-04-24T06:09:52,717  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:52,720  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table_for_testpartition, dbName:default, owner:alex, createTime:1713964192, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER)	
2024-04-24T06:09:52,725  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition
2024-04-24T06:09:52,756  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,776  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:52,785  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,802  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition/ds=2008-04-08/hr=12
2024-04-24T06:09:52,834  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,843  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition/ds=2008-04-08/hr=13
2024-04-24T06:09:52,858  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,868  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition/ds=2008-04-08/hr=14
2024-04-24T06:09:52,883  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,892  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition/ds=2008-04-07/hr=12
2024-04-24T06:09:52,911  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,921  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testpartition/ds=2008-04-07/hr=13
2024-04-24T06:09:52,939  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:52,999  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_with_auth : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:53,057  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partition_with_auth : tbl=hive.default.table_for_testpartition[2008-04-07,12]	
2024-04-24T06:09:53,132  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_spec_by_expr : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:53,156  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PartFilterExprUtil: Unable to make the expression tree from expression string [(true and true)]Error parsing partition filter; lexer error: null; exception NoViableAltException(15@[])
2024-04-24T06:09:53,215  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T06:09:53,215  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T06:09:53,215  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T06:09:53,216  WARN [TThreadPoolServer WorkerProcess-%d] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T06:09:53,284  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_ps_with_auth : tbl=hive.default.table_for_testpartition[2008-04-07,]	
2024-04-24T06:09:53,324  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_ps_with_auth : tbl=hive.default.table_for_testpartition[2008-04-08,]	
2024-04-24T06:09:53,361  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_ps_with_auth : tbl=hive.default.table_for_testpartition[,13]	
2024-04-24T06:09:53,392  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_ps_with_auth : tbl=hive.default.table_for_testpartition[,]	
2024-04-24T06:09:53,395  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: Redirecting to directSQL enabled API: db: default tbl: table_for_testpartition partVals: ,
2024-04-24T06:09:53,404  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partitions_ps_with_auth : tbl=hive.default.table_for_testpartition[,14]	
2024-04-24T06:09:53,429  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:53,440  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:53,441  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_table : tbl=hive.default.table_for_testpartition	
2024-04-24T06:09:53,538  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:53,538  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:53,538  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:53,539  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:53,539  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@60488dff, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@51e24a2f will be shutdown
2024-04-24T06:09:53,539  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:53,539  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,540  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,547  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,550  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,553  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:53,554  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:53,554  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,555  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,562  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,565  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,568  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:53,570  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:53,570  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:53,570  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:53,570  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:53,571  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:53,573  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions	
2024-04-24T06:09:53,574  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:53,574  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:53,575  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52be586, with PersistenceManager: null will be shutdown
2024-04-24T06:09:53,576  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52be586, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3464a441 created in the thread with id: 61
2024-04-24T06:09:53,611  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52be586 from thread id: 61
2024-04-24T06:09:53,616  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:53,617  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table_for_testAutoPurgeTablesAndPartitions, dbName:default, owner:alex, createTime:1713964193, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER)	
2024-04-24T06:09:53,623  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testautopurgetablesandpartitions
2024-04-24T06:09:53,665  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions	
2024-04-24T06:09:53,675  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:53,676  WARN [main] metadata.Hive: Cannot get a table snapshot for table_for_testAutoPurgeTablesAndPartitions
2024-04-24T06:09:53,681  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 alter_table: hive.default.table_for_testAutoPurgeTablesAndPartitions newtbl=table_for_testautopurgetablesandpartitions	
2024-04-24T06:09:53,700  WARN [TThreadPoolServer WorkerProcess-%d] metastore.HiveAlterHandler: Alter table not cascaded to partitions.
2024-04-24T06:09:53,735  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testautopurgetablesandpartitions	
2024-04-24T06:09:53,746  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testautopurgetablesandpartitions/ds=20141216/hr=12
2024-04-24T06:09:53,768  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partition_with_auth : tbl=hive.default.table_for_testautopurgetablesandpartitions[20141216,12]	
2024-04-24T06:09:53,793  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_partition : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions[20141216,12]	
2024-04-24T06:09:53,793  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Partition values:[20141216, 12]
2024-04-24T06:09:53,866  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: dropPartition() will purge pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testautopurgetablesandpartitions/ds=20141216/hr=12 directly, skipping trash.
2024-04-24T06:09:53,868  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions	
2024-04-24T06:09:53,878  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:53,878  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_table : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions	
2024-04-24T06:09:53,975  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testAutoPurgeTablesAndPartitions	
2024-04-24T06:09:53,978  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:53,978  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:53,978  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:53,979  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:53,979  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52be586, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3464a441 will be shutdown
2024-04-24T06:09:53,979  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:53,979  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,979  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:53,986  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,989  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:53,992  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:53,993  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:53,994  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:53,994  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:53,994  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:53,995  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:53,995  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_database: db_for_testgettables	
2024-04-24T06:09:53,997  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:53,997  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:53,997  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3cc1692a, with PersistenceManager: null will be shutdown
2024-04-24T06:09:53,998  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3cc1692a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@d377394 created in the thread with id: 69
2024-04-24T06:09:54,001  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3cc1692a from thread id: 69
2024-04-24T06:09:54,006  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_database: Database(name:db_for_testgettables, description:null, locationUri:null, parameters:null, catalogName:hive)	
2024-04-24T06:09:54,012  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Creating database path in external directory pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/db_for_testgettables.db
2024-04-24T06:09:54,012  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/db_for_testgettables.db
2024-04-24T06:09:54,015  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created database path in external directory pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/db_for_testgettables.db
2024-04-24T06:09:54,019  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:54,033  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table1, dbName:db_for_testgettables, owner:alex, createTime:1713964194, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:unionfield1, type:array<int>, comment:from deserializer), FieldSchema(name:attributes, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:lintstring, type:map<string,map<string,map<string,uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>>>>, comment:from deserializer), FieldSchema(name:mstringstring, type:array<struct<underscore_int:int,myint:int,mystring:string>>, comment:from deserializer), FieldSchema(name:lstring, type:array<string>, comment:from deserializer), FieldSchema(name:astring, type:int, comment:from deserializer), FieldSchema(name:lint, type:map<string,string>, comment:from deserializer), FieldSchema(name:unionfield2, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:aint, type:string, comment:from deserializer), FieldSchema(name:unionfield3, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER)	
2024-04-24T06:09:54,038  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/db_for_testgettables.db/table1
2024-04-24T06:09:54,063  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table2, dbName:db_for_testgettables, owner:alex, createTime:1713964194, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:unionfield1, type:array<int>, comment:from deserializer), FieldSchema(name:attributes, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:lintstring, type:map<string,map<string,map<string,uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>>>>, comment:from deserializer), FieldSchema(name:mstringstring, type:array<struct<underscore_int:int,myint:int,mystring:string>>, comment:from deserializer), FieldSchema(name:lstring, type:array<string>, comment:from deserializer), FieldSchema(name:astring, type:int, comment:from deserializer), FieldSchema(name:lint, type:map<string,string>, comment:from deserializer), FieldSchema(name:unionfield2, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:aint, type:string, comment:from deserializer), FieldSchema(name:unionfield3, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER)	
2024-04-24T06:09:54,068  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/db_for_testgettables.db/table2
2024-04-24T06:09:54,095  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_multi_table : db=db_for_testgettables tbls=null	
java.lang.AssertionError: expected:<[table1, table2]> but was:<[]>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:120)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.hadoop.hive.ql.metadata.TestHive.testGetAndDropTables(TestHive.java:427)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

testGetAndDropTables() failed
2024-04-24T06:09:54,107  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,107  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,108  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:54,108  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:54,108  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3cc1692a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@d377394 will be shutdown
2024-04-24T06:09:54,108  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,109  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,116  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,122  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,125  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,128  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:54,129  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:54,129  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:54,129  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:54,129  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:54,130  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:54,131  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_test_thrifttable	
2024-04-24T06:09:54,132  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 7: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:54,132  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:54,132  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a4a9af9, with PersistenceManager: null will be shutdown
2024-04-24T06:09:54,133  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a4a9af9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b6ad52d created in the thread with id: 77
2024-04-24T06:09:54,136  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a4a9af9 from thread id: 77
2024-04-24T06:09:54,138  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:54,140  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table_for_test_thrifttable, dbName:default, owner:alex, createTime:1713964194, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:unionfield1, type:array<int>, comment:from deserializer), FieldSchema(name:attributes, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:lintstring, type:map<string,map<string,map<string,uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>>>>, comment:from deserializer), FieldSchema(name:mstringstring, type:array<struct<underscore_int:int,myint:int,mystring:string>>, comment:from deserializer), FieldSchema(name:lstring, type:array<string>, comment:from deserializer), FieldSchema(name:astring, type:int, comment:from deserializer), FieldSchema(name:lint, type:map<string,string>, comment:from deserializer), FieldSchema(name:unionfield2, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer), FieldSchema(name:aint, type:string, comment:from deserializer), FieldSchema(name:unionfield3, type:uniontype<int,bigint,string,double,boolean,array<string>,map<string,string>>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex, serialization.format=org.apache.thrift.protocol.TBinaryProtocol}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false, catName:hive, ownerType:USER)	
2024-04-24T06:09:54,144  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_test_thrifttable
2024-04-24T06:09:54,160  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_test_thrifttable	
2024-04-24T06:09:54,170  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:54,173  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_database: default	
2024-04-24T06:09:54,176  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_test_thrifttable	
2024-04-24T06:09:54,187  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:54,188  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_table : tbl=hive.default.table_for_test_thrifttable	
2024-04-24T06:09:54,218  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,218  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,218  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:54,218  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:54,218  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a4a9af9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b6ad52d will be shutdown
2024-04-24T06:09:54,219  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,219  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,219  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,226  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,228  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,231  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
DEBUG StatusLogger Starting StringOutputStreamManager StringStream
2024-04-24T06:09:54,234  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:54,234  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:54,235  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:54,235  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:54,236  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:54,240  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_databases: @hive#	
2024-04-24T06:09:54,241  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 8: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:54,241  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:54,242  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67e41cc9, with PersistenceManager: null will be shutdown
2024-04-24T06:09:54,242  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67e41cc9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56c9f83c created in the thread with id: 84
2024-04-24T06:09:54,246  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67e41cc9 from thread id: 84
2024-04-24T06:09:54,251  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:54,252  INFO [main] metadata.Hive: Dumping metastore api call timing information for : test phase
2024-04-24T06:09:54,252  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getAllDatabases_()=16}
2024-04-24T06:09:54,252  INFO [main] metadata.Hive: Dumping metastore api call timing information for : test phase
2024-04-24T06:09:54,252  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T06:09:54,255  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,255  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,255  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:54,255  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:54,255  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67e41cc9, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@56c9f83c will be shutdown
2024-04-24T06:09:54,255  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,256  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,256  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,262  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,265  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,267  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:54,268  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:54,268  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:54,268  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:54,269  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:54,270  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:54,270  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_database: default	
2024-04-24T06:09:54,271  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 9: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:54,271  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:54,271  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7abf2ab8, with PersistenceManager: null will be shutdown
2024-04-24T06:09:54,272  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7abf2ab8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b0f102b created in the thread with id: 89
2024-04-24T06:09:54,275  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7abf2ab8 from thread id: 89
2024-04-24T06:09:54,277  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:54,278  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:54,278  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7abf2ab8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b0f102b will be shutdown
2024-04-24T06:09:54,278  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,278  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,279  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:54,279  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:54,279  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:54,279  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 1
2024-04-24T06:09:54,280  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:54,280  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_database: default	
2024-04-24T06:09:54,281  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 10: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:54,281  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:54,282  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@20233b0e, with PersistenceManager: null will be shutdown
2024-04-24T06:09:54,282  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@20233b0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@11c9df03 created in the thread with id: 90
2024-04-24T06:09:54,285  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@20233b0e from thread id: 90
2024-04-24T06:09:54,287  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.connect.retries changed from 3 to 4
2024-04-24T06:09:54,287  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
2024-04-24T06:09:54,287  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:54,287  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@20233b0e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@11c9df03 will be shutdown
2024-04-24T06:09:54,288  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,288  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:54,288  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:54,289  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,290  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,296  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,298  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:54,301  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T06:09:54,340  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T06:09:54,341  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T06:09:54,341  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T06:09:54,341  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T06:09:54,341  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
Hive Session ID = efd512fd-f2e7-4696-b5a1-05e9da25c5ad
2024-04-24T06:09:54,342  INFO [main] SessionState: Hive Session ID = efd512fd-f2e7-4696-b5a1-05e9da25c5ad
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,342  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,348  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/efd512fd-f2e7-4696-b5a1-05e9da25c5ad
2024-04-24T06:09:54,350  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/efd512fd-f2e7-4696-b5a1-05e9da25c5ad
2024-04-24T06:09:54,353  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/efd512fd-f2e7-4696-b5a1-05e9da25c5ad/_tmp_space.db
2024-04-24T06:09:54,388  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T06:09:54,388  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T06:09:54,388  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T06:09:54,389  WARN [Thread-82] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
Hive Session ID = de5cfc88-9191-4ef8-be6a-e99b6e7bb54c
2024-04-24T06:09:54,390  INFO [Thread-82] SessionState: Hive Session ID = de5cfc88-9191-4ef8-be6a-e99b6e7bb54c
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,390  INFO [Thread-82] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:54,396  INFO [Thread-82] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/de5cfc88-9191-4ef8-be6a-e99b6e7bb54c
2024-04-24T06:09:54,399  INFO [Thread-82] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/de5cfc88-9191-4ef8-be6a-e99b6e7bb54c
2024-04-24T06:09:54,402  INFO [Thread-82] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/de5cfc88-9191-4ef8-be6a-e99b6e7bb54c/_tmp_space.db
2024-04-24T06:09:54,403  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:54,405  INFO [main] metastore.HMSHandler: 11: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:54,405  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:54,406  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T06:09:54,413  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T06:09:54,416  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T06:09:54,467  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T06:09:54,468  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T06:09:54,987  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T06:09:54,987  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d, with PersistenceManager: null will be shutdown
2024-04-24T06:09:54,987  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@706ddbc8 created in the thread with id: 1
2024-04-24T06:09:56,004  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d from thread id: 1
2024-04-24T06:09:56,053  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T06:09:56,068  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T06:09:56,094  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T06:09:56,096  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T06:09:56,165  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T06:09:56,165  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T06:09:56,166  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:56,217  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:56,217  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:56,218  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@706ddbc8 will be shutdown
2024-04-24T06:09:56,219  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c5d3a37 created in the thread with id: 1
2024-04-24T06:09:56,228  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T06:09:56,229  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:56,266 ERROR [main] metastore.RetryingHMSHandler: InvalidOperationException(message:Resource plan must be disabled to edit it.)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMWMResourcePlan(ObjectStore.java:13224)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMWMResourcePlan(ObjectStore.java:13200)
	at org.apache.hadoop.hive.metastore.ObjectStore.createPool(ObjectStore.java:13806)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy37.createPool(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_wm_pool(HMSHandler.java:9916)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy39.create_wm_pool(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createWMPool(HiveMetaStoreClient.java:4706)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy48.createWMPool(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createWMPool(Hive.java:6358)
	at org.apache.hadoop.hive.ql.metadata.TestHive.testWmNamespaceHandling(TestHive.java:494)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:123)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T06:09:56,297  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/efd512fd-f2e7-4696-b5a1-05e9da25c5ad on fs with scheme file
2024-04-24T06:09:56,297  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/efd512fd-f2e7-4696-b5a1-05e9da25c5ad on fs with scheme file
2024-04-24T06:09:56,301  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:56,301  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@628b819d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c5d3a37 will be shutdown
2024-04-24T06:09:56,301  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:56,301  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:56,302  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:56,307  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T06:09:56,308  INFO [main] metastore.HiveMetaStoreClient: Resolved metastore uris: [thrift://localhost:37927]
2024-04-24T06:09:56,308  INFO [main] metastore.HiveMetaStoreClient: Trying to connect to metastore with URI (thrift://localhost:37927)
2024-04-24T06:09:56,308  INFO [main] metastore.HiveMetaStoreClient: Opened a connection to metastore, URI (thrift://localhost:37927) current connections: 0
2024-04-24T06:09:56,309  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T06:09:56,310  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testDropPartitionsWithPurge	
2024-04-24T06:09:56,311  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: 12: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T06:09:56,311  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T06:09:56,311  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T06:09:56,314  WARN [TThreadPoolServer WorkerProcess-%d] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T06:09:56,315  INFO [TThreadPoolServer WorkerProcess-%d] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T06:09:56,317  INFO [TThreadPoolServer WorkerProcess-%d] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T06:09:56,317  INFO [TThreadPoolServer WorkerProcess-%d] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T06:09:56,691  INFO [TThreadPoolServer WorkerProcess-%d] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T06:09:56,691  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6838c40e, with PersistenceManager: null will be shutdown
2024-04-24T06:09:56,691  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6838c40e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46cb1e45 created in the thread with id: 112
2024-04-24T06:09:57,161  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6838c40e from thread id: 112
2024-04-24T06:09:57,170  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a45ea97c-e210-4b49-9e1d-f46322a15ee3, clientType=HIVECLI]
2024-04-24T06:09:57,171  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 create_table_req: Table(tableName:table_for_testDropPartitionsWithPurge, dbName:default, owner:alex, createTime:1713964197, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), catName:hive, ownerType:USER)	
2024-04-24T06:09:57,176  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testdroppartitionswithpurge
2024-04-24T06:09:57,214  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testDropPartitionsWithPurge	
2024-04-24T06:09:57,228  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:57,229  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testdroppartitionswithpurge	
2024-04-24T06:09:57,242  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testdroppartitionswithpurge/ds=20141216/hr=12
2024-04-24T06:09:57,264  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partition_with_auth : tbl=hive.default.table_for_testdroppartitionswithpurge[20141216,12]	
2024-04-24T06:09:57,282  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_partition : tbl=hive.default.table_for_testDropPartitionsWithPurge[20141216,12]	
2024-04-24T06:09:57,282  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Partition values:[20141216, 12]
2024-04-24T06:09:57,350  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: dropPartition() will purge pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testdroppartitionswithpurge/ds=20141216/hr=12 directly, skipping trash.
2024-04-24T06:09:57,352  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 add_partition : tbl=hive.default.table_for_testdroppartitionswithpurge	
2024-04-24T06:09:57,359  INFO [TThreadPoolServer WorkerProcess-%d] utils.FileUtils: Creating directory if it doesn't exist: pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testdroppartitionswithpurge/ds=20141216/hr=12
2024-04-24T06:09:57,373  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_partition_with_auth : tbl=hive.default.table_for_testdroppartitionswithpurge[20141216,12]	
2024-04-24T06:09:57,383  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_partition : tbl=hive.default.table_for_testDropPartitionsWithPurge[20141216,12]	
2024-04-24T06:09:57,383  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Partition values:[20141216, 12]
2024-04-24T06:09:57,410  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: dropPartition() will move pfile:/home/alex/Repositories/hive/ql/target/warehouse/37927/table_for_testdroppartitionswithpurge/ds=20141216/hr=12 to trash-directory.
2024-04-24T06:09:57,418  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 get_table : tbl=hive.default.table_for_testDropPartitionsWithPurge	
2024-04-24T06:09:57,427  INFO [TThreadPoolServer WorkerProcess-%d] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T06:09:57,428  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=source:127.0.0.1 drop_table : tbl=hive.default.table_for_testDropPartitionsWithPurge	
2024-04-24T06:09:57,554  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:57,554  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:57,554  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T06:09:57,554  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Cleaning up thread local RawStore...	
2024-04-24T06:09:57,554  INFO [TThreadPoolServer WorkerProcess-%d] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6838c40e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46cb1e45 will be shutdown
2024-04-24T06:09:57,555  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
2024-04-24T06:09:57,555  INFO [TThreadPoolServer WorkerProcess-%d] HiveMetaStore.audit: ugi=alex	ip=127.0.0.1	cmd=Done cleaning up thread local RawStore	
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:57,556  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/alex/Repositories/hive/conf/ivysettings.xml will be used
2024-04-24T06:09:57,562  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:57,565  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3
2024-04-24T06:09:57,567  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3/_tmp_space.db
2024-04-24T06:09:57,568  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/scratchdir/alex/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
2024-04-24T06:09:57,569  INFO [main] cleanup.SyncCleanupService: Deleted directory: /home/alex/Repositories/hive/ql/target/tmp/localscratchdir/a45ea97c-e210-4b49-9e1d-f46322a15ee3 on fs with scheme file
