<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="47.597" tests="29" errors="4" skipped="5" failures="2">
  <properties>
    <property name="build.test.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="test.tmp.dir.uri" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="qfile" value=""/>
    <property name="test.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/warehouse"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="run_disabled" value=""/>
    <property name="nondexPrintstack" value="false"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="user.country.format" value="PT"/>
    <property name="test.tmp.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter6569287701978481175.jar /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire 2024-04-23T21-59-46_173-jvmRun1 surefire3161027742425798137tmp surefire_37861013086316746832454tmp"/>
    <property name="nondexExecid" value="0ggB4wJvgjbFfVd2uKl80UyrBJDENmh26OwmBxHk0="/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/java-8-openjdk-amd64/jre"/>
    <property name="basedir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="file.separator" value="/"/>
    <property name="nondexEnd" value="9223372036854775807"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="derby.stream.error.file" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/derby.log"/>
    <property name="sun.boot.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex/nondex-instr.jar:/home/alex/.m2/repository/edu/illinois/nondex-common/2.1.7/nondex-common-2.1.7.jar:/home/alex/.m2/repository/edu/illinois/nondex-annotations/2.1.7/nondex-annotations-2.1.7.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes"/>
    <property name="log4j.debug" value="true"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="spark.home" value=""/>
    <property name="java.runtime.version" value="1.8.0_402-8u402-ga-2ubuntu1~22.04-b06"/>
    <property name="user.name" value="alex"/>
    <property name="test.src.tables" value=""/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="qfile_regex" value=""/>
    <property name="hadoop.bin.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../testutils/hadoop"/>
    <property name="localRepository" value="/home/alex/.m2/repository"/>
    <property name="nondexDir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex"/>
    <property name="build.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target"/>
    <property name="mapred.job.tracker" value="local"/>
    <property name="nondexJarDir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/.nondex"/>
    <property name="java.io.tmpdir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="com.zaxxer.hikari.pool_number" value="3"/>
    <property name="java.version" value="1.8.0_402"/>
    <property name="hive.version" value="4.0.0-SNAPSHOT"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib"/>
    <property name="test.data.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="java.vendor" value="Private Build"/>
    <property name="java.specification.maintenance.version" value="5"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="initScript" value=""/>
    <property name="sun.desktop" value="gnome"/>
    <property name="calcite.enable.rexnode.digest.normalize" value="false"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="nondexStart" value="0"/>
    <property name="log4j.configurationFile" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties"/>
    <property name="java.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="java.vm.vendor" value="Private Build"/>
    <property name="hadoop.version" value="3.1.0"/>
    <property name="user.timezone" value="US/Pacific"/>
    <property name="test.local.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/localfs/warehouse"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="os.name" value="Linux"/>
    <property name="test.build.data" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="clustermode" value=""/>
    <property name="user.country" value="US"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="test.data.files" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="nondexLogging" value="CONFIG"/>
    <property name="surefire.test.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/alex"/>
    <property name="user.language" value="en"/>
    <property name="java.security.krb5.conf" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/krb5.conf"/>
    <property name="test.dfs.mkdir" value=""/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="nondexFilter" value=".*"/>
    <property name="nondexSeed" value="933178"/>
    <property name="surefire.real.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter6569287701978481175.jar"/>
    <property name="hadoop.log.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="6.5.0-28-generic"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="hive.test.console.log.level" value="INFO"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="maven.local.repository" value="/home/alex/.m2/repository"/>
    <property name="user.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="os.arch" value="amd64"/>
    <property name="nondexMode" value="FULL"/>
    <property name="user.language.format" value="pt"/>
    <property name="antlr.version" value="3.5.2"/>
    <property name="derby.version" value="10.14.1.0"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.402-b06"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="test.output.overwrite" value=""/>
    <property name="java.class.version" value="52.0"/>
    <property name="hive.root" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../"/>
  </properties>
  <testcase name="testWriteTinyint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
    <system-err><![CDATA[SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,124819 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@330bedb4
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@185a6e9]...
DEBUG StatusLogger Reconfiguration started for context[name=330bedb4] at URI null (org.apache.logging.log4j.core.LoggerContext@185a6e9) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@77f1baf5
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,020742 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(header="null", alwaysWriteExceptions="null", Replace=null, noConsoleNoAnsi="null", PatternSelector=null, disableAnsi="null", Configuration(HiveLog4j2Test), pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", footer="null", charset="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", immediateFlush="null", bufferSize="null", Configuration(HiveLog4j2Test), ignoreExceptions="null", name="console", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), ={}, Filter=null)
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(footer="null", charset="null", PatternSelector=null, disableAnsi="null", header="null", noConsoleNoAnsi="null", alwaysWriteExceptions="null", pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", Replace=null, Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", maxRandomDelay="null", modulate="true")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(Configuration(HiveLog4j2Test), tempCompressedFilePattern="null", ={}, stopCustomActionsOnError="null", fileIndex="null", compressionLevel="null", max="30", min="null")
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(advertise="null", filePattern="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log.%d{yyyy-MM-dd}", fileOwner="null", fileName="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log", DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), filePermissions="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), fileGroup="null", append="null", advertiseURI="null", bufferedIo="null", immediateFlush="null", bufferSize="null", ignoreExceptions="null", name="DRFA", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log seek to 279784522
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-24T10:13:58.149-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/24-10:13:59.757, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/04/25-00:00:00.000, nextFileTime=2024/04/24-00:00:00.000, prevFileTime=2024/04/24-00:00:00.000, current=2024/04/24-10:13:59.758, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@37271612 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@780cb77...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@780cb77 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@35645047
TRACE StatusLogger Reregistering context (1/1): '330bedb4' org.apache.logging.log4j.core.LoggerContext@185a6e9
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=330bedb4,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=330bedb4] at URI /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@185a6e9) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=330bedb4, org.apache.logging.log4j.core.LoggerContext@185a6e9] started OK.
2024-04-24T10:13:59,935  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
]]></system-err>
  </testcase>
  <testcase name="testWriteSmallint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDateCharTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteTimestamp" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteDecimalXY" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDynamicPartitioningMultiPartColsInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="13.7"/>
  <testcase name="testEmptyStore" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.284"/>
  <testcase name="testStoreFuncAllSimpleTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.632"/>
  <testcase name="testDynamicPartitioningMultiPartColsInDataPartialSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.671"/>
  <testcase name="testWriteDate2" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.323"/>
  <testcase name="testWriteDate3" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="2.55"/>
  <testcase name="testStoreInPartiitonedTbl" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.312">
    <failure message="expected:&lt;1&gt; but was:&lt;2&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testStoreInPartiitonedTbl(AbstractHCatStorerTest.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,167  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,169  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,169  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,169  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@11a0aee8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6c76a792 will be shutdown
2024-04-24T10:14:26,170  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@11a0aee8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@57d354fc created in the thread with id: 1
2024-04-24T10:14:26,172  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 3f99bf3d-948e-4270-a46f-e3b4a5e48a6b
2024-04-24T10:14:26,172  INFO [main] SessionState: Hive Session ID = 3f99bf3d-948e-4270-a46f-e3b4a5e48a6b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:26,172  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:26,177  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3f99bf3d-948e-4270-a46f-e3b4a5e48a6b
2024-04-24T10:14:26,180  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/3f99bf3d-948e-4270-a46f-e3b4a5e48a6b
2024-04-24T10:14:26,182  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3f99bf3d-948e-4270-a46f-e3b4a5e48a6b/_tmp_space.db
2024-04-24T10:14:26,182  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): drop table if exists junit_unparted
2024-04-24T10:14:26,183  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:26,185  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:26,185  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:26,185  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:26,185  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:26,185  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.003 seconds
2024-04-24T10:14:26,185  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:26,186  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:26,186  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): drop table if exists junit_unparted
2024-04-24T10:14:26,186  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:26,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:26,188  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:26,188  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:26,188  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.002 seconds
2024-04-24T10:14:26,189  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): use default
2024-04-24T10:14:26,189  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:26,190  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:26,190  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:26,191  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:26,191  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:14:26,191  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.001 seconds
2024-04-24T10:14:26,191  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:26,191  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:26,191  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): use default
2024-04-24T10:14:26,191  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:26,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:26,193  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:26,194  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:26,194  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:26,194  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.003 seconds
2024-04-24T10:14:26,195  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:26,196  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7
2024-04-24T10:14:26,196  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:26,196  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3f99bf3d-948e-4270-a46f-e3b4a5e48a6b, clientType=HIVECLI]
2024-04-24T10:14:26,196  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:26,196  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:26,196  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@11a0aee8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@57d354fc will be shutdown
2024-04-24T10:14:26,196  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:26,196  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-24T10:14:26,197  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,198  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:26,198  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,198  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1, with PersistenceManager: null will be shutdown
2024-04-24T10:14:26,199  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@49f35dcc created in the thread with id: 1
2024-04-24T10:14:26,200  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1 from thread id: 1
2024-04-24T10:14:26,200  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:26,200  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:26,201  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted position=13
2024-04-24T10:14:26,202  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,202  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,202  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@49f35dcc will be shutdown
2024-04-24T10:14:26,202  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2737fd1d created in the thread with id: 1
2024-04-24T10:14:26,204  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:26,205  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:26,205  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:26,206  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:26,206  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:26,206  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.011 seconds
2024-04-24T10:14:26,206  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:26,206  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:26,207  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:26,207  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:26,207  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3823d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2737fd1d will be shutdown
2024-04-24T10:14:26,207  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:26,207  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-24T10:14:26,208  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,209  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:26,209  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,209  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e, with PersistenceManager: null will be shutdown
2024-04-24T10:14:26,210  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39a2973d created in the thread with id: 1
2024-04-24T10:14:26,212  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e from thread id: 1
2024-04-24T10:14:26,212  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:26,212  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:26,212  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted, dbName:default, owner:alex, createTime:1713978866, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:b, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:14:26,216  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/junit_unparted
2024-04-24T10:14:26,235  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:26,235  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=23}
2024-04-24T10:14:26,235  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101426_10194238-e60f-44fd-b232-2ad3ddc4e5f7); Time taken: 0.029 seconds
2024-04-24T10:14:26,257  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:14:26,258  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-717cd75a-4225-480d-b53e-8ad02cf2f0f5
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,289  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,289  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:26,290  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:14:26,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,293  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:26,303  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:26,323  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,324  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,324  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:26,325  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,328  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-24T10:14:26,335  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:14:26,335  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:14:26,336  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:14:26,337  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-24T10:14:26,337  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:14:26,342  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,344  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:14:26,344  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:14:26,345  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:14:26,345  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:14:26,345  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:14:26,345  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713978866345-0
2024-04-24T10:14:26,350  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:14:26,352  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,370  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,370  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:26,371  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,372  INFO [JobControl] metastore.HMSHandler: 26: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:26,372  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,372  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33b2f8cb, with PersistenceManager: null will be shutdown
2024-04-24T10:14:26,373  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33b2f8cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39f6fd66 created in the thread with id: 1061
2024-04-24T10:14:26,374  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33b2f8cb from thread id: 1061
2024-04-24T10:14:26,374  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:26,374  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:26,375  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,387  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:14:26,391  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:14:26,392  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:14:26,392  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:26,392  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:14:26,393  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:14:26,408  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:14:26,420  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1554887300_0018
2024-04-24T10:14:26,420  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:14:26,472  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:14:26,472  INFO [Thread-995] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:14:26,476  INFO [Thread-995] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,476  INFO [Thread-995] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,477  INFO [Thread-995] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:14:26,479  INFO [Thread-995] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,479  INFO [Thread-995] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,488  INFO [Thread-995] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:14:26,488  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1554887300_0018_m_000000_0
2024-04-24T10:14:26,493  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,493  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,496  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,496  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,496  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:14:26,497  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 23
Input split[0]:
   Length = 23
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:14:26,498  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:14:26,498  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/input.data:0+23
2024-04-24T10:14:26,500  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,500  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,503  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:14:26,503  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-24T10:14:26,504  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1] C:  R: 
2024-04-24T10:14:26,514  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:26,514  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1554887300_0018_m_000000_0 is done. And is in the process of committing
2024-04-24T10:14:26,516  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,516  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,517  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:26,517  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1554887300_0018_m_000000_0 is allowed to commit now
2024-04-24T10:14:26,518  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:26,518  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:26,519  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1554887300_0018_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/junit_unparted/_SCRATCH0,2811000047343942/b=1
2024-04-24T10:14:26,520  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:14:26,520  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1554887300_0018_m_000000_0' done.
2024-04-24T10:14:26,520  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1554887300_0018_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=61554
		FILE: Number of bytes written=10284975
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1032323072
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:14:26,520  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1554887300_0018_m_000000_0
2024-04-24T10:14:26,520  INFO [Thread-995] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:14:26,564  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,565  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,565  INFO [Thread-995] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:26,566  INFO [Thread-995] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:26,567  INFO [Thread-995] metastore.HMSHandler: 27: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:26,567  INFO [Thread-995] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:26,567  INFO [Thread-995] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e6c920, with PersistenceManager: null will be shutdown
2024-04-24T10:14:26,567  INFO [Thread-995] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e6c920, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c5cfc52 created in the thread with id: 1081
2024-04-24T10:14:26,569  INFO [Thread-995] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e6c920 from thread id: 1081
2024-04-24T10:14:26,569  INFO [Thread-995] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:26,569  INFO [Thread-995] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:26,569  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,605  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,606  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_unparted has new partitions [{b=1}].
2024-04-24T10:14:26,608  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_unparted[1]	
2024-04-24T10:14:26,615  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:14:26,616  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:14:26,624  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_unparted
2024-04-24T10:14:26,624  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 265
2024-04-24T10:14:26,663  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,696  WARN [Thread-995] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,697  INFO [Thread-995] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:26,697  INFO [Thread-995] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,699  INFO [Thread-995] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:14:26,851  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1554887300_0018
2024-04-24T10:14:26,851  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A
2024-04-24T10:14:26,851  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1] C:  R: 
2024-04-24T10:14:26,856  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,862  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,867  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,896  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:14:26,896  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:14:26	2024-04-24 10:14:26	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1554887300_0018	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A	MAP_ONLY	default.junit_unparted,

Input(s):
Successfully read 11 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/input.data"

Output(s):
Successfully stored 11 records in: "default.junit_unparted"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1554887300_0018


2024-04-24T10:14:26,899  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,901  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,903  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,928  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,929  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,929  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,931  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:26,941  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:26,956  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,956  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,956  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,957  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,957  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:26,960  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:26,969  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:26,971  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-24T10:14:26,978  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:14:26,978  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:14:26,979  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:14:26,980  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-24T10:14:26,980  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:14:26,985  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:26,987  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:14:26,987  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:26,999  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:27,000  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:27,000  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:27,000  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:14:27,001  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:27,004  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:27,013  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:27,014  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_by_filter : tbl=hive.default.junit_unparted	
2024-04-24T10:14:27,054  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:14:27,058  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:14:27,060  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,067  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:14:27,071  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:14:27,074  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, test.data.files=${hive.root}/data/files, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-04-24T10:14:27,078  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:27,078  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:14:27,094  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:14:27,107  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1991853289_0019
2024-04-24T10:14:27,107  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:14:27,152  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:14:27,152  INFO [Thread-1037] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:14:27,156  INFO [Thread-1037] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:27,156  INFO [Thread-1037] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:27,156  INFO [Thread-1037] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:14:27,164  INFO [Thread-1037] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:14:27,164  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1991853289_0019_m_000000_0
2024-04-24T10:14:27,168  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:27,168  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:27,170  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:14:27,170  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 265
Input split[0]:
   Length = 265
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-04-24T10:14:27,173  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@714b8648
2024-04-24T10:14:27,174  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:27,174  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:27,185  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.avro.AvroSerDe with properties {name=default.junit_unparted, columns.types=int, serialization.format=1, columns=a, columns.comments=, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.avro.AvroSerDe, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713978866}
2024-04-24T10:14:27,186  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:14:27,186  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:14:27,186  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: B[3,4] C:  R: 
2024-04-24T10:14:27,186  WARN [LocalJobRunner Map Task Executor #0] avro.AvroDeserializer: Received different schemas.  Have to re-encode: {"type":"record","name":"junit_unparted","fields":[{"name":"a","type":["null","int"],"default":null}]}
SIZE{1eaa57e:18f1119c710:-7fed=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@6b34d03f} ID 1eaa57e:18f1119c710:-7fed
2024-04-24T10:14:27,187  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:27,187  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1991853289_0019_m_000000_0 is done. And is in the process of committing
2024-04-24T10:14:27,189  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:27,189  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1991853289_0019_m_000000_0 is allowed to commit now
2024-04-24T10:14:27,191  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1991853289_0019_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713978855072/pig/temp/temp-1648704498/tmp872715584
2024-04-24T10:14:27,192  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:14:27,192  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1991853289_0019_m_000000_0' done.
2024-04-24T10:14:27,192  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1991853289_0019_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=65145
		FILE: Number of bytes written=10849135
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=2102
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1032323072
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:14:27,192  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1991853289_0019_m_000000_0
2024-04-24T10:14:27,192  INFO [Thread-1037] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:14:27,352  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1991853289_0019
2024-04-24T10:14:27,352  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases B
2024-04-24T10:14:27,352  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: B[3,4] C:  R: 
2024-04-24T10:14:27,357  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,365  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,371  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,377  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:14:27,377  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:14:26	2024-04-24 10:14:27	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1991853289_0019	1	0	n/a	n/a	n/a	n/a	0	0	0	0	B	MAP_ONLY	file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713978855072/pig/temp/temp-1648704498/tmp872715584,

Input(s):
Successfully read 11 records from: "default.junit_unparted"

Output(s):
Successfully stored 11 records in: "file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1713978855072/pig/temp/temp-1648704498/tmp872715584"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1991853289_0019


2024-04-24T10:14:27,382  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,387  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:27,389  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:14:27,390  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:14:27,399  INFO [main] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:27,399  INFO [main] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:14:27,400  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-24T10:14:27,410  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testWriteChar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.164">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:27,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:27,449  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:27,449  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:27,449  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:27,449  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:27,450  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:27,451  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:27,451  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@39a2973d will be shutdown
2024-04-24T10:14:27,451  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f0eabb8 created in the thread with id: 1
2024-04-24T10:14:27,454  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 99c284e3-6d33-434a-84ed-3ec9f92553d5
2024-04-24T10:14:27,454  INFO [main] SessionState: Hive Session ID = 99c284e3-6d33-434a-84ed-3ec9f92553d5
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:27,455  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:27,460  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/99c284e3-6d33-434a-84ed-3ec9f92553d5
2024-04-24T10:14:27,463  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/99c284e3-6d33-434a-84ed-3ec9f92553d5
2024-04-24T10:14:27,465  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/99c284e3-6d33-434a-84ed-3ec9f92553d5/_tmp_space.db
2024-04-24T10:14:27,466  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): drop table if exists junitTypeTest1
2024-04-24T10:14:27,467  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:27,476  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:27,477  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:27,477  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:27,477  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:27,477  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=11}
2024-04-24T10:14:27,477  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.011 seconds
2024-04-24T10:14:27,477  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:27,477  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:27,477  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): drop table if exists junitTypeTest1
2024-04-24T10:14:27,478  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:27,478  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:27,489  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:27,489  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:27,501  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:27,501  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:27,542  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:27,542  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=53, getTable_(GetTableRequest)=11}
2024-04-24T10:14:27,542  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.065 seconds
2024-04-24T10:14:27,542  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): use default
2024-04-24T10:14:27,543  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:27,545  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:27,545  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:27,545  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:27,546  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:14:27,546  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.003 seconds
2024-04-24T10:14:27,546  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:27,546  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:27,546  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): use default
2024-04-24T10:14:27,546  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:27,546  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:27,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:27,550  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:27,550  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=4}
2024-04-24T10:14:27,550  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.004 seconds
2024-04-24T10:14:27,550  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:27,551  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7
2024-04-24T10:14:27,551  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:27,551  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=99c284e3-6d33-434a-84ed-3ec9f92553d5, clientType=HIVECLI]
2024-04-24T10:14:27,551  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:27,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:27,552  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2464490e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f0eabb8 will be shutdown
2024-04-24T10:14:27,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:27,552  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-24T10:14:27,553  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:27,554  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:27,554  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:27,555  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e, with PersistenceManager: null will be shutdown
2024-04-24T10:14:27,555  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@669603c5 created in the thread with id: 1
2024-04-24T10:14:27,557  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e from thread id: 1
2024-04-24T10:14:27,557  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:27,557  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:27,557  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:14:27,558  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:27,558  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:27,559  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@669603c5 will be shutdown
2024-04-24T10:14:27,559  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ef27af9 created in the thread with id: 1
2024-04-24T10:14:27,561  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:27,561  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:27,561  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:27,563  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:27,563  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:27,563  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.013 seconds
2024-04-24T10:14:27,563  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:27,563  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:27,563  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:27,564  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:27,564  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dce3e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ef27af9 will be shutdown
2024-04-24T10:14:27,564  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:27,564  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-24T10:14:27,575 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:27,576 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:27,578  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:14:27,578  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:14:27,578  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:27,579 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:27,579  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:27,579  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:27,579  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101427_b6bf0dd4-1b7a-4f6f-8b25-bbdfde0b87c7); Time taken: 0.016 seconds
]]></system-err>
  </testcase>
  <testcase name="testWriteDate" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="6.551"/>
  <testcase name="testDynamicPartitioningMultiPartColsNoDataInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.923"/>
  <testcase name="testWriteVarchar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.14">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:35,086  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:35,088  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:35,088  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:35,088  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24b1fc38, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@29a3e9e8 will be shutdown
2024-04-24T10:14:35,088  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24b1fc38, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5fa380d9 created in the thread with id: 1
2024-04-24T10:14:35,090  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4
2024-04-24T10:14:35,090  INFO [main] SessionState: Hive Session ID = 89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:35,090  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:35,095  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4
2024-04-24T10:14:35,098  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4
2024-04-24T10:14:35,100  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4/_tmp_space.db
2024-04-24T10:14:35,100  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): drop table if exists junitTypeTest1
2024-04-24T10:14:35,101  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:35,109  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:35,109  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:35,109  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:35,109  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:35,110  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=8, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:35,110  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.009 seconds
2024-04-24T10:14:35,110  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:35,110  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:35,110  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): drop table if exists junitTypeTest1
2024-04-24T10:14:35,110  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:35,110  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:35,121  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:35,121  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:35,131  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:35,131  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:35,172  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:35,172  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=51, getTable_(GetTableRequest)=11, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:35,173  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.062 seconds
2024-04-24T10:14:35,173  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): use default
2024-04-24T10:14:35,173  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:35,175  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:35,175  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:35,175  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:35,175  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:14:35,175  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.002 seconds
2024-04-24T10:14:35,176  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:35,176  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:35,176  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): use default
2024-04-24T10:14:35,176  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:35,176  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:35,178  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:35,179  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:35,179  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:35,180  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.003 seconds
2024-04-24T10:14:35,180  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:35,181  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a
2024-04-24T10:14:35,181  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:35,181  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=89a0c5c2-cb67-4bfd-bbc1-1d2e1c4894b4, clientType=HIVECLI]
2024-04-24T10:14:35,181  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:35,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:35,181  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24b1fc38, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5fa380d9 will be shutdown
2024-04-24T10:14:35,182  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:35,182  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-24T10:14:35,183  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:35,183  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:35,184  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:35,184  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: null will be shutdown
2024-04-24T10:14:35,184  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@17bf4515 created in the thread with id: 1
2024-04-24T10:14:35,186  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31 from thread id: 1
2024-04-24T10:14:35,186  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:35,186  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:35,186  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:14:35,187  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:35,187  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:35,188  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@17bf4515 will be shutdown
2024-04-24T10:14:35,188  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@e36e90b created in the thread with id: 1
2024-04-24T10:14:35,190  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:35,190  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:35,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:35,191  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a
2024-04-24T10:14:35,191  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:35,191  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:35,191  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:35,191  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:35,191  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.011 seconds
2024-04-24T10:14:35,191  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:35,191  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:35,191  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:35,192  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:35,192  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:35,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:35,192  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6d9a8a31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@e36e90b will be shutdown
2024-04-24T10:14:35,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:35,192  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-24T10:14:35,193 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:35,193 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:35,194  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:14:35,194  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:14:35,194  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:35,194 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:35,194  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:35,194  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:35,194  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101435_9adc3e53-e290-4eeb-a250-9fa91cc1c28a); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testBagNStruct" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.263"/>
  <testcase name="testStaticPartitioningMultiPartCols" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.213"/>
  <testcase name="testPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.345"/>
  <testcase name="testStoreWithNoCtorArgs" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.168"/>
  <testcase name="testWriteDecimalX" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.078">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:14:40,215  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:40,216  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:40,217  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,217  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,218  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6d645c87 will be shutdown
2024-04-24T10:14:40,218  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b6c92d1 created in the thread with id: 1
2024-04-24T10:14:40,220  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 80c0019e-15e1-400f-bc4a-395108708860
2024-04-24T10:14:40,220  INFO [main] SessionState: Hive Session ID = 80c0019e-15e1-400f-bc4a-395108708860
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:40,220  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:40,225  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/80c0019e-15e1-400f-bc4a-395108708860
2024-04-24T10:14:40,228  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/80c0019e-15e1-400f-bc4a-395108708860
2024-04-24T10:14:40,230  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/80c0019e-15e1-400f-bc4a-395108708860/_tmp_space.db
2024-04-24T10:14:40,231  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): drop table if exists junitTypeTest1
2024-04-24T10:14:40,232  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:40,234  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,234  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,234  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,234  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,234  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.003 seconds
2024-04-24T10:14:40,234  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,234  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,235  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): drop table if exists junitTypeTest1
2024-04-24T10:14:40,235  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,235  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:40,237  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,237  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,237  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.002 seconds
2024-04-24T10:14:40,238  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): use default
2024-04-24T10:14:40,238  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,239  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,239  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,239  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,239  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-24T10:14:40,240  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.001 seconds
2024-04-24T10:14:40,240  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,240  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,240  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): use default
2024-04-24T10:14:40,240  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,240  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,242  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,242  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:14:40,242  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.002 seconds
2024-04-24T10:14:40,243  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:40,244  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab
2024-04-24T10:14:40,244  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:40,246  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=80c0019e-15e1-400f-bc4a-395108708860, clientType=HIVECLI]
2024-04-24T10:14:40,246  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:40,246  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:40,246  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1f7e2395, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4b6c92d1 will be shutdown
2024-04-24T10:14:40,246  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:40,246  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -38
2024-04-24T10:14:40,248  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,249  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:40,249  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,249  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: null will be shutdown
2024-04-24T10:14:40,249  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5189077a created in the thread with id: 1
2024-04-24T10:14:40,251  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77 from thread id: 1
2024-04-24T10:14:40,251  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:40,251  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:40,251  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:14:40,253  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,253  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,254  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5189077a will be shutdown
2024-04-24T10:14:40,254  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@26fcf280 created in the thread with id: 1
2024-04-24T10:14:40,257  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:40,257  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:40,257  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,258  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab
2024-04-24T10:14:40,258  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,258  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,258  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,258  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:40,258  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.015 seconds
2024-04-24T10:14:40,259  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,259  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,259  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:40,259  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,259  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:40,259  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:40,259  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@24af2f77, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@26fcf280 will be shutdown
2024-04-24T10:14:40,259  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:40,259  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -39
2024-04-24T10:14:40,260 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:40,261 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:40,261  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:14:40,261  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:14:40,261  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:40,261 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:40,261  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,262  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:40,262  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_a5c80e5b-6f29-4646-a6d2-94b15148cbab); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testPartitionPublish" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.648"/>
  <testcase name="testWriteDecimal" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.07">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 54 more
]]></error>
    <system-err><![CDATA[2024-04-24T10:14:40,940  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:40,940  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:40,940  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:40,940  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:40,941  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:40,943  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,944  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,944  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@217b0f0e will be shutdown
2024-04-24T10:14:40,944  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa9893 created in the thread with id: 1
2024-04-24T10:14:40,946  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 3c52ca02-1b26-43f1-97aa-d44787538069
2024-04-24T10:14:40,946  INFO [main] SessionState: Hive Session ID = 3c52ca02-1b26-43f1-97aa-d44787538069
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:40,946  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:40,952  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3c52ca02-1b26-43f1-97aa-d44787538069
2024-04-24T10:14:40,954  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/3c52ca02-1b26-43f1-97aa-d44787538069
2024-04-24T10:14:40,956  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3c52ca02-1b26-43f1-97aa-d44787538069/_tmp_space.db
2024-04-24T10:14:40,957  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): drop table if exists junitTypeTest1
2024-04-24T10:14:40,957  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:40,959  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,959  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,959  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,960  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,960  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.002 seconds
2024-04-24T10:14:40,960  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,960  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,960  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): drop table if exists junitTypeTest1
2024-04-24T10:14:40,960  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,960  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-24T10:14:40,962  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,962  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,962  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.002 seconds
2024-04-24T10:14:40,963  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): use default
2024-04-24T10:14:40,963  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,964  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,964  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,964  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,964  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,964  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.001 seconds
2024-04-24T10:14:40,964  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,964  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,964  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): use default
2024-04-24T10:14:40,965  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,965  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,965  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,966  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,966  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:40,967  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.002 seconds
2024-04-24T10:14:40,967  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:40,967  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92
2024-04-24T10:14:40,967  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:40,967  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3c52ca02-1b26-43f1-97aa-d44787538069, clientType=HIVECLI]
2024-04-24T10:14:40,968  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:40,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:40,968  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b44c03e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa9893 will be shutdown
2024-04-24T10:14:40,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:40,968  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-04-24T10:14:40,969  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,970  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:40,970  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,970  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: null will be shutdown
2024-04-24T10:14:40,970  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4440a153 created in the thread with id: 1
2024-04-24T10:14:40,972  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790 from thread id: 1
2024-04-24T10:14:40,972  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:40,972  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:40,972  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-24T10:14:40,973  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:40,973  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:40,973  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4440a153 will be shutdown
2024-04-24T10:14:40,973  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f415ae0 created in the thread with id: 1
2024-04-24T10:14:40,975  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:40,975  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:40,975  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:40,976  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92
2024-04-24T10:14:40,976  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:40,976  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:40,976  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:40,976  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:40,976  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.009 seconds
2024-04-24T10:14:40,976  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:40,976  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:40,976  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:40,977  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:40,977  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:40,977  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:40,977  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7ce8c790, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f415ae0 will be shutdown
2024-04-24T10:14:40,977  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:40,977  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-04-24T10:14:40,978 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:40,978 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:40,979  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:14:40,979  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-24T10:14:40,979  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:40,979 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-24T10:14:40,979  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:40,979  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:40,979  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101440_b40c90c8-a468-4a20-b11e-21ef91f96c92); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testMultiPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.646">
    <failure message="expected:&lt;1&gt; but was:&lt;5&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<5>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-24T10:14:41,011  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,011  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,012  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,014  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,015  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,015  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,016  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,016  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c44dccf created in the thread with id: 1
2024-04-24T10:14:41,017  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2 from thread id: 1
2024-04-24T10:14:41,017  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = c0908094-0493-4f3a-abb6-dc648c2bf021
2024-04-24T10:14:41,018  INFO [main] SessionState: Hive Session ID = c0908094-0493-4f3a-abb6-dc648c2bf021
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:41,018  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:14:41,023  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c0908094-0493-4f3a-abb6-dc648c2bf021
2024-04-24T10:14:41,025  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/c0908094-0493-4f3a-abb6-dc648c2bf021
2024-04-24T10:14:41,027  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c0908094-0493-4f3a-abb6-dc648c2bf021/_tmp_space.db
2024-04-24T10:14:41,027  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): drop table if exists employee
2024-04-24T10:14:41,028  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,029  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,029  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c44dccf will be shutdown
2024-04-24T10:14:41,029  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3644e451 created in the thread with id: 1
2024-04-24T10:14:41,030  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,031  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,031  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,033  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:41,033  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:41,033  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:41,033  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:14:41,033  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.006 seconds
2024-04-24T10:14:41,033  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:41,033  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:41,033  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): drop table if exists employee
2024-04-24T10:14:41,034  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:41,034  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,036  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:41,036  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:41,036  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.003 seconds
2024-04-24T10:14:41,037  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): use default
2024-04-24T10:14:41,037  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:41,038  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:41,038  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:41,038  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:41,038  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:14:41,038  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.001 seconds
2024-04-24T10:14:41,039  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:41,039  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:41,039  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): use default
2024-04-24T10:14:41,039  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:41,039  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:41,040  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:41,041  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:41,041  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:14:41,041  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.002 seconds
2024-04-24T10:14:41,041  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:41,042  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505
2024-04-24T10:14:41,042  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:41,042  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c0908094-0493-4f3a-abb6-dc648c2bf021, clientType=HIVECLI]
2024-04-24T10:14:41,042  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:14:41,042  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:41,042  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6796beb2, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3644e451 will be shutdown
2024-04-24T10:14:41,042  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,042  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-04-24T10:14:41,043  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,044  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,044  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,044  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,045  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b4937c6 created in the thread with id: 1
2024-04-24T10:14:41,046  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c from thread id: 1
2024-04-24T10:14:41,046  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,046  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,046  INFO [main] parse.CalcitePlanner: Creating table default.employee position=13
2024-04-24T10:14:41,047  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,047  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,047  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b4937c6 will be shutdown
2024-04-24T10:14:41,047  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@79b7d980 created in the thread with id: 1
2024-04-24T10:14:41,049  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,049  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,049  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:14:41,050  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:14:41,050  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:41,050  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.009 seconds
2024-04-24T10:14:41,050  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-24T10:14:41,050  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:14:41,050  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:14:41,051  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:41,051  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@45ab982c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@79b7d980 will be shutdown
2024-04-24T10:14:41,051  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,051  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-04-24T10:14:41,052  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,053  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,053  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,053  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,053  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fa7cc82 created in the thread with id: 1
2024-04-24T10:14:41,055  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2e8022f8 from thread id: 1
2024-04-24T10:14:41,055  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,055  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:employee, dbName:default, owner:alex, createTime:1713978881, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:emp_id, type:int, comment:null), FieldSchema(name:emp_name, type:string, comment:null), FieldSchema(name:emp_start_date, type:string, comment:null), FieldSchema(name:emp_gender, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:emp_country, type:string, comment:null), FieldSchema(name:emp_state, type:string, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:14:41,059  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/employee
2024-04-24T10:14:41,079  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:41,079  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=24}
2024-04-24T10:14:41,079  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.029 seconds
2024-04-24T10:14:41,098  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:14:41,099  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-717cd75a-4225-480d-b53e-8ad02cf2f0f5
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,184  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,184  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:41,185  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:14:41,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,188  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,197  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,218  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,219  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,234  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,234  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:41,235  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:14:41,236  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,238  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,247  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,267  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,267  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,268  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,282  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,283  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,283  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:41,284  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:14:41,285  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,287  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,298  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,317  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,318  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,318  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,318  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,318  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,318  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,334  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:14:41,334  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:14:41,335  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,338  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:41,346  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,365  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,366  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,366  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,366  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,366  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,366  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,366  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,366  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,369  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:14:41,377  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:14:41,377  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:14:41,387  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:14:41,390  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 5
2024-04-24T10:14:41,391  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 map-only splittees.
2024-04-24T10:14:41,391  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 out of total 5 MR operators.
2024-04-24T10:14:41,391  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:14:41,397  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:41,399  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:14:41,399  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:14:41,400  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:14:41,400  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:14:41,400  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:14:41,400  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713978881400-0
2024-04-24T10:14:41,408  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:14:41,410  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:41,425  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,426  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,426  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,427  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,427  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,428  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,428  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@e5ecb83, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,428  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@e5ecb83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7cda5831 created in the thread with id: 1970
2024-04-24T10:14:41,429  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@e5ecb83 from thread id: 1970
2024-04-24T10:14:41,429  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,429  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,430  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:41,430  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@e5ecb83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7cda5831 will be shutdown
2024-04-24T10:14:41,430  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,430  INFO [JobControl] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-04-24T10:14:41,430  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,431  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,432  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f7aa437, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,432  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f7aa437, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@d31a362 created in the thread with id: 1970
2024-04-24T10:14:41,433  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2f7aa437 from thread id: 1970
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,450  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,451  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,451  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,471  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,471  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,472  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,489  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,489  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,490  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,503  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:14:41,508  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:14:41,509  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:14:41,510  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:41,510  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:14:41,510  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:14:41,528  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:14:41,540  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1667404171_0037
2024-04-24T10:14:41,540  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:14:41,592  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:14:41,592  INFO [Thread-1844] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:14:41,597  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,597  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,600  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,600  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,603  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,603  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,606  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,606  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,607  INFO [Thread-1844] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:14:41,608  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,609  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,621  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,621  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,632  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,632  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,643  INFO [Thread-1844] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,643  INFO [Thread-1844] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,653  INFO [Thread-1844] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:14:41,654  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1667404171_0037_m_000000_0
2024-04-24T10:14:41,660  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,660  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,664  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,664  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,668  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,668  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,672  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,672  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,675  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,675  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,677  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,677  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,679  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,679  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,681  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,681  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,681  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:14:41,682  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 132
Input split[0]:
   Length = 132
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:14:41,684  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:14:41,684  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/input.data:0+132
2024-04-24T10:14:41,685  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:14:41,685  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-24T10:14:41,688  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-24T10:14:41,692  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,692  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,700  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,700  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,706  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,706  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,711  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,711  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,751  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:41,752  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1667404171_0037_m_000000_0 is done. And is in the process of committing
2024-04-24T10:14:41,753  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,753  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,757  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:14:41,757  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1667404171_0037_m_000000_0 is allowed to commit now
2024-04-24T10:14:41,758  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,758  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,759  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1667404171_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/employee/_SCRATCH0,6404989669012364/emp_country=IN/emp_state=TN
2024-04-24T10:14:41,760  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,760  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,762  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1667404171_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/employee/_SCRATCH0,8740099305105308/emp_country=IN/emp_state=KA
2024-04-24T10:14:41,763  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,763  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,764  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1667404171_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/employee/_SCRATCH0,0913416771723089/emp_country=IN/emp_state=KL
2024-04-24T10:14:41,765  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:14:41,765  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:14:41,767  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1667404171_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/warehouse/employee/_SCRATCH0,2201262096716552/emp_country=IN/emp_state=AP
2024-04-24T10:14:41,767  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:14:41,767  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1667404171_0037_m_000000_0' done.
2024-04-24T10:14:41,767  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1667404171_0037_m_000000_0: Counters: 19
	File System Counters
		FILE: Number of bytes read=107859
		FILE: Number of bytes written=21151177
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=0
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=982515712
	MultiStoreCounters
		Output records in _0_employee=1
		Output records in _1_employee=1
		Output records in _2_employee=1
		Output records in _3_employee=1
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:14:41,768  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1667404171_0037_m_000000_0
2024-04-24T10:14:41,768  INFO [Thread-1844] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,808  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,809  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,809  INFO [Thread-1844] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:14:41,810  INFO [Thread-1844] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,810  INFO [Thread-1844] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:14:41,810  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d316b98, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,811  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d316b98, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a3aaaa9 created in the thread with id: 1990
2024-04-24T10:14:41,812  INFO [Thread-1844] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d316b98 from thread id: 1990
2024-04-24T10:14:41,812  INFO [Thread-1844] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:14:41,812  INFO [Thread-1844] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:14:41,813  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:14:41,813  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5d316b98, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a3aaaa9 will be shutdown
2024-04-24T10:14:41,813  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,813  INFO [Thread-1844] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-04-24T10:14:41,813  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,814  INFO [Thread-1844] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:14:41,814  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6dc972d8, with PersistenceManager: null will be shutdown
2024-04-24T10:14:41,815  INFO [Thread-1844] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6dc972d8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7de96832 created in the thread with id: 1990
2024-04-24T10:14:41,816  INFO [Thread-1844] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6dc972d8 from thread id: 1990
2024-04-24T10:14:41,868  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,868  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-04-24T10:14:41,868  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,868  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -51
2024-04-24T10:14:41,868  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:14:41,868  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -52
2024-04-24T10:14:41,889  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,889  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,889  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,890  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,890  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=TN}].
2024-04-24T10:14:41,892  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,TN]	
2024-04-24T10:14:41,898  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:14:41,899  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:14:41,906  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:14:41,906  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 458
2024-04-24T10:14:41,909  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1667404171_0037
2024-04-24T10:14:41,909  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,AP,KA,KL,TN
2024-04-24T10:14:41,909  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-24T10:14:41,911  INFO [main] mapReduceLayer.MapReduceLauncher: 50% complete
2024-04-24T10:14:41,911  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1667404171_0037]
2024-04-24T10:14:41,938  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:14:41,970  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:41,970  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:41,970  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:41,971  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:41,971  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:41,971  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:41,973  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,012  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,013  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,013  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,013  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,013  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,013  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,013  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,022  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_state=KA, emp_country=IN}].
2024-04-24T10:14:42,023  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KA]	
2024-04-24T10:14:42,026  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:14:42,027  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:14:42,034  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:14:42,035  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 458
2024-04-24T10:14:42,059  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,092  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,093  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,093  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,094  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,140  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,140  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,141  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,150  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KL}].
2024-04-24T10:14:42,151  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KL]	
2024-04-24T10:14:42,154  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:14:42,154  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:14:42,161  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:14:42,162  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 456
2024-04-24T10:14:42,187  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,222  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,222  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,223  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,224  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:14:42,265  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,266  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,266  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,266  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,275  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=AP}].
2024-04-24T10:14:42,276  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,AP]	
2024-04-24T10:14:42,280  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:14:42,281  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:14:42,290  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-24T10:14:42,290  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 456
2024-04-24T10:14:42,316  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:14:42,351  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:14:42,352  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:14:42,352  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:14:42,352  WARN [Thread-1844] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:14:42,352  INFO [Thread-1844] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:14:42,352  INFO [Thread-1844] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:14:42,354  INFO [Thread-1844] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:14:42,413  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:42,416  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:42,418  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:42,439  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:14:42,439  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:42,440  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:14:42,440  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:42,441  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:14:42,441  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:42,441  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:14:42,441  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:14:42,442  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:14:42,442  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:14:41	2024-04-24 10:14:42	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1667404171_0037	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,AP,KA,KL,TN	MULTI_QUERY,MAP_ONLY	employee,employee,employee,employee,

Input(s):
Successfully read 4 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713978839842/input.data"

Output(s):
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"

Counters:
Total records written : 4
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1667404171_0037


2024-04-24T10:14:42,447  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:42,449  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:14:42,451  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:14:42,453  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): select * from employee
2024-04-24T10:14:42,453  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505
2024-04-24T10:14:42,453  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:14:42,453  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-24T10:14:42,453  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T10:14:42,454  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:42,464  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:14:42,464  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T10:14:42,464  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T10:14:42,464  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-24T10:14:42,475  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.employee	
2024-04-24T10:14:42,483  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.employee, projIndxSet: [0, 1, 2, 3, 4, 5], allowMissingStats: true
2024-04-24T10:14:42,484  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.employee	
2024-04-24T10:14:42,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.employee	
2024-04-24T10:14:42,531  WARN [main] calcite.RelOptHiveTable: No Stats for default@employee, Columns: emp_gender, emp_start_date, emp_name, emp_id
No Stats for default@employee, Columns: emp_gender, emp_start_date, emp_name, emp_id
2024-04-24T10:14:42,531  INFO [main] SessionState: No Stats for default@employee, Columns: emp_gender, emp_start_date, emp_name, emp_id
2024-04-24T10:14:42,579  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-24T10:14:42,579  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-24T10:14:42,579  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-24T10:14:42,581  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/c0908094-0493-4f3a-abb6-dc648c2bf021/hive_2024-04-24_10-14-42_453_3572216640372415350-1/-mr-10001/.hive-staging_hive_2024-04-24_10-14-42_453_3572216640372415350-1
2024-04-24T10:14:42,588  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-24T10:14:42,590  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-24T10:14:42,591  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-24T10:14:42,591  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505
2024-04-24T10:14:42,591  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:14:42,591  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:employee.emp_id, type:int, comment:null), FieldSchema(name:employee.emp_name, type:string, comment:null), FieldSchema(name:employee.emp_start_date, type:string, comment:null), FieldSchema(name:employee.emp_gender, type:string, comment:null), FieldSchema(name:employee.emp_country, type:string, comment:null), FieldSchema(name:employee.emp_state, type:string, comment:null)], properties:null)
2024-04-24T10:14:42,592  INFO [main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-04-24T10:14:42,593  INFO [main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-04-24T10:14:42,593  INFO [main] exec.SelectOperator: SELECT struct<emp_id:int,emp_name:string,emp_start_date:string,emp_gender:string,emp_country:string,emp_state:string>
2024-04-24T10:14:42,593  INFO [main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-04-24T10:14:42,594  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:14:42,594  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getAggrColStatsFor_(String, String, List, List, String, String)=15, isCompatibleWith_(Configuration)=0, listPartitions_(String, String, short)=31, flushCache_()=0, getTable_(GetTableRequest)=11, getAllTableConstraints_(AllTableConstraintsRequest)=5}
2024-04-24T10:14:42,594  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.14 seconds
2024-04-24T10:14:42,594  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:14:42,594  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:14:42,594  INFO [main] ql.Driver: Executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505): select * from employee
2024-04-24T10:14:42,594  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:14:42,594  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:14:42,594  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424101441_62d1da86-06ef-4fd1-aee6-14d87ef5c505); Time taken: 0.0 seconds
2024-04-24T10:14:42,598  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:42,599  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:14:42,603  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:42,604  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:14:42,604  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:14:42,609  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:42,609  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:14:42,609  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:14:42,614  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:14:42,614  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-24T10:14:42,614  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-24T10:14:42,616  INFO [main] exec.TableScanOperator: Closing Operator: TS[0]
2024-04-24T10:14:42,616  INFO [main] exec.TableScanOperator: RECORDS_OUT_OPERATOR_TS_0:4, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T10:14:42,616  INFO [main] exec.SelectOperator: Closing Operator: SEL[1]
2024-04-24T10:14:42,616  INFO [main] exec.SelectOperator: RECORDS_OUT_OPERATOR_SEL_1:4, RECORDS_OUT_INTERMEDIATE:0, 
2024-04-24T10:14:42,616  INFO [main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-04-24T10:14:42,616  INFO [main] exec.ListSinkOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_LIST_SINK_3:4, 
2024-04-24T10:14:42,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-24T10:14:42,625  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testStaticPartitioningMultiPartColsNoData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.308"/>
  <testcase name="testStoreMultiTables" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.558"/>
  <testcase name="testStoreWithNoSchema" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.877"/>
  <testcase name="testNoAlias" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.314"/>
  <testcase name="testStoreFuncSimple" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.854"/>
</testsuite>