2024-04-24T10:36:55,434  INFO [main] conf.HiveConf: Found configuration file file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-site.xml
2024-04-24T10:36:55,734  WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-24T10:36:55,794  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:36:55,794  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:36:55,795  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:36:55,795  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:36:55,795  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:36:55,796  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:36:55,796  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:36:55,796  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:36:55,797  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:36:55,797  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:36:55,797  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:36:55,979  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:36:56,192  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:36:56,226  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:36:56,233  INFO [main] metastore.PersistenceManagerProvider: Updating the pmf due to property change
2024-04-24T10:36:56,233  INFO [main] metastore.PersistenceManagerProvider: Current pmf properties are uninitialized
2024-04-24T10:36:56,253  WARN [main] hikari.HikariConfig: HikariPool-1 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T10:36:56,258  INFO [main] hikari.HikariDataSource: HikariPool-1 - Starting...
2024-04-24T10:36:57,032  INFO [main] pool.PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T10:36:57,035  INFO [main] hikari.HikariDataSource: HikariPool-1 - Start completed.
log4j: Trying to find [log4j.xml] using context classloader sun.misc.Launcher$AppClassLoader@330bedb4.
log4j: Trying to find [log4j.xml] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.xml] using ClassLoader.getSystemResource().
log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@330bedb4.
log4j: Trying to find [log4j.properties] using sun.misc.Launcher$AppClassLoader@330bedb4 class loader.
log4j: Trying to find [log4j.properties] using ClassLoader.getSystemResource().
log4j: Could not find resource: [null].
log4j:WARN No appenders could be found for logger (DataNucleus.General).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2024-04-24T10:36:57,692  INFO [main] metastore.PersistenceManagerProvider: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2024-04-24T10:36:57,692  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06, with PersistenceManager: null will be shutdown
2024-04-24T10:36:57,716  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1460c81d created in the thread with id: 1
2024-04-24T10:37:00,168  WARN [main] metastore.ObjectStore: Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 4.0.0
2024-04-24T10:37:00,168  WARN [main] metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 4.0.0, comment = Set by MetaStore alex@127.0.1.1
2024-04-24T10:37:00,168  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06 from thread id: 1
2024-04-24T10:37:00,392  INFO [main] metastore.HMSHandler: Started creating a default database with name: default
2024-04-24T10:37:00,426  INFO [main] metastore.HMSHandler: Successfully created a default database with name: default
2024-04-24T10:37:00,462  INFO [main] metastore.HMSHandler: Added admin role in metastore
2024-04-24T10:37:00,465  INFO [main] metastore.HMSHandler: Added public role in metastore
2024-04-24T10:37:00,581  INFO [main] metastore.HMSHandler: Added hive_admin_user to admin role
2024-04-24T10:37:00,589  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.HiveProtoEventsCleanerTask service with frequency 86400000ms.
2024-04-24T10:37:00,590  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask service with frequency 3600000ms.
2024-04-24T10:37:00,591  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask service with frequency 60000ms.
2024-04-24T10:37:00,592  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.ReplicationMetricsMaintTask service with frequency 86400000ms.
2024-04-24T10:37:00,619  WARN [main] hikari.HikariConfig: HikariPool-2 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T10:37:00,622  INFO [main] hikari.HikariDataSource: HikariPool-2 - Starting...
2024-04-24T10:37:00,624  INFO [main] pool.PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T10:37:00,625  INFO [main] hikari.HikariDataSource: HikariPool-2 - Start completed.
2024-04-24T10:37:00,627  WARN [main] hikari.HikariConfig: HikariPool-3 - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.
2024-04-24T10:37:00,629  INFO [main] hikari.HikariDataSource: HikariPool-3 - Starting...
2024-04-24T10:37:00,631  INFO [main] pool.PoolBase: HikariPool-3 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
2024-04-24T10:37:00,631  INFO [main] hikari.HikariDataSource: HikariPool-3 - Start completed.
2024-04-24T10:37:00,636  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.metrics.AcidMetricService service with frequency 300000ms.
2024-04-24T10:37:00,637  INFO [main] metastore.HMSHandler: Scheduling for org.apache.hadoop.hive.metastore.events.EventCleanerTask service with frequency 0ms.
2024-04-24T10:37:00,640  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 83db998c-cf99-4fb4-9b5c-83cae9b6838d
2024-04-24T10:37:00,845  INFO [main] SessionState: Hive Session ID = 83db998c-cf99-4fb4-9b5c-83cae9b6838d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:00,857  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:00,920  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/83db998c-cf99-4fb4-9b5c-83cae9b6838d
2024-04-24T10:37:00,924  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/83db998c-cf99-4fb4-9b5c-83cae9b6838d
2024-04-24T10:37:00,928  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/83db998c-cf99-4fb4-9b5c-83cae9b6838d/_tmp_space.db
2024-04-24T10:37:00,976  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:02,192  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,195  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,196  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,198  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,199  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,199  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,203  WARN [main] exec.FunctionRegistry: UDF Class org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.
2024-04-24T10:37:02,263  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:02,263  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:02,265  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1460c81d will be shutdown
2024-04-24T10:37:02,265  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@48bc2fce created in the thread with id: 1
2024-04-24T10:37:02,281  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:02,281  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:02,323  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_functions	
2024-04-24T10:37:02,609  INFO [main] reflections.Reflections: Reflections took 201 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T10:37:02,782  INFO [main] reflections.Reflections: Reflections took 138 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T10:37:02,910  INFO [main] reflections.Reflections: Reflections took 122 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T10:37:02,918  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,059  INFO [main] reflections.Reflections: Reflections took 124 ms to scan 2 urls, producing 53 keys and 784 values 
2024-04-24T10:37:03,110  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,112  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,115  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,115  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getAllFunctions_()=49, isCompatibleWith_(Configuration)=1, flushCache_()=0, getDatabase_(String)=14}
2024-04-24T10:37:03,116  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 2.141 seconds
2024-04-24T10:37:03,117  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,117  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,120  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,124  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,125  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,128  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,130  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,130  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=4}
2024-04-24T10:37:03,131  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.01 seconds
2024-04-24T10:37:03,132  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,208  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,210  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,229  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=83db998c-cf99-4fb4-9b5c-83cae9b6838d, clientType=HIVECLI]
2024-04-24T10:37:03,231  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:37:03,233  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:03,233  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1bf39d06, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@48bc2fce will be shutdown
2024-04-24T10:37:03,233  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:03,233  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -1
2024-04-24T10:37:03,235  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:03,236  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:03,236  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:03,238  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342, with PersistenceManager: null will be shutdown
2024-04-24T10:37:03,238  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eb1c615 created in the thread with id: 1
2024-04-24T10:37:03,247  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342 from thread id: 1
2024-04-24T10:37:03,247  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:03,248  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:03,254  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:37:03,268  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:03,268  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:03,269  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eb1c615 will be shutdown
2024-04-24T10:37:03,269  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2ee92e7d created in the thread with id: 1
2024-04-24T10:37:03,274  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:03,274  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:03,277  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,288  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,288  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,288  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,288  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,288  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:37:03,289  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.156 seconds
2024-04-24T10:37:03,289  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,290  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,290  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,290  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,290  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:37:03,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:03,291  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@38087342, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2ee92e7d will be shutdown
2024-04-24T10:37:03,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:03,291  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -2
2024-04-24T10:37:03,372  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:03,373  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:03,373  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:03,374  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: null will be shutdown
2024-04-24T10:37:03,375  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23e2c1ca created in the thread with id: 1
2024-04-24T10:37:03,379  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d from thread id: 1
2024-04-24T10:37:03,379  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:03,379  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:03,380  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0, numFiles=0, bucketing_version=2, transactional=false, numRows=0, rawDataSize=0, totalSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,400  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:37:03,547  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,548  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=167}
2024-04-24T10:37:03,548  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.257 seconds
2024-04-24T10:37:03,549  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,552  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,552  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,553  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,553  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,553  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.004 seconds
2024-04-24T10:37:03,553  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,553  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,554  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,554  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,554  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,557  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,559  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,560  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=5, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,560  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,561  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,564  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,564  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,564  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:37:03,565  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,567  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,567  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,567  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,567  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,567  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3, flushCache_()=0}
2024-04-24T10:37:03,568  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.007 seconds
2024-04-24T10:37:03,568  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,568  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,568  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,568  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,572  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=0, numRows=0, numFilesErasureCoded=0, rawDataSize=0, totalSize=0, transactional=false, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,586  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:37:03,621  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,621  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=49}
2024-04-24T10:37:03,621  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.053 seconds
2024-04-24T10:37:03,622  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,623  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,625  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,625  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,625  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,625  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:03,626  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.003 seconds
2024-04-24T10:37:03,626  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,626  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,626  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,626  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,627  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,629  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,631  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,631  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getDatabase_(String)=4}
2024-04-24T10:37:03,632  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,632  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,634  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,634  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,634  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:37:03,634  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,637  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,637  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,637  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,637  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,637  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, flushCache_()=0, isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:03,638  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,638  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,638  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,638  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,638  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,648  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:37:03,683  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,683  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, createTable_(Table)=43}
2024-04-24T10:37:03,683  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.045 seconds
2024-04-24T10:37:03,684  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,685  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,687  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,687  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,687  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,687  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,687  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.003 seconds
2024-04-24T10:37:03,688  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,688  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,688  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,688  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,689  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,691  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,693  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,693  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=5}
2024-04-24T10:37:03,694  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,694  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,696  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,696  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,696  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:37:03,697  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,699  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,699  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,699  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,699  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,700  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=1, flushCache_()=0}
2024-04-24T10:37:03,700  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,700  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,700  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,700  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,700  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,702  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, totalSize=0, transactional=false, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, rawDataSize=0, numFiles=0, numRows=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,710  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:37:03,736  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,736  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=34}
2024-04-24T10:37:03,736  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.036 seconds
2024-04-24T10:37:03,737  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create database if not exists junit_specific_db
2024-04-24T10:37:03,738  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,738  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,738  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,738  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,739  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.001 seconds
2024-04-24T10:37:03,739  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,739  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,739  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create database if not exists junit_specific_db
2024-04-24T10:37:03,739  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,740  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:37:03,741  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:37:03,748  INFO [main] metastore.HMSHandler: Creating database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:37:03,748  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:37:03,751  INFO [main] metastore.HMSHandler: Created database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:37:03,756  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,756  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, createDatabase_(Database)=16}
2024-04-24T10:37:03,756  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.017 seconds
2024-04-24T10:37:03,757  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use junit_specific_db
2024-04-24T10:37:03,758  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:03,760  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,760  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,760  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,760  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,760  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.003 seconds
2024-04-24T10:37:03,761  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,761  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,761  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use junit_specific_db
2024-04-24T10:37:03,761  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,761  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:03,764  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:03,766  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,766  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=5, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,767  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,767  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,768  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,769  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,769  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:37:03,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:03,771  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,771  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,771  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,771  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,772  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:37:03,772  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.004 seconds
2024-04-24T10:37:03,772  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,772  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,772  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,772  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,774  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, totalSize=0, transactional=false, numFiles=0, numRows=0, numFilesErasureCoded=0, bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,781  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2
2024-04-24T10:37:03,808  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,808  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=34, isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:03,808  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.036 seconds
2024-04-24T10:37:03,809  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,810  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,812  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,812  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,813  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,813  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:03,813  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.003 seconds
2024-04-24T10:37:03,813  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,814  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,814  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): use default
2024-04-24T10:37:03,814  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,814  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,817  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,819  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,819  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=5, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,820  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.005 seconds
2024-04-24T10:37:03,820  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,821  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,822  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,822  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_date position=13
2024-04-24T10:37:03,822  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,824  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,824  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,824  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,824  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,824  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,825  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.004 seconds
2024-04-24T10:37:03,825  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,825  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,825  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:03,825  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,827  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_date, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:date, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,834  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date
2024-04-24T10:37:03,862  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,862  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=34, isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:03,862  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.037 seconds
2024-04-24T10:37:03,869  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:37:03,874  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,874  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:03,874  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_alltypes position=13
2024-04-24T10:37:03,878  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:03,881  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,882  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:03,882  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:03,882  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:03,882  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3, flushCache_()=0}
2024-04-24T10:37:03,882  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.013 seconds
2024-04-24T10:37:03,883  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:03,883  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:03,883  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:37:03,883  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:03,928  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_alltypes, dbName:default, owner:alex, createTime:1713980223, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c_boolean, type:boolean, comment:null), FieldSchema(name:c_tinyint, type:tinyint, comment:null), FieldSchema(name:c_smallint, type:smallint, comment:null), FieldSchema(name:c_int, type:int, comment:null), FieldSchema(name:c_bigint, type:bigint, comment:null), FieldSchema(name:c_float, type:float, comment:null), FieldSchema(name:c_double, type:double, comment:null), FieldSchema(name:c_decimal, type:decimal(5,2), comment:null), FieldSchema(name:c_string, type:string, comment:null), FieldSchema(name:c_char, type:char(10), comment:null), FieldSchema(name:c_varchar, type:varchar(20), comment:null), FieldSchema(name:c_binary, type:binary, comment:null), FieldSchema(name:c_date, type:date, comment:null), FieldSchema(name:c_timestamp, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, field.delim=	}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numFiles=0, rawDataSize=0, bucketing_version=2, numFilesErasureCoded=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"c_bigint":"true","c_binary":"true","c_boolean":"true","c_char":"true","c_date":"true","c_decimal":"true","c_double":"true","c_float":"true","c_int":"true","c_smallint":"true","c_string":"true","c_timestamp":"true","c_tinyint":"true","c_varchar":"true"}}}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:03,936  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_alltypes
2024-04-24T10:37:03,979  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:03,979  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=51, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:03,979  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.096 seconds
2024-04-24T10:37:03,980  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:37:03,982  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:03,989  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:04,069  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:04,106  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85
2024-04-24T10:37:04,106  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:04,106  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:04,106  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:04,106  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=83}
2024-04-24T10:37:04,106  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.126 seconds
2024-04-24T10:37:04,107  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:04,107  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:04,107  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:37:04,107  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.junit_unparted_alltypes
2024-04-24T10:37:04,108  INFO [main] exec.Task: Loading data to table default.junit_unparted_alltypes from file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data
2024-04-24T10:37:04,108  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:04,125  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:04,128  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:04,144  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:04,165  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:37:04,165  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:37:04,227  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T10:37:04,228  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:04,242  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:04,243  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T10:37:04,243  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:04,258  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:04,261  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:37:04,262  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:37:04,308  INFO [main] stats.BasicStatsTask: Table default.junit_unparted_alltypes stats: [numFiles=1, numRows=0, totalSize=329, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T10:37:04,308  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:04,308  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=64, alter_table_(String, String, String, Table, EnvironmentContext, String)=106, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:04,308  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.201 seconds
2024-04-24T10:37:04,360  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:37:04,369  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-88f80a0e-6ba7-4d63-9377-1b5134cf8ce6
2024-04-24T10:37:04,896  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:04,896  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -3
2024-04-24T10:37:04,960  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:04,960  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:04,960  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:04,960  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:04,960  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:04,961  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:04,961  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:04,963  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:37:04,971  INFO [main] common.HiveClientCache: Initializing cache: eviction-timeout=120 initial-capacity=50 maximum-capacity=50
2024-04-24T10:37:04,979  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:04,979  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:04,980  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23e2c1ca will be shutdown
2024-04-24T10:37:04,980  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@ef85567 created in the thread with id: 1
2024-04-24T10:37:04,990  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:04,990  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:05,029  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,040  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:05,053  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,151  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,151  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,151  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,151  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,152  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,153  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,155  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,175  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,175  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,175  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,175  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,176  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,176  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,177  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:37:05,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,184  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:05,199  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,236  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,237  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,237  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,237  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,238  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,265  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,266  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:37:05,268  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,271  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:37:05,284  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,316  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,317  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,317  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,317  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,317  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,317  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,318  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,338  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,339  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,339  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,339  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,340  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.stats.column.autogather=true, test.property1=value1, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:37:05,341  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,344  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:05,358  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,384  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,384  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,384  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,384  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,385  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,385  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,386  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,404  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,405  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,405  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,405  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,405  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,405  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,406  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:37:05,407  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:05,423  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,448  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,449  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,449  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,449  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,449  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,449  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,450  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,471  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,472  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,473  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:37:05,474  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,477  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:05,491  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,525  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,526  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,526  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,526  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,546  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,547  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,547  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:37:05,548  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:37:05,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:05,570  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:05,597  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:05,597  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:05,597  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:05,597  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:05,597  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:05,598  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:05,598  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:05,599  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:05,614  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:37:05,643  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:37:05,796  INFO [main] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:37:05,845  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:37:05,867  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 8
2024-04-24T10:37:05,868  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 map-only splittees.
2024-04-24T10:37:05,868  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 out of total 6 MR operators.
2024-04-24T10:37:05,868  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 3
2024-04-24T10:37:05,973  INFO [main] beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2024-04-24T10:37:05,981  WARN [main] impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties
2024-04-24T10:37:05,994  INFO [main] impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2024-04-24T10:37:05,994  INFO [main] impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-24T10:37:06,013  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:37:06,017  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:37:06,023  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:37:06,030  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:37:06,030  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:37:06,030  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980226030-0
2024-04-24T10:37:06,056  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:37:06,056  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:37:06,058  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:37:06,058  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:37:06,058  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:37:06,058  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980226058-0
2024-04-24T10:37:06,068  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:37:06,069  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:37:06,070  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:37:06,071  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:37:06,071  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:37:06,071  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980226071-0
2024-04-24T10:37:06,085  INFO [main] mapReduceLayer.MapReduceLauncher: 3 map-reduce job(s) waiting for submission.
2024-04-24T10:37:06,090  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,131  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,132  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:06,133  INFO [JobControl] metastore.HMSHandler: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:06,133  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:06,134  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1eee454a, with PersistenceManager: null will be shutdown
2024-04-24T10:37:06,134  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1eee454a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@223d1906 created in the thread with id: 92
2024-04-24T10:37:06,140  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1eee454a from thread id: 92
2024-04-24T10:37:06,140  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:06,140  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:06,141  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,186  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:37:06,193  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:37:06,204  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:06,208  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:37:06,209  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:37:06,223  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:37:06,257  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:37:06,293  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local2125082259_0001
2024-04-24T10:37:06,293  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:37:06,405  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:37:06,406  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:06,406  INFO [Thread-83] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:37:06,433  INFO [Thread-83] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:37:06,445  INFO [Thread-83] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:37:06,445  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local2125082259_0001_m_000000_0
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,447  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,448  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,449  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,460  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:37:06,465  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:37:06,467  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:06,468  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:37:06,468  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:37:06,469  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:37:06,490  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:37:06,509  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local926914912_0002
2024-04-24T10:37:06,509  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:37:06,513  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:37:06,517  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 65
Input split[0]:
   Length = 65
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:37:06,526  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:06,530  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data:0+65
2024-04-24T10:37:06,548  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:37:06,549  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:37:06,555  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:37:06,569  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,569  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,580  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,580  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,585  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:06,586  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-14 15%3A10%3A15/_temporary/0/_temporary/attempt_local2125082259_0001_m_000000_0/part-m-00000
2024-04-24T10:37:06,588  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local2125082259_0001
2024-04-24T10:37:06,588  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases E,F
2024-04-24T10:37:06,588  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:37:06,589  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:06,590  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:06,595  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:37:06,596  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local926914912_0002
2024-04-24T10:37:06,596  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases D
2024-04-24T10:37:06,596  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:37:06,596  INFO [Thread-107] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:37:06,597  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:06,606  INFO [Thread-107] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,606  INFO [Thread-107] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,609  INFO [Thread-107] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:37:06,613  INFO [Thread-107] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,613  INFO [Thread-107] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:06,615  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:06,623  INFO [Thread-107] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:37:06,623  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local926914912_0002_m_000000_0
2024-04-24T10:37:06,632  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,632  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,632  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,632  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,633  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,634  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,635  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,639  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,639  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,644  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,644  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,646  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:37:06,647  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 342
Input split[0]:
   Length = 342
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:37:06,650  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:06,650  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data:0+342
2024-04-24T10:37:06,658  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,658  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,663  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,3093619691924844/_temporary/0/_temporary/attempt_local926914912_0002_m_000000_0/part-m-00000
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:06,664  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:06,665  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:06,667  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:06,667  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:06,667  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:06,673  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,673  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,673  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,673  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,673  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,674  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,674  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,676  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,714  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,715  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,715  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,715  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,716  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,752  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,752  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,752  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,753  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,753  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,754  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:06,790  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:06,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:06,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:06,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:06,791  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:06,792  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:06,804  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:37:06,810  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:37:06,812  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:06,814  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:37:06,814  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:37:06,814  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:37:06,839  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:37:06,873  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1041204478_0003
2024-04-24T10:37:06,873  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:37:06,930  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:37:06,930  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1041204478_0003
2024-04-24T10:37:06,930  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,B,B2,C,C2
2024-04-24T10:37:06,931  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:37:06,931  INFO [Thread-143] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:37:06,937  INFO [main] mapReduceLayer.MapReduceLauncher: 0% complete
2024-04-24T10:37:06,937  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local2125082259_0001,job_local926914912_0002,job_local1041204478_0003]
2024-04-24T10:37:06,943  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,943  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,950  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,950  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,955  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,955  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,961  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,961  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,968  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,968  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,970  INFO [Thread-143] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:37:06,973  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,973  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,982  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,982  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:06,992  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:06,992  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,002  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,002  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,014  INFO [Thread-143] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,014  INFO [Thread-143] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,024  INFO [Thread-143] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:37:07,025  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1041204478_0003_m_000000_0
2024-04-24T10:37:07,037  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,037  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,052  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,052  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,058  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,058  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,063  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,063  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,068  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,068  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,071  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,071  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,073  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,073  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,076  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,076  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,078  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,078  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,078  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:37:07,079  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 54
Input split[0]:
   Length = 54
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:37:07,081  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:37:07,081  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data:0+54
2024-04-24T10:37:07,084  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:37:07,084  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:37:07,087  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:37:07,094  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,094  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,1394730786070879/_temporary/0/_temporary/attempt_local1041204478_0003_m_000000_0/part-m-00000
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,096  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,151  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@6079d545
2024-04-24T10:37:07,152  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@53b5f1fc
2024-04-24T10:37:07,152  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@498fadff
2024-04-24T10:37:07,157  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:37:07,157  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:37:07,160  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:37:07,160  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,161  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,04377916598129872/_temporary/0/_temporary/attempt_local1041204478_0003_m_000000_0/part-m-00000
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,164  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,167  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,167  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,177  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,177  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,177  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@3bde907a
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-15 18%3A54%3A55/_temporary/0/_temporary/attempt_local2125082259_0001_m_000000_0/part-m-00000
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,178  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,179  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,179  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,179  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,179  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,182  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,185  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,185  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,23138424770921873/_temporary/0/_temporary/attempt_local1041204478_0003_m_000000_0/part-m-00000
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,188  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,191  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@79cbc7cd
2024-04-24T10:37:07,193  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,201  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@37e151c2
2024-04-24T10:37:07,208  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,208  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,3637125257578998/bkt=0/_temporary/0/_temporary/attempt_local1041204478_0003_m_000000_0/part-m-00000
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,211  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,212  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,223  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@6515f97e
2024-04-24T10:37:07,231  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,231  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,08288380696577524/bkt=1/_temporary/0/_temporary/attempt_local1041204478_0003_m_000000_0/part-m-00000
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:37:07,234  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:37:07,235  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:37:07,247  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@313f82ae
2024-04-24T10:37:07,601  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Registering committer for TaskAttemptID:attempt_local2125082259_0001_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:37:07,607  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local2125082259_0001_m_000000_0 is done. And is in the process of committing
2024-04-24T10:37:07,607  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local926914912_0002_m_000000_0 is done. And is in the process of committing
2024-04-24T10:37:07,609  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,609  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,610  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,610  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local2125082259_0001_m_000000_0 is allowed to commit now
2024-04-24T10:37:07,611  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,612  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local926914912_0002_m_000000_0 is allowed to commit now
2024-04-24T10:37:07,613  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Committing TaskAttempt:attempt_local2125082259_0001_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:37:07,613  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:37:07,614  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,614  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,615  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local2125082259_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:37:07,615  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:37:07,616  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local926914912_0002_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,3093619691924844
2024-04-24T10:37:07,618  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:37:07,618  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local926914912_0002_m_000000_0' done.
2024-04-24T10:37:07,618  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local2125082259_0001_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:37:07,619  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Discarding all cleanup for TaskAttemptID:attempt_local2125082259_0001_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.12888298985379743/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:37:07,619  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:37:07,619  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local2125082259_0001_m_000000_0' done.
2024-04-24T10:37:07,620  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local2125082259_0001_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=8251
		FILE: Number of bytes written=1862809
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=494
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=15
		Total committed heap usage (bytes)=928514048
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:37:07,620  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local926914912_0002_m_000000_0: Counters: 16
	File System Counters
		FILE: Number of bytes read=8251
		FILE: Number of bytes written=1862204
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=488
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=15
		Total committed heap usage (bytes)=928514048
	MultiStoreCounters
		Output records in _1_junit_specific_size=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:37:07,620  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local926914912_0002_m_000000_0
2024-04-24T10:37:07,620  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local2125082259_0001_m_000000_0
2024-04-24T10:37:07,621  INFO [Thread-83] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:37:07,621  INFO [Thread-107] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:37:07,630  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,630  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1041204478_0003_m_000000_0 is done. And is in the process of committing
2024-04-24T10:37:07,633  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,633  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,641  INFO [Thread-83] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,641  INFO [Thread-83] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,644  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:37:07,644  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1041204478_0003_m_000000_0 is allowed to commit now
2024-04-24T10:37:07,647  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,647  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,648  INFO [Thread-83] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,648  INFO [Thread-83] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,649  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1041204478_0003_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,1394730786070879
2024-04-24T10:37:07,652  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,652  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,654  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1041204478_0003_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,04377916598129872
2024-04-24T10:37:07,657  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,657  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,660  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1041204478_0003_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,23138424770921873
2024-04-24T10:37:07,662  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,662  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,664  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1041204478_0003_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,3637125257578998/bkt=0
2024-04-24T10:37:07,667  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:37:07,667  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:37:07,670  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1041204478_0003_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,08288380696577524/bkt=1
2024-04-24T10:37:07,671  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:37:07,671  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1041204478_0003_m_000000_0' done.
2024-04-24T10:37:07,671  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1041204478_0003_m_000000_0: Counters: 20
	File System Counters
		FILE: Number of bytes read=9739
		FILE: Number of bytes written=1863476
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=9
		Map output records=0
		Input split bytes=486
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=928514048
	MultiStoreCounters
		Output records in _0_junit_unparted_basic=9
		Output records in _1_junit_specific_size=9
		Output records in _2_junit_specific_db.junit_specific_size2=9
		Output records in _3_junit_parted_basic=3
		Output records in _4_junit_parted_basic=6
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:37:07,671  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1041204478_0003_m_000000_0
2024-04-24T10:37:07,671  INFO [Thread-143] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,679  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,680  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,680  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,680  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,680  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,681  INFO [Thread-107] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,683  INFO [Thread-107] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:07,684  INFO [Thread-107] metastore.HMSHandler: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:07,684  INFO [Thread-107] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:07,685  INFO [Thread-107] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dddd3d6, with PersistenceManager: null will be shutdown
2024-04-24T10:37:07,685  INFO [Thread-107] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dddd3d6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f066fd4 created in the thread with id: 137
2024-04-24T10:37:07,690  INFO [Thread-107] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5dddd3d6 from thread id: 137
2024-04-24T10:37:07,690  INFO [Thread-107] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:07,690  INFO [Thread-107] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:07,691  INFO [Thread-107] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,694  INFO [Thread-107] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_complex newtbl=junit_unparted_complex	
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,710  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,711  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,711  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,711  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,711  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,711  INFO [Thread-83] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,713  INFO [Thread-83] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:07,713  INFO [Thread-83] metastore.HMSHandler: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:07,714  INFO [Thread-83] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:07,714  INFO [Thread-83] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1ba31209, with PersistenceManager: null will be shutdown
2024-04-24T10:37:07,714  INFO [Thread-83] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1ba31209, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7a6a6ccf created in the thread with id: 111
2024-04-24T10:37:07,718  INFO [Thread-83] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1ba31209 from thread id: 111
2024-04-24T10:37:07,718  INFO [Thread-83] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:07,718  INFO [Thread-83] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:07,719  INFO [Thread-83] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,724  INFO [Thread-107] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_complex
2024-04-24T10:37:07,724  INFO [Thread-107] utils.MetaStoreServerUtils: Updated size of table junit_unparted_complex to 2660
2024-04-24T10:37:07,734  INFO [Thread-107] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,745  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,746  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,746  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,746  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,746  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,746  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,746  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,748  INFO [Thread-143] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:07,750  INFO [Thread-143] metastore.HMSHandler: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:07,750  INFO [Thread-143] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:07,750  INFO [Thread-143] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3946c3cf, with PersistenceManager: null will be shutdown
2024-04-24T10:37:07,750  INFO [Thread-143] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3946c3cf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@58ac09d created in the thread with id: 175
2024-04-24T10:37:07,755  INFO [Thread-143] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3946c3cf from thread id: 175
2024-04-24T10:37:07,755  INFO [Thread-143] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:07,755  INFO [Thread-143] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:07,755  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,759  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_basic newtbl=junit_unparted_basic	
2024-04-24T10:37:07,769  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,769  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,769  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,769  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,769  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,770  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,771  INFO [Thread-83] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_date has new partitions [{dt=2016-07-14 15:10:15}, {dt=2016-07-15 18:54:55}].
2024-04-24T10:37:07,784  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,784  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,785  WARN [Thread-107] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,786  INFO [Thread-107] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,786  INFO [Thread-107] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,790  INFO [Thread-107] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:07,792  INFO [Thread-143] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_basic
2024-04-24T10:37:07,793  INFO [Thread-143] utils.MetaStoreServerUtils: Updated size of table junit_unparted_basic to 647
2024-04-24T10:37:07,827  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:07,839  INFO [Thread-83] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:37:07,839  INFO [Thread-83] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:37:07,869  INFO [HMSHandler #1] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:37:07,869  INFO [HMSHandler #0] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:37:07,869  INFO [HMSHandler #1] utils.MetaStoreServerUtils: Updated size to 469
2024-04-24T10:37:07,870  INFO [HMSHandler #0] utils.MetaStoreServerUtils: Updated size to 476
2024-04-24T10:37:07,895  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,896  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,897  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,897  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,900  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:07,938  INFO [main] mapReduceLayer.MapReduceLauncher: 66% complete
2024-04-24T10:37:07,938  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local2125082259_0001,job_local1041204478_0003]
2024-04-24T10:37:07,941  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:07,946  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:07,950  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:07,969  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:07,970  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:07,970  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:07,970  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:07,970  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:07,971  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:07,971  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:07,975  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_specific_size newtbl=junit_specific_size	
2024-04-24T10:37:08,002  INFO [Thread-143] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size
2024-04-24T10:37:08,002  INFO [Thread-143] utils.MetaStoreServerUtils: Updated size of table junit_specific_size to 647
2024-04-24T10:37:08,012  INFO [Thread-83] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:08,014  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:08,067  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,067  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,067  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,067  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,067  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,067  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,067  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,067  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,067  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,067  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,068  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,068  WARN [Thread-83] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,068  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,068  INFO [Thread-83] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,069  INFO [Thread-83] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,069  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,071  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:08,071  INFO [Thread-83] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,114  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,115  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,115  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,119  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.junit_specific_db.junit_specific_size2 newtbl=junit_specific_size2	
2024-04-24T10:37:08,143  INFO [Thread-143] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size2
2024-04-24T10:37:08,143  INFO [Thread-143] utils.MetaStoreServerUtils: Updated size of table junit_specific_size2 to 647
2024-04-24T10:37:08,153  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,190  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,191  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,191  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,193  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,235  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,235  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,236  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,272  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,272  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,272  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,273  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,273  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=0}].
2024-04-24T10:37:08,275  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[0]	
2024-04-24T10:37:08,280  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:37:08,280  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:37:08,291  INFO [HMSHandler #2] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:37:08,291  INFO [HMSHandler #2] utils.MetaStoreServerUtils: Updated size to 597
2024-04-24T10:37:08,324  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,358  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,358  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,359  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,361  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,410  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,410  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,411  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,416  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=1}].
2024-04-24T10:37:08,418  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[1]	
2024-04-24T10:37:08,425  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:37:08,425  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:37:08,436  INFO [HMSHandler #3] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:37:08,436  INFO [HMSHandler #3] utils.MetaStoreServerUtils: Updated size to 635
2024-04-24T10:37:08,466  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:37:08,482  INFO [main] mapReduceLayer.MapReduceLauncher: 83% complete
2024-04-24T10:37:08,482  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1041204478_0003]
2024-04-24T10:37:08,484  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,486  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,488  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,504  WARN [Thread-143] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,505  INFO [Thread-143] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:37:08,505  INFO [Thread-143] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,508  INFO [Thread-143] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:37:08,643  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,646  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,648  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,653  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:37:08,653  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_unparted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:08,654  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:37:08,654  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_size does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:08,655  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:37:08,655  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_db.junit_specific_size2 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:08,656  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:37:08,656  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:08,656  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:37:08,656  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:08,657  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:37:08,659  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:37:06	2024-04-24 10:37:08	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1041204478_0003	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,B,B2,C,C2	MULTI_QUERY,MAP_ONLY	junit_unparted_basic,junit_specific_size,junit_specific_db.junit_specific_size2,junit_parted_basic,junit_parted_basic,
job_local2125082259_0001	1	0	n/a	n/a	n/a	n/a	0	0	0	0	E,F	MAP_ONLY	junit_parted_date,
job_local926914912_0002	1	0	n/a	n/a	n/a	n/a	0	0	0	0	D	MAP_ONLY	junit_unparted_complex,

Input(s):
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data"
Successfully read 9 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data"
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data"

Output(s):
Successfully stored 2 records in: "junit_unparted_complex"
Successfully stored 9 records in: "junit_unparted_basic"
Successfully stored 9 records in: "junit_specific_size"
Successfully stored 9 records in: "junit_specific_db.junit_specific_size2"
Successfully stored 3 records in: "junit_parted_basic"
Successfully stored 6 records in: "junit_parted_basic"
Successfully stored 2 records in: "junit_parted_date"

Counters:
Total records written : 40
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local926914912_0002
job_local1041204478_0003
job_local2125082259_0001


2024-04-24T10:37:08,661  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,664  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,667  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,669  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,674  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,676  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:37:08,677  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:37:08,699  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:37:08,700  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-88f80a0e-6ba7-4d63-9377-1b5134cf8ce6
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,723  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,724  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,727  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:08,739  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:08,754  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:08,755  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:37:08,757  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:08,770  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
K: {name: chararray,studentid: int,contact: (phno: chararray,email: chararray),currently_registered_courses: {innertuple: (innerfield: chararray)},current_grades: map[],phnos: {innertuple: (phno: chararray,type: chararray)}}
2024-04-24T10:37:08,772  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_basic
2024-04-24T10:37:08,775  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:08,789  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:08,789  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:08,789  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:08,789  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:08,789  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=14, isCompatibleWith_(Configuration)=0, flushCache_()=1}
2024-04-24T10:37:08,790  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.017 seconds
2024-04-24T10:37:08,790  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:08,790  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:08,790  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_basic
2024-04-24T10:37:08,790  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:08,791  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:08,803  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:08,804  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:08,816  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:08,816  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:09,050  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,050  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=246}
2024-04-24T10:37:09,050  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.26 seconds
2024-04-24T10:37:09,051  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_complex
2024-04-24T10:37:09,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:09,099  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,100  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,100  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,100  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,100  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=47}
2024-04-24T10:37:09,100  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.049 seconds
2024-04-24T10:37:09,100  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,100  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,100  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_complex
2024-04-24T10:37:09,101  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,101  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:09,114  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,114  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:09,129  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:09,190  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,190  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=76, getTable_(GetTableRequest)=13}
2024-04-24T10:37:09,190  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.09 seconds
2024-04-24T10:37:09,191  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_parted_basic
2024-04-24T10:37:09,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:09,203  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,204  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,204  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,204  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,204  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=13}
2024-04-24T10:37:09,204  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.013 seconds
2024-04-24T10:37:09,204  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,204  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,204  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_parted_basic
2024-04-24T10:37:09,205  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,205  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:09,218  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:09,232  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,232  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:09,343  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,343  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=13, dropTable_(String, String, boolean, boolean, boolean)=123, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:09,343  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.139 seconds
2024-04-24T10:37:09,343  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_specific_size
2024-04-24T10:37:09,344  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:09,356  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,356  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,356  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,356  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,356  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=12, flushCache_()=0}
2024-04-24T10:37:09,356  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.013 seconds
2024-04-24T10:37:09,357  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,357  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,357  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_specific_size
2024-04-24T10:37:09,357  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,357  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:09,369  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,370  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:09,382  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:09,431  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,431  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=61, getTable_(GetTableRequest)=13}
2024-04-24T10:37:09,431  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.074 seconds
2024-04-24T10:37:09,432  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_parted_date
2024-04-24T10:37:09,433  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:09,445  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,445  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,445  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,445  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,445  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=12, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:09,446  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.013 seconds
2024-04-24T10:37:09,446  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,446  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,446  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_parted_date
2024-04-24T10:37:09,446  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,447  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:09,458  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:09,469  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,469  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:09,567  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,567  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, dropTable_(String, String, boolean, boolean, boolean)=108, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:09,567  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.12 seconds
2024-04-24T10:37:09,567  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:09,568  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:09,583  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,583  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,584  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,584  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,584  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=15, flushCache_()=0}
2024-04-24T10:37:09,584  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.017 seconds
2024-04-24T10:37:09,584  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,584  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,584  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:09,584  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,585  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:09,598  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,599  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:09,613  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:09,613  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:09,663  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:09,664  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=13, dropTable_(String, String, boolean, boolean, boolean)=65}
2024-04-24T10:37:09,664  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.079 seconds
2024-04-24T10:37:09,664  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop database if exists junit_specific_db cascade
2024-04-24T10:37:09,665  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:09,672  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:37:09,684  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:37:09,685  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:09,685  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:09,686  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:09,686  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=3, getTables_(String, String)=14, getTableObjectsByName_(String, List)=2}
2024-04-24T10:37:09,686  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 0.022 seconds
2024-04-24T10:37:09,686  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:09,686  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:09,686  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85): drop database if exists junit_specific_db cascade
2024-04-24T10:37:09,686  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:09,687  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:09,689  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:09,689  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:09,697  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:09,704  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:37:09,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:09,715  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:09,716  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:09,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:09,768  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:09,803  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:09,821  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:09,871  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:09,872  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:09,882 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:11,884  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:11,891  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:11,896  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:11,902  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:11,907  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:11,912  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:11,925  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:11,934  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:11,934  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:11,936 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:13,938  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:13,942  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:13,943  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:13,946  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:13,947  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:13,949  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:13,955  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:13,961  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:13,961  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:13,964 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:15,964  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:15,970  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:15,971  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:15,974  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:15,976  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:15,978  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:15,986  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:15,994  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:15,994  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:15,996 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:17,998  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:18,002  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:18,003  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:18,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:18,007  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:18,009  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:18,014  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:18,019  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:18,019  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:18,020 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:20,021  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:20,027  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:20,028  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:20,030  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:20,032  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:20,033  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:20,038  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:20,044  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:20,044  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:20,046 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:22,047  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:22,053  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:22,054  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:22,056  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:22,058  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:22,061  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:22,068  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:22,075  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:22,075  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:22,077 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:24,078  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:24,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:24,083  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:24,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:24,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:24,089  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:24,094  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:24,099  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:24,099  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:24,101 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:26,102  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:26,106  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:26,107  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:26,109  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:26,111  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:26,112  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:26,118  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:26,123  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:26,123  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:26,125 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:28,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:28,131  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:28,132  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:28,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:28,135  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:28,137  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:28,141  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:28,147  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:28,147  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:28,149 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:30,150  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:30,155  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:30,156  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:30,159  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:30,161  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:30,163  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:30,169  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:30,174  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:30,174  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:30,176 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:30,189 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:37:30,192 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:37:30,196  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:37:30,196  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:37:30,197  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:37:30,197 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:37:30,197  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,197  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:30,197  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103655_7e60febd-1aa2-46af-aa10-14dcd4de3b85); Time taken: 20.511 seconds
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:30,241  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:30,243  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:30,243  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:30,244  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@ef85567 will be shutdown
2024-04-24T10:37:30,244  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@892f1fa created in the thread with id: 1
2024-04-24T10:37:30,272  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 61bea02e-28a4-4a18-a496-d016eea880db
2024-04-24T10:37:30,272  INFO [main] SessionState: Hive Session ID = 61bea02e-28a4-4a18-a496-d016eea880db
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:30,272  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:30,278  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/61bea02e-28a4-4a18-a496-d016eea880db
2024-04-24T10:37:30,280  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/61bea02e-28a4-4a18-a496-d016eea880db
2024-04-24T10:37:30,282  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/61bea02e-28a4-4a18-a496-d016eea880db/_tmp_space.db
2024-04-24T10:37:30,283  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,284  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,285  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,285  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,285  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,285  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,285  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.002 seconds
2024-04-24T10:37:30,285  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,285  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,285  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,286  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,286  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,287  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,288  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,288  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:37:30,289  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.003 seconds
2024-04-24T10:37:30,289  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,290  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,290  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:30,290  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=61bea02e-28a4-4a18-a496-d016eea880db, clientType=HIVECLI]
2024-04-24T10:37:30,290  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:37:30,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:30,291  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e4894d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@892f1fa will be shutdown
2024-04-24T10:37:30,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:30,291  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -4
2024-04-24T10:37:30,292  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:30,292  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:30,292  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:30,293  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c, with PersistenceManager: null will be shutdown
2024-04-24T10:37:30,293  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d45b457 created in the thread with id: 1
2024-04-24T10:37:30,295  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c from thread id: 1
2024-04-24T10:37:30,295  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:30,295  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:30,295  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:37:30,296  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:30,296  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:30,297  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d45b457 will be shutdown
2024-04-24T10:37:30,297  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6984f7d1 created in the thread with id: 1
2024-04-24T10:37:30,299  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:30,299  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:30,300  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,301  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,301  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,301  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,301  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,301  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:37:30,301  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.012 seconds
2024-04-24T10:37:30,301  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,301  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,301  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,302  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,302  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:37:30,302  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:30,302  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@ba4c99c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6984f7d1 will be shutdown
2024-04-24T10:37:30,302  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:30,302  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -5
2024-04-24T10:37:30,304  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:30,305  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:30,305  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:30,305  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe, with PersistenceManager: null will be shutdown
2024-04-24T10:37:30,305  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d0ca8a5 created in the thread with id: 1
2024-04-24T10:37:30,308  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe from thread id: 1
2024-04-24T10:37:30,308  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:30,308  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:30,308  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980250, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0, bucketing_version=2, rawDataSize=0, numFiles=0, totalSize=0, numRows=0, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:30,317  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:37:30,368  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,368  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=60}
2024-04-24T10:37:30,368  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.067 seconds
2024-04-24T10:37:30,369  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,369  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,371  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,371  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,372  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,372  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:37:30,372  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.003 seconds
2024-04-24T10:37:30,372  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,372  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,372  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,372  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,373  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,375  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,377  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,377  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=4, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,378  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,378  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,380  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,380  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:30,380  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:37:30,380  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,382  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,382  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,382  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,382  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,383  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:37:30,383  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,383  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,383  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,383  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,383  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,385  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980250, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{transactional=false, rawDataSize=0, numFilesErasureCoded=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, totalSize=0, numFiles=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:30,392  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:37:30,418  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,418  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=33}
2024-04-24T10:37:30,418  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.035 seconds
2024-04-24T10:37:30,419  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,420  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,422  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,422  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,422  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,422  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:37:30,422  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.003 seconds
2024-04-24T10:37:30,423  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,423  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,423  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,423  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,423  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,425  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,428  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,428  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=4, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,428  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,428  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,429  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,429  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:30,429  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:37:30,429  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,432  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,432  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,432  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,432  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,432  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:37:30,432  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,433  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,433  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,433  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,433  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,434  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980250, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:30,441  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:37:30,461  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,461  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=27}
2024-04-24T10:37:30,461  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.028 seconds
2024-04-24T10:37:30,462  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,462  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,464  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,464  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,464  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,464  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:30,465  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.002 seconds
2024-04-24T10:37:30,465  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,465  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,465  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use default
2024-04-24T10:37:30,465  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,465  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,468  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,470  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,470  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=4, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,470  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,471  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,471  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,471  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:30,472  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:37:30,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:30,474  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,474  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,474  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,474  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,474  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:37:30,474  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.003 seconds
2024-04-24T10:37:30,475  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,475  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,475  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,475  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,476  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980250, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, numFiles=0, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0, numRows=0, transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:30,483  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:37:30,504  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,505  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=28, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,505  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.029 seconds
2024-04-24T10:37:30,505  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create database if not exists junit_specific_db
2024-04-24T10:37:30,506  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,506  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,506  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,506  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:30,507  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.001 seconds
2024-04-24T10:37:30,507  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,507  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,507  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create database if not exists junit_specific_db
2024-04-24T10:37:30,507  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,507  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:37:30,508  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:37:30,512 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Database junit_specific_db already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:1151)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:605)
	at org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.execute(CreateDatabaseOperation.java:68)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:129)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:37:30,513  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,513  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,513  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.006 seconds
2024-04-24T10:37:30,514  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use junit_specific_db
2024-04-24T10:37:30,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,516  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,516  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,516  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,517  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:37:30,517  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.002 seconds
2024-04-24T10:37:30,517  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,517  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,517  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): use junit_specific_db
2024-04-24T10:37:30,517  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,518  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,520  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,521  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,522  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:37:30,522  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,522  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,523  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,523  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:30,523  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:37:30,523  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,525  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5
2024-04-24T10:37:30,525  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,525  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,525  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,526  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:30,526  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.003 seconds
2024-04-24T10:37:30,526  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,526  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,526  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:30,526  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,527  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980250, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFiles=0, transactional=false, totalSize=0, numRows=0, rawDataSize=0, numFilesErasureCoded=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:30,576  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:30,577 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:37:30,577 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:37:30,578 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:37:30,579  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:37:30,579  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
2024-04-24T10:37:30,579  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:37:30,579 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:37:30,579  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,579  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,579  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.053 seconds
2024-04-24T10:37:30,580  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_basic
2024-04-24T10:37:30,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:37:30,584  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,584  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,584  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,585  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:30,585  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,585  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,585  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,585  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_basic
2024-04-24T10:37:30,585  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,586  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:37:30,589  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,590  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,590  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,590  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_complex
2024-04-24T10:37:30,591  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:37:30,595  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,595  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,595  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,595  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,595  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,595  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,595  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,595  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_complex
2024-04-24T10:37:30,596  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,596  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:37:30,600  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,601  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,601  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,601  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_parted_basic
2024-04-24T10:37:30,602  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:37:30,605  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,606  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,606  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,606  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,606  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,606  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,606  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,606  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_parted_basic
2024-04-24T10:37:30,607  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,607  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:37:30,610  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,611  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,611  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,611  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_specific_size
2024-04-24T10:37:30,612  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:37:30,616  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,616  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,616  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,616  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,616  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,617  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,617  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,617  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_specific_size
2024-04-24T10:37:30,617  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,617  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:37:30,621  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,621  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,621  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,622  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_parted_date
2024-04-24T10:37:30,623  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:37:30,626  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,626  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,626  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,627  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:30,627  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,627  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,627  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,627  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_parted_date
2024-04-24T10:37:30,627  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,628  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:37:30,631  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,632  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-24T10:37:30,632  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,632  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:30,633  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,637  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,637  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.005 seconds
2024-04-24T10:37:30,637  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:30,637  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,638  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:37:30,641  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:30,641  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:30,642  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.004 seconds
2024-04-24T10:37:30,642  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop database if exists junit_specific_db cascade
2024-04-24T10:37:30,643  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,646  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:37:30,655  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:37:30,655  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:30,655  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:30,655  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:30,655  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=1, getTables_(String, String)=9, getTableObjectsByName_(String, List)=2, flushCache_()=0}
2024-04-24T10:37:30,655  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 0.013 seconds
2024-04-24T10:37:30,655  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:30,655  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:30,655  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5): drop database if exists junit_specific_db cascade
2024-04-24T10:37:30,656  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:30,656  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:30,658  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:30,658  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:30,661  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:30,664  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:37:30,668  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:30,670  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:30,671  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:30,673  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:30,675  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:30,677  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:30,682  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:30,687  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:30,687  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:30,689 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:32,690  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:32,696  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:32,697  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:32,699  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:32,700  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:32,702  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:32,706  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:32,712  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:32,712  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:32,714 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:34,715  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:34,719  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:34,720  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:34,723  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:34,724  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:34,726  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:34,731  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:34,736  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:34,737  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:34,738 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:36,739  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:36,741  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:36,743  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:36,746  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:36,748  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:36,750  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:36,757  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:36,764  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:36,764  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:36,767 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:38,770  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:38,777  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:38,781  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:38,790  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:38,797  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:38,802  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:38,812  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:38,818  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:38,818  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:38,821 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 95 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 97 more

2024-04-24T10:37:40,822  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:40,829  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:40,833  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:40,836  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:40,837  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:40,839  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:40,845  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:40,856  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:40,856  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:40,858 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:42,859  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:42,865  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:42,865  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:42,868  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:42,869  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:42,871  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:42,876  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:42,884  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:42,885  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:42,886 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:44,887  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:44,893  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:44,894  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:44,897  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:44,898  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:44,899  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:44,904  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:44,926  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:44,926  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:44,927 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:46,928  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:46,932  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:46,932  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:46,934  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:46,936  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:46,937  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:46,942  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:46,950  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:46,950  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:46,952 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:48,953  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:48,958  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:48,959  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:48,961  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:48,962  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:48,964  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:48,968  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:48,976  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:48,976  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:48,977 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:50,978  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:50,981  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:50,982  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:50,984  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:50,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:50,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:50,991  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:50,999  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:50,999  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:51,001 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:51,001 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:37:51,005 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:37:51,009  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:37:51,009  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:37:51,009  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:37:51,009 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:37:51,009  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,009  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,009  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103730_6970a897-7875-475e-aa74-319769fe6db5); Time taken: 20.354 seconds
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:37:51,045  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:37:51,046  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:37:51,046  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:37:51,046  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:37:51,049  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:51,049  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:51,049  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d0ca8a5 will be shutdown
2024-04-24T10:37:51,050  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46984524 created in the thread with id: 1
2024-04-24T10:37:51,052  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = c9814e7e-1f9d-493a-9f01-b76675e11235
2024-04-24T10:37:51,052  INFO [main] SessionState: Hive Session ID = c9814e7e-1f9d-493a-9f01-b76675e11235
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:51,053  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:37:51,058  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c9814e7e-1f9d-493a-9f01-b76675e11235
2024-04-24T10:37:51,061  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/c9814e7e-1f9d-493a-9f01-b76675e11235
2024-04-24T10:37:51,063  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c9814e7e-1f9d-493a-9f01-b76675e11235/_tmp_space.db
2024-04-24T10:37:51,064  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): use default
2024-04-24T10:37:51,064  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:51,066  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,066  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,066  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,066  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,066  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.002 seconds
2024-04-24T10:37:51,066  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,067  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,067  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): use default
2024-04-24T10:37:51,067  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,067  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:51,069  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:51,070  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,070  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:37:51,070  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.003 seconds
2024-04-24T10:37:51,071  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:51,071  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540
2024-04-24T10:37:51,071  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:37:51,072  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c9814e7e-1f9d-493a-9f01-b76675e11235, clientType=HIVECLI]
2024-04-24T10:37:51,072  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:37:51,072  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:51,072  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@33fefbbe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46984524 will be shutdown
2024-04-24T10:37:51,072  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:51,072  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -6
2024-04-24T10:37:51,073  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:51,075  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:51,075  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:51,075  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db, with PersistenceManager: null will be shutdown
2024-04-24T10:37:51,075  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@59294447 created in the thread with id: 1
2024-04-24T10:37:51,077  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db from thread id: 1
2024-04-24T10:37:51,078  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:51,078  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:51,078  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:37:51,079  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:51,079  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:51,080  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@59294447 will be shutdown
2024-04-24T10:37:51,080  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2de22e7 created in the thread with id: 1
2024-04-24T10:37:51,082  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:51,082  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:51,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:37:51,084  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,084  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,084  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.013 seconds
2024-04-24T10:37:51,084  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:37:51,084  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,084  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:37:51,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:37:51,085  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@34e785db, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2de22e7 will be shutdown
2024-04-24T10:37:51,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:37:51,085  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -7
2024-04-24T10:37:51,087  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:37:51,088  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:37:51,088  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:37:51,088  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b, with PersistenceManager: null will be shutdown
2024-04-24T10:37:51,089  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f3d33bc created in the thread with id: 1
2024-04-24T10:37:51,091  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b from thread id: 1
2024-04-24T10:37:51,091  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:37:51,091  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:37:51,091  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980271, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=0, transactional=false, rawDataSize=0, totalSize=0, bucketing_version=2, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:37:51,103  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:37:51,104 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:37:51,104 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:37:51,104 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:37:51,105  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:37:51,105  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
2024-04-24T10:37:51,105  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:37:51,105 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:37:51,105  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,105  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:37:51,105  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.021 seconds
2024-04-24T10:37:51,106  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_basic
2024-04-24T10:37:51,106  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:51,117  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,117  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,117  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,117  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,117  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=11}
2024-04-24T10:37:51,118  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.011 seconds
2024-04-24T10:37:51,118  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,118  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,118  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_basic
2024-04-24T10:37:51,118  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,118  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:51,130  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,130  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:51,141  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,141  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:37:51,273  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic does not exist; Force to delete it.
2024-04-24T10:37:51,273 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:37:51,273  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,273  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=143, getTable_(GetTableRequest)=12}
2024-04-24T10:37:51,273  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.155 seconds
2024-04-24T10:37:51,274  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_complex
2024-04-24T10:37:51,274  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:51,285  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,286  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,286  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.012 seconds
2024-04-24T10:37:51,286  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_complex
2024-04-24T10:37:51,286  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,287  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:51,298  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,298  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:51,310  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,310  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:37:51,355  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex does not exist; Force to delete it.
2024-04-24T10:37:51,355 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:37:51,356  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,356  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, dropTable_(String, String, boolean, boolean, boolean)=57, getTable_(GetTableRequest)=11}
2024-04-24T10:37:51,356  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.07 seconds
2024-04-24T10:37:51,356  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_parted_basic
2024-04-24T10:37:51,357  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:51,369  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,369  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,369  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,369  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,370  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=12}
2024-04-24T10:37:51,370  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.013 seconds
2024-04-24T10:37:51,370  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,370  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,370  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_parted_basic
2024-04-24T10:37:51,370  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,370  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:51,382  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:51,394  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,394  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:37:51,434  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic does not exist; Force to delete it.
2024-04-24T10:37:51,434 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:37:51,434  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,434  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, dropTable_(String, String, boolean, boolean, boolean)=52, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,435  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.064 seconds
2024-04-24T10:37:51,435  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_specific_size
2024-04-24T10:37:51,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:51,447  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,448  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,448  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.013 seconds
2024-04-24T10:37:51,448  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_specific_size
2024-04-24T10:37:51,448  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,449  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:51,464  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,464  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:51,474  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:37:51,474  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:37:51,512  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size does not exist; Force to delete it.
2024-04-24T10:37:51,512 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:37:51,512  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,513  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=48, getTable_(GetTableRequest)=15}
2024-04-24T10:37:51,513  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.064 seconds
2024-04-24T10:37:51,513  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_parted_date
2024-04-24T10:37:51,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:51,517  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,517  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,517  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,518  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:37:51,518  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.004 seconds
2024-04-24T10:37:51,518  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,518  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,518  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_parted_date
2024-04-24T10:37:51,518  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,519  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:37:51,522  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,522  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,522  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.004 seconds
2024-04-24T10:37:51,523  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:51,523  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:51,526  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,526  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,526  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,526  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,526  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.004 seconds
2024-04-24T10:37:51,527  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,527  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,527  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop table if exists junit_unparted_alltypes
2024-04-24T10:37:51,527  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,527  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:37:51,530  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:37:51,530  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:37:51,530  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.003 seconds
2024-04-24T10:37:51,531  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop database if exists junit_specific_db cascade
2024-04-24T10:37:51,531  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:51,534  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:37:51,539  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:37:51,539  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:37:51,539  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:37:51,539  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:37:51,540  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0, getTables_(String, String)=5, getTableObjectsByName_(String, List)=1}
2024-04-24T10:37:51,540  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 0.008 seconds
2024-04-24T10:37:51,540  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:37:51,540  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:37:51,540  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540): drop database if exists junit_specific_db cascade
2024-04-24T10:37:51,540  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:37:51,541  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:37:51,542  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:51,542  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:51,545  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:51,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:37:51,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:51,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:51,553  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:51,555  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:51,556  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:51,558  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:51,562  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:51,569  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:51,569  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:51,571 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:53,572  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:53,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:53,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:53,578  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:53,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:53,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:53,586  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:53,593  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:53,593  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:53,595 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:55,595  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:55,599  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:55,600  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:55,602  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:55,604  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:55,605  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:55,610  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:55,618  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:55,618  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:55,620 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:57,621  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:57,626  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:57,627  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:57,629  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:57,631  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:57,632  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:57,637  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:57,645  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:57,645  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:57,647 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:37:59,648  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:37:59,652  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:37:59,653  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:37:59,655  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:37:59,657  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:37:59,658  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:37:59,664  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:37:59,672  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:37:59,672  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:37:59,673 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:00,594  INFO [Metastore Scheduled Worker 0] metastore.HMSHandler: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:00,595  INFO [Metastore Scheduled Worker 0] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@df8871c, with PersistenceManager: null will be shutdown
2024-04-24T10:38:00,596  INFO [Metastore Scheduled Worker 0] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@df8871c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@34ca4295 created in the thread with id: 26
2024-04-24T10:38:00,599  INFO [Metastore Scheduled Worker 0] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@df8871c from thread id: 26
2024-04-24T10:38:01,675  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:01,679  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:01,680  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:01,683  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:01,685  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:01,686  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:01,691  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:01,700  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:01,700  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:01,702 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:03,703  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:03,708  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:03,709  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:03,712  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:03,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:03,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:03,719  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:03,726  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:03,726  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:03,727 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:05,728  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:05,733  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:05,734  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:05,736  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:05,737  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:05,739  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:05,744  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:05,751  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:05,751  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:05,752 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:07,753  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:07,756  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:07,757  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:07,759  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:07,760  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:07,762  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:07,766  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:07,773  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:07,773  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:07,775 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:09,776  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:09,779  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:09,780  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:09,783  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:09,784  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:09,786  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:09,791  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:09,798  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:09,798  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:09,799 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:11,800  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:11,803  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:11,804  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:11,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:11,808  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:11,809  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:11,814  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:11,822  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:11,822  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:11,824 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:11,824 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:11,827 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:11,831  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:11,831  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:38:11,831  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:11,831 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:11,831  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:11,831  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:11,831  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103751_37ec4587-4aa3-456b-a0da-be4530027540); Time taken: 20.291 seconds
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:11,864  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:11,865  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:11,865  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:11,865  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:11,865  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:11,867  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:11,867  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:11,867  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f3d33bc will be shutdown
2024-04-24T10:38:11,868  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@271ced5f created in the thread with id: 1
2024-04-24T10:38:11,870  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 223ee949-8b71-4b15-8f4b-ea5817cf6cac
2024-04-24T10:38:11,871  INFO [main] SessionState: Hive Session ID = 223ee949-8b71-4b15-8f4b-ea5817cf6cac
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:11,871  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:11,876  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/223ee949-8b71-4b15-8f4b-ea5817cf6cac
2024-04-24T10:38:11,879  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/223ee949-8b71-4b15-8f4b-ea5817cf6cac
2024-04-24T10:38:11,881  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/223ee949-8b71-4b15-8f4b-ea5817cf6cac/_tmp_space.db
2024-04-24T10:38:11,882  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:11,882  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,884  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:11,884  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:11,884  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:11,884  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:11,885  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.002 seconds
2024-04-24T10:38:11,885  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:11,885  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:11,885  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:11,885  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:11,886  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,887  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,889  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:11,889  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getDatabase_(String)=2}
2024-04-24T10:38:11,889  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:11,890  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:11,890  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:11,890  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:11,890  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=223ee949-8b71-4b15-8f4b-ea5817cf6cac, clientType=HIVECLI]
2024-04-24T10:38:11,890  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:38:11,891  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:11,891  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8b814b, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@271ced5f will be shutdown
2024-04-24T10:38:11,891  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:11,891  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -8
2024-04-24T10:38:11,892  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:11,893  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:11,893  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:11,893  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917, with PersistenceManager: null will be shutdown
2024-04-24T10:38:11,893  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b80be13 created in the thread with id: 1
2024-04-24T10:38:11,896  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917 from thread id: 1
2024-04-24T10:38:11,896  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:11,896  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:11,896  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:38:11,897  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:11,897  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:11,897  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b80be13 will be shutdown
2024-04-24T10:38:11,898  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@24a29d8 created in the thread with id: 1
2024-04-24T10:38:11,900  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:11,900  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:11,900  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,901  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:11,901  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:11,901  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:11,901  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:11,901  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:11,901  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.011 seconds
2024-04-24T10:38:11,902  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:11,902  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:11,902  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:11,902  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:11,902  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:38:11,902  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:11,902  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3776c917, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@24a29d8 will be shutdown
2024-04-24T10:38:11,902  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:11,903  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -9
2024-04-24T10:38:11,904  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:11,905  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:11,905  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:11,905  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e, with PersistenceManager: null will be shutdown
2024-04-24T10:38:11,905  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71c1ba6d created in the thread with id: 1
2024-04-24T10:38:11,907  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e from thread id: 1
2024-04-24T10:38:11,907  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:11,907  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:11,907  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980291, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=0, rawDataSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, bucketing_version=2, transactional=false, totalSize=0, numFilesErasureCoded=0, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:11,912  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:38:11,961  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:11,961  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=54}
2024-04-24T10:38:11,962  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.059 seconds
2024-04-24T10:38:11,962  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:11,963  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,965  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:11,965  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:11,965  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:11,965  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:11,965  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:11,965  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:11,965  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:11,965  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:11,966  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:11,966  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,970  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:11,970  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:11,970  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.005 seconds
2024-04-24T10:38:11,970  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:11,971  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:11,971  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:11,972  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:38:11,972  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:11,974  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:11,974  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:11,974  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:11,974  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:11,974  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:38:11,974  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:11,974  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:11,974  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:11,974  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:11,975  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:11,976  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980291, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, rawDataSize=0, numFilesErasureCoded=0, transactional=false, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, numFiles=0, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:11,982  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:38:12,006  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,006  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=31}
2024-04-24T10:38:12,006  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.032 seconds
2024-04-24T10:38:12,007  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:12,007  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,009  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,009  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,009  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,009  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,009  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.002 seconds
2024-04-24T10:38:12,010  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,010  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,010  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:12,010  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,010  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,012  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,014  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,014  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,014  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,014  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,015  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,015  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:12,015  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:38:12,015  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,017  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,017  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,017  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,017  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,017  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:12,017  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,018  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,018  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,018  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,018  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,019  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980292, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:12,025  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:38:12,043  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,043  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=24, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,043  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.025 seconds
2024-04-24T10:38:12,044  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:12,044  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,046  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,046  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,046  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,046  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:38:12,046  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.002 seconds
2024-04-24T10:38:12,047  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,047  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,047  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use default
2024-04-24T10:38:12,047  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,047  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,049  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,050  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,050  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,051  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,051  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,052  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,052  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:12,052  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:38:12,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:12,053  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,053  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,054  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,054  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,054  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=1, getDatabase_(String)=1}
2024-04-24T10:38:12,054  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,054  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,054  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,054  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,054  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980292, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{transactional=false, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, rawDataSize=0, bucketing_version=2, numRows=0, numFilesErasureCoded=0, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:12,061  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:38:12,079  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,079  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, createTable_(Table)=24}
2024-04-24T10:38:12,079  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.025 seconds
2024-04-24T10:38:12,080  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create database if not exists junit_specific_db
2024-04-24T10:38:12,080  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,080  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,080  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,080  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,081  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.0 seconds
2024-04-24T10:38:12,081  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,081  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,081  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create database if not exists junit_specific_db
2024-04-24T10:38:12,081  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,081  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:38:12,081  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:38:12,083 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Database junit_specific_db already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:1151)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:605)
	at org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.execute(CreateDatabaseOperation.java:68)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:129)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:12,083  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,083  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,084  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.002 seconds
2024-04-24T10:38:12,084  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use junit_specific_db
2024-04-24T10:38:12,084  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,086  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,086  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,086  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,086  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:12,086  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.002 seconds
2024-04-24T10:38:12,087  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,087  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,087  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): use junit_specific_db
2024-04-24T10:38:12,087  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,089  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,091  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,091  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,091  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,092  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,092  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,092  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:12,092  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:38:12,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,094  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad
2024-04-24T10:38:12,094  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,094  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,094  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,094  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,095  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,095  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,095  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,095  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:12,095  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,096  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980292, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{transactional=false, numRows=0, bucketing_version=2, rawDataSize=0, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:12,108  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:12,111 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:12,111 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:12,113 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:12,116  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:12,116  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
2024-04-24T10:38:12,116  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:38:12,116 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:38:12,116  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,116  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,117  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.021 seconds
2024-04-24T10:38:12,117  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_basic
2024-04-24T10:38:12,118  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:38:12,121  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,121  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,121  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,121  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:12,121  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,121  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,122  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,122  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_basic
2024-04-24T10:38:12,122  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,122  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:38:12,125  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,125  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,125  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,126  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_complex
2024-04-24T10:38:12,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:38:12,129  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,129  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,129  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,130  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,130  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,130  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,130  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,130  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_complex
2024-04-24T10:38:12,130  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,130  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:38:12,134  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,134  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,134  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,134  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_parted_basic
2024-04-24T10:38:12,135  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:38:12,138  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,138  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,138  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,138  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:12,138  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,138  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,138  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,139  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_parted_basic
2024-04-24T10:38:12,139  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,139  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:38:12,142  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,142  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,142  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,143  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_specific_size
2024-04-24T10:38:12,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:38:12,146  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,146  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,147  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,147  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:12,147  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,147  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,147  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,147  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_specific_size
2024-04-24T10:38:12,148  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,148  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:38:12,151  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,151  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,152  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,152  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_parted_date
2024-04-24T10:38:12,153  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:38:12,156  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,156  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,156  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,156  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:12,156  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.004 seconds
2024-04-24T10:38:12,157  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,157  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,157  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_parted_date
2024-04-24T10:38:12,157  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,157  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:38:12,160  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,160  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,160  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,161  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:12,161  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:38:12,164  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,164  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,164  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,164  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:12,165  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,165  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,165  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,165  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:12,165  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,165  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:38:12,168  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:12,168  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:12,168  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.003 seconds
2024-04-24T10:38:12,169  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop database if exists junit_specific_db cascade
2024-04-24T10:38:12,169  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,173  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:38:12,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:38:12,181  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:12,181  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:12,181  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:12,181  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, getTables_(String, String)=8, getTableObjectsByName_(String, List)=2, flushCache_()=0}
2024-04-24T10:38:12,181  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 0.012 seconds
2024-04-24T10:38:12,181  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:12,181  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:12,181  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad): drop database if exists junit_specific_db cascade
2024-04-24T10:38:12,182  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:12,182  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:12,183  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:12,183  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:12,185  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:12,188  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:38:12,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:12,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:12,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:12,194  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:12,196  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:12,198  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:12,204  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:12,212  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:12,212  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:12,213 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:14,214  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:14,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:14,220  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:14,222  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:14,224  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:14,225  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:14,230  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:14,238  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:14,238  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:14,240 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:16,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:16,245  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:16,246  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:16,249  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:16,250  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:16,251  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:16,256  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:16,263  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:16,264  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:16,265 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:18,266  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:18,271  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:18,272  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:18,274  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:18,275  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:18,277  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:18,281  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:18,288  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:18,288  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:18,289 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:20,290  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:20,295  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:20,296  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:20,299  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:20,300  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:20,302  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:20,307  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:20,314  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:20,314  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:20,315 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:22,317  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:22,320  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:22,321  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:22,323  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:22,325  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:22,327  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:22,331  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:22,339  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:22,339  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:22,340 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:24,342  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:24,346  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:24,347  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:24,349  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:24,350  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:24,351  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:24,356  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:24,364  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:24,364  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:24,365 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:26,366  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:26,371  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:26,372  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:26,375  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:26,377  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:26,379  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:26,385  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:26,392  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:26,392  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:26,394 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:28,395  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:28,400  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:28,401  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:28,403  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:28,405  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:28,406  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:28,412  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:28,422  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:28,422  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:28,424 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:30,425  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:30,430  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:30,431  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:30,434  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:30,435  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:30,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:30,441  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:30,448  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:30,448  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:30,450 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:32,451  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:32,454  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:32,455  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:32,457  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:32,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:32,460  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:32,464  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:32,472  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:32,472  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:32,474 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:32,474 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:32,478 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:32,481  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:32,481  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:38:32,481  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:32,481 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:32,481  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,481  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,481  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103811_ee489420-285c-4bb0-8a1b-1cd029f246ad); Time taken: 20.3 seconds
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:32,514  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:32,516  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:32,516  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:32,516  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@71c1ba6d will be shutdown
2024-04-24T10:38:32,517  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@18e69455 created in the thread with id: 1
2024-04-24T10:38:32,519  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 417c0726-d56f-4483-a7a5-b70b0900ba27
2024-04-24T10:38:32,520  INFO [main] SessionState: Hive Session ID = 417c0726-d56f-4483-a7a5-b70b0900ba27
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:32,520  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:32,525  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/417c0726-d56f-4483-a7a5-b70b0900ba27
2024-04-24T10:38:32,528  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/417c0726-d56f-4483-a7a5-b70b0900ba27
2024-04-24T10:38:32,530  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/417c0726-d56f-4483-a7a5-b70b0900ba27/_tmp_space.db
2024-04-24T10:38:32,531  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): use default
2024-04-24T10:38:32,531  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:32,532  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,532  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,532  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,533  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,533  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.001 seconds
2024-04-24T10:38:32,533  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,533  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,533  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): use default
2024-04-24T10:38:32,533  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,534  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:32,535  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:32,536  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,536  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:38:32,536  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.003 seconds
2024-04-24T10:38:32,536  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:32,537  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff
2024-04-24T10:38:32,537  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:32,537  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=417c0726-d56f-4483-a7a5-b70b0900ba27, clientType=HIVECLI]
2024-04-24T10:38:32,537  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:38:32,537  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:32,538  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@447dec7e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@18e69455 will be shutdown
2024-04-24T10:38:32,538  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:32,538  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -10
2024-04-24T10:38:32,539  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:32,539  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:32,539  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:32,540  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6, with PersistenceManager: null will be shutdown
2024-04-24T10:38:32,540  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fc0f3c2 created in the thread with id: 1
2024-04-24T10:38:32,542  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6 from thread id: 1
2024-04-24T10:38:32,542  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:32,542  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:32,542  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:38:32,543  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:32,543  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:32,543  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3fc0f3c2 will be shutdown
2024-04-24T10:38:32,544  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@62d59d60 created in the thread with id: 1
2024-04-24T10:38:32,548  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:32,548  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:32,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:32,549  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff
2024-04-24T10:38:32,549  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,549  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,549  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,549  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:32,549  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.013 seconds
2024-04-24T10:38:32,549  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,549  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,549  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:32,550  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,550  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:38:32,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:32,550  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@74baaba6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@62d59d60 will be shutdown
2024-04-24T10:38:32,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:32,550  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -11
2024-04-24T10:38:32,552  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:32,553  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:32,553  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:32,553  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521, with PersistenceManager: null will be shutdown
2024-04-24T10:38:32,554  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@27868127 created in the thread with id: 1
2024-04-24T10:38:32,556  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521 from thread id: 1
2024-04-24T10:38:32,556  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:32,556  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:32,557  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980312, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, bucketing_version=2, numRows=0, numFiles=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, transactional=false, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:32,569  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:32,571 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:32,572 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:32,574 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:32,576  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:32,577  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
2024-04-24T10:38:32,577  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:38:32,577 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:38:32,577  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,577  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:32,577  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.028 seconds
2024-04-24T10:38:32,577  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_basic
2024-04-24T10:38:32,578  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:32,588  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,588  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,588  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,588  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,588  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=10, flushCache_()=0}
2024-04-24T10:38:32,588  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.011 seconds
2024-04-24T10:38:32,588  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,589  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,589  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_basic
2024-04-24T10:38:32,589  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,589  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:32,599  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,599  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:32,609  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,610  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:32,747  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic does not exist; Force to delete it.
2024-04-24T10:38:32,747 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:38:32,747  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,747  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=148, getTable_(GetTableRequest)=10}
2024-04-24T10:38:32,748  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.158 seconds
2024-04-24T10:38:32,748  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_complex
2024-04-24T10:38:32,749  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:32,760  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,760  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,760  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,760  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,760  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=12, flushCache_()=0}
2024-04-24T10:38:32,761  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.012 seconds
2024-04-24T10:38:32,761  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,761  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,761  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_complex
2024-04-24T10:38:32,761  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,761  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:32,773  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,773  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:32,784  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,784  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:32,825  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex does not exist; Force to delete it.
2024-04-24T10:38:32,825 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:38:32,826  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,826  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, dropTable_(String, String, boolean, boolean, boolean)=52, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,826  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.065 seconds
2024-04-24T10:38:32,827  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_parted_basic
2024-04-24T10:38:32,827  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:32,837  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,837  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,838  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,838  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,838  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10, flushCache_()=0}
2024-04-24T10:38:32,838  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.011 seconds
2024-04-24T10:38:32,838  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,839  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,839  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_parted_basic
2024-04-24T10:38:32,839  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,839  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:32,849  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,850  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:32,862  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,862  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:32,898  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic does not exist; Force to delete it.
2024-04-24T10:38:32,898 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:38:32,898  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,898  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=48, getTable_(GetTableRequest)=11}
2024-04-24T10:38:32,899  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.059 seconds
2024-04-24T10:38:32,899  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_specific_size
2024-04-24T10:38:32,900  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:32,909  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,909  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,909  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,909  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,909  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,909  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.01 seconds
2024-04-24T10:38:32,910  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,910  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,910  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_specific_size
2024-04-24T10:38:32,910  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,910  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:32,920  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,920  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:32,931  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:32,932  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:32,975  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size does not exist; Force to delete it.
2024-04-24T10:38:32,975 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:38:32,976  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,976  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=56, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:38:32,976  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.066 seconds
2024-04-24T10:38:32,977  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_parted_date
2024-04-24T10:38:32,977  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:32,980  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,980  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,980  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,981  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,981  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.003 seconds
2024-04-24T10:38:32,981  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,981  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,981  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_parted_date
2024-04-24T10:38:32,981  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,981  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:32,984  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,985  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,985  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.003 seconds
2024-04-24T10:38:32,985  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:32,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:32,989  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:32,989  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.004 seconds
2024-04-24T10:38:32,989  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:32,989  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:32,990  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:32,993  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:32,993  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:32,993  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.004 seconds
2024-04-24T10:38:32,993  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop database if exists junit_specific_db cascade
2024-04-24T10:38:32,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:32,997  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:38:33,002  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:38:33,003  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:33,003  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:33,003  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:33,003  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTables_(String, String)=5, getTableObjectsByName_(String, List)=1, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:33,003  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 0.01 seconds
2024-04-24T10:38:33,004  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:33,004  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:33,004  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff): drop database if exists junit_specific_db cascade
2024-04-24T10:38:33,004  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:33,004  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:33,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:33,006  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:33,008  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:33,011  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:38:33,013  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:33,015  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:33,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:33,018  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:33,019  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:33,021  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:33,028  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:33,038  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:33,038  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:33,040 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:35,041  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:35,047  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:35,047  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:35,049  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:35,051  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:35,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:35,056  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:35,063  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:35,063  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:35,065 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:37,066  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:37,071  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:37,074  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:37,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:37,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:37,096  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:37,114  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:37,128  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:37,128  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:37,130 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:39,131  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:39,135  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:39,136  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:39,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:39,139  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:39,141  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:39,146  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:39,153  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:39,153  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:39,154 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:41,155  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:41,157  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:41,157  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:41,159  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:41,161  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:41,162  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:41,167  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:41,174  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:41,174  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:41,175 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:43,176  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:43,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:43,182  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:43,184  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:43,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:43,187  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:43,195  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:43,204  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:43,204  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:43,205 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:45,206  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:45,211  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:45,211  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:45,214  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:45,215  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:45,216  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:45,221  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:45,228  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:45,229  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:45,230 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:47,231  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:47,233  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:47,235  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:47,238  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:47,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:47,243  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:47,253  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:47,266  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:47,266  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:47,268 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:49,269  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:49,271  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:49,273  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:49,276  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:49,277  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:49,279  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:49,284  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:49,291  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:49,291  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:49,293 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:51,294  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:51,299  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:51,300  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:51,303  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:51,304  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:51,305  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:51,311  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:51,318  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:51,318  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:51,320 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:53,321  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:53,326  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:53,327  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:53,329  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:53,332  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:53,334  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:53,342  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:53,352  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:53,352  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:53,354 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:38:53,355 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:53,358 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:38:53,362  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:53,362  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:38:53,362  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:53,362 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (2).  The statement has been rolled back.)
2024-04-24T10:38:53,362  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,362  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,362  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103832_24a27c1e-b925-4233-9912-49fabd1eb7ff); Time taken: 20.358 seconds
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:53,393  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:53,395  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,395  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,396  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@27868127 will be shutdown
2024-04-24T10:38:53,396  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d22dd52 created in the thread with id: 1
2024-04-24T10:38:53,418  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 4c9e6210-65a5-4ae5-850c-a171b606e577
2024-04-24T10:38:53,418  INFO [main] SessionState: Hive Session ID = 4c9e6210-65a5-4ae5-850c-a171b606e577
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:53,418  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:53,424  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/4c9e6210-65a5-4ae5-850c-a171b606e577
2024-04-24T10:38:53,426  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/4c9e6210-65a5-4ae5-850c-a171b606e577
2024-04-24T10:38:53,429  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/4c9e6210-65a5-4ae5-850c-a171b606e577/_tmp_space.db
2024-04-24T10:38:53,429  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,430  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,431  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,431  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,431  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,431  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,431  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,431  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,432  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,432  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,432  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,432  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,433  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,434  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,434  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:53,434  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,435  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,435  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,435  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,435  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4c9e6210-65a5-4ae5-850c-a171b606e577, clientType=HIVECLI]
2024-04-24T10:38:53,436  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:38:53,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:53,436  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@42a3e521, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5d22dd52 will be shutdown
2024-04-24T10:38:53,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,436  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -12
2024-04-24T10:38:53,437  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,438  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:53,438  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,438  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075, with PersistenceManager: null will be shutdown
2024-04-24T10:38:53,439  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5f638200 created in the thread with id: 1
2024-04-24T10:38:53,440  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075 from thread id: 1
2024-04-24T10:38:53,440  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,440  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,441  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:38:53,442  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,442  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,442  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5f638200 will be shutdown
2024-04-24T10:38:53,442  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@67feb5d0 created in the thread with id: 1
2024-04-24T10:38:53,444  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,444  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,444  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,445  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,445  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,445  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,445  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,445  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:53,445  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.01 seconds
2024-04-24T10:38:53,445  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,446  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,446  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,446  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,446  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:38:53,446  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:53,446  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b98075, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@67feb5d0 will be shutdown
2024-04-24T10:38:53,446  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,446  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -13
2024-04-24T10:38:53,447  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,448  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:53,448  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,448  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78, with PersistenceManager: null will be shutdown
2024-04-24T10:38:53,449  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33e0d356 created in the thread with id: 1
2024-04-24T10:38:53,450  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78 from thread id: 1
2024-04-24T10:38:53,450  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,451  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,451  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=0, totalSize=0, transactional=false, rawDataSize=0, bucketing_version=2, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,457  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:38:53,502  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,502  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=51}
2024-04-24T10:38:53,502  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.056 seconds
2024-04-24T10:38:53,503  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,503  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,505  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,505  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,505  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,505  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:53,505  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,505  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,505  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,506  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,506  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,506  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,508  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,509  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,509  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:38:53,509  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,510  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,511  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,511  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,511  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:38:53,511  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,513  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,513  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,513  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=1, getDatabase_(String)=2}
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,513  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,513  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, totalSize=0, bucketing_version=2, numFiles=0, numFilesErasureCoded=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,520  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:38:53,541  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,541  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=27, isCompatibleWith_(Configuration)=1}
2024-04-24T10:38:53,541  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.028 seconds
2024-04-24T10:38:53,542  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,542  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,544  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,544  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,544  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,544  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,544  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,544  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,544  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,544  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,545  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,545  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,546  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,548  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,548  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,548  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.004 seconds
2024-04-24T10:38:53,548  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,549  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,549  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,549  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:38:53,549  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,551  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,551  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,551  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,551  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,551  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2, flushCache_()=0}
2024-04-24T10:38:53,552  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,552  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,552  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,552  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,552  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,553  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,558  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:38:53,575  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,576  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=22}
2024-04-24T10:38:53,576  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.023 seconds
2024-04-24T10:38:53,576  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,577  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,578  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,578  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,578  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,578  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,579  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,579  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,579  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,579  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use default
2024-04-24T10:38:53,579  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,579  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,582  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,582  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:38:53,582  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,583  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,583  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,583  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,583  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:38:53,584  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,585  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,585  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,585  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,585  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,585  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2, flushCache_()=0}
2024-04-24T10:38:53,586  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.002 seconds
2024-04-24T10:38:53,586  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,586  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,586  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,586  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,587  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, bucketing_version=2, transactional=false, numFiles=0, totalSize=0, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,593  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:38:53,610  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,610  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=23}
2024-04-24T10:38:53,610  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.024 seconds
2024-04-24T10:38:53,611  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create database if not exists junit_specific_db
2024-04-24T10:38:53,611  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,611  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,611  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,611  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,612  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.0 seconds
2024-04-24T10:38:53,612  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,612  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,612  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create database if not exists junit_specific_db
2024-04-24T10:38:53,612  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,612  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:38:53,612  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:38:53,614 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Database junit_specific_db already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:1151)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:605)
	at org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.execute(CreateDatabaseOperation.java:68)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:129)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:53,615  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,615  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,615  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,615  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use junit_specific_db
2024-04-24T10:38:53,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,617  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,618  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,618  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,618  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=1}
2024-04-24T10:38:53,618  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,618  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,618  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,618  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): use junit_specific_db
2024-04-24T10:38:53,618  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,618  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,620  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,622  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,622  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:38:53,622  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,622  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,623  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,623  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,623  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:38:53,623  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,625  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf
2024-04-24T10:38:53,625  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,625  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,625  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,625  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,625  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,625  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,625  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,625  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,626  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,627  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, numFiles=0, rawDataSize=0, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numRows=0, transactional=false, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,637  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:53,640 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:53,640 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:53,642 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:53,645  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:53,645  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
2024-04-24T10:38:53,645  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:38:53,645 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:38:53,645  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,645  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,645  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.02 seconds
2024-04-24T10:38:53,646  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_basic
2024-04-24T10:38:53,646  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:38:53,649  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,649  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,649  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,649  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,650  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,650  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,650  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,650  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_basic
2024-04-24T10:38:53,650  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,650  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:38:53,653  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,653  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,654  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,654  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_complex
2024-04-24T10:38:53,655  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:38:53,657  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,658  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,658  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,658  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,658  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.004 seconds
2024-04-24T10:38:53,658  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,658  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,658  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_complex
2024-04-24T10:38:53,658  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,659  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:38:53,661  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,662  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,662  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,662  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_parted_basic
2024-04-24T10:38:53,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:38:53,665  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,666  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,666  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,666  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:53,666  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.004 seconds
2024-04-24T10:38:53,666  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,666  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,666  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_parted_basic
2024-04-24T10:38:53,666  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,666  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:38:53,669  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,669  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,670  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,670  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_specific_size
2024-04-24T10:38:53,671  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:38:53,673  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,673  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,673  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,673  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,674  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,674  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,674  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,674  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_specific_size
2024-04-24T10:38:53,674  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,674  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:38:53,677  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,677  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,677  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,678  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_parted_date
2024-04-24T10:38:53,678  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:38:53,681  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,681  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,681  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,681  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,681  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,681  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,681  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,682  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_parted_date
2024-04-24T10:38:53,682  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,682  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:38:53,685  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,685  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,685  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,685  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:53,686  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:38:53,688  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,688  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,689  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,689  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,689  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.004 seconds
2024-04-24T10:38:53,689  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,689  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,689  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:53,689  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,690  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:38:53,692  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,692  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,693  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.003 seconds
2024-04-24T10:38:53,693  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop database if exists junit_specific_db cascade
2024-04-24T10:38:53,694  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,696  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:38:53,701  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,702  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,702  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTables_(String, String)=5, getTableObjectsByName_(String, List)=2, flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.009 seconds
2024-04-24T10:38:53,702  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf): drop database if exists junit_specific_db cascade
2024-04-24T10:38:53,702  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,703  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:53,704  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:53,704  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:53,707  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:53,710  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:38:53,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:53,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:53,715  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:53,717  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:53,719  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:53,720  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:53,734  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:38:53,767  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:53,776  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:53,776  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:53,779  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:38:53,779 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:38:53,780  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:38:53,780 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:38:53,781  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,781  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropDatabase_(String, boolean, boolean, boolean)=78}
2024-04-24T10:38:53,781  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_3cdd7afd-f53a-4d4f-a04b-a8120956c3cf); Time taken: 0.079 seconds
2024-04-24T10:38:53,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:53,817  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:53,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:53,817  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:53,818  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:53,820  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,820  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,820  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33e0d356 will be shutdown
2024-04-24T10:38:53,821  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@fbd1a1b created in the thread with id: 1
2024-04-24T10:38:53,822  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 78047716-2a4c-4560-9137-f1d9a4b7131d
2024-04-24T10:38:53,823  INFO [main] SessionState: Hive Session ID = 78047716-2a4c-4560-9137-f1d9a4b7131d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:53,823  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:53,828  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/78047716-2a4c-4560-9137-f1d9a4b7131d
2024-04-24T10:38:53,831  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/78047716-2a4c-4560-9137-f1d9a4b7131d
2024-04-24T10:38:53,833  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/78047716-2a4c-4560-9137-f1d9a4b7131d/_tmp_space.db
2024-04-24T10:38:53,834  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): use default
2024-04-24T10:38:53,834  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,835  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,835  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,836  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,836  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:38:53,836  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.002 seconds
2024-04-24T10:38:53,836  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,836  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,836  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): use default
2024-04-24T10:38:53,836  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,836  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,837  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,839  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,839  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,839  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.003 seconds
2024-04-24T10:38:53,840  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,840  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2
2024-04-24T10:38:53,840  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:53,841  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=78047716-2a4c-4560-9137-f1d9a4b7131d, clientType=HIVECLI]
2024-04-24T10:38:53,841  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:38:53,841  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:53,841  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb49e78, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@fbd1a1b will be shutdown
2024-04-24T10:38:53,841  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,841  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-24T10:38:53,842  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,843  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:53,843  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,843  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab, with PersistenceManager: null will be shutdown
2024-04-24T10:38:53,843  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d121ab6 created in the thread with id: 1
2024-04-24T10:38:53,845  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab from thread id: 1
2024-04-24T10:38:53,845  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,845  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,846  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:38:53,847  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,847  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,847  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d121ab6 will be shutdown
2024-04-24T10:38:53,847  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f1dd46b created in the thread with id: 1
2024-04-24T10:38:53,849  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,849  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,850  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:53,851  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,851  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,851  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.011 seconds
2024-04-24T10:38:53,851  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:53,851  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,851  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:38:53,852  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:53,852  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2683fbab, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4f1dd46b will be shutdown
2024-04-24T10:38:53,852  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,852  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-24T10:38:53,853  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:53,854  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:53,854  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:53,854  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76, with PersistenceManager: null will be shutdown
2024-04-24T10:38:53,854  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@607fa188 created in the thread with id: 1
2024-04-24T10:38:53,856  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76 from thread id: 1
2024-04-24T10:38:53,856  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:53,856  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:53,857  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980333, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=0, bucketing_version=2, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, rawDataSize=0, numFilesErasureCoded=0, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:53,867  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:53,870 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:38:53,871 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:53,873 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:38:53,876  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:38:53,876  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
2024-04-24T10:38:53,876  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:38:53,876 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:38:53,877  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,877  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:53,877  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.025 seconds
2024-04-24T10:38:53,877  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_basic
2024-04-24T10:38:53,877  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:53,888  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,888  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,888  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=11, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.011 seconds
2024-04-24T10:38:53,888  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_basic
2024-04-24T10:38:53,888  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,889  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:53,928  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:53,929  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:53,929  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,929  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-24T10:38:53,929  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,929  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-24T10:38:53,929  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,929  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -18
2024-04-24T10:38:53,929  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,929  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -19
2024-04-24T10:38:53,930  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:53,930  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -20
2024-04-24T10:38:53,939  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:53,939  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:53,982  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic does not exist; Force to delete it.
2024-04-24T10:38:53,982 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:38:53,982  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:53,982  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=39, dropTable_(String, String, boolean, boolean, boolean)=54, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:53,982  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.094 seconds
2024-04-24T10:38:53,983  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_complex
2024-04-24T10:38:53,983  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:53,993  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:53,993  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:53,994  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:53,994  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:53,994  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:38:53,994  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.011 seconds
2024-04-24T10:38:53,994  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:53,994  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:53,994  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_complex
2024-04-24T10:38:53,994  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:53,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:54,005  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,005  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:54,015  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:54,054  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex does not exist; Force to delete it.
2024-04-24T10:38:54,054 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:38:54,054  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,054  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=49, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=11}
2024-04-24T10:38:54,054  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.06 seconds
2024-04-24T10:38:54,055  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_parted_basic
2024-04-24T10:38:54,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:54,064  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,065  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,065  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,065  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,065  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:38:54,065  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.01 seconds
2024-04-24T10:38:54,065  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,065  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,065  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_parted_basic
2024-04-24T10:38:54,066  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,066  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:54,075  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,075  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:54,084  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:54,121  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic does not exist; Force to delete it.
2024-04-24T10:38:54,121 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:38:54,121  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,121  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=46, getTable_(GetTableRequest)=9}
2024-04-24T10:38:54,121  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.056 seconds
2024-04-24T10:38:54,122  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_specific_size
2024-04-24T10:38:54,122  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:54,133  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,133  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,133  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,133  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,133  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=11, flushCache_()=0}
2024-04-24T10:38:54,134  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.011 seconds
2024-04-24T10:38:54,134  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,134  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,134  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_specific_size
2024-04-24T10:38:54,134  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:54,147  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,147  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:54,163  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,164  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:54,212  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size does not exist; Force to delete it.
2024-04-24T10:38:54,212 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:38:54,212  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,212  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=65, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=13}
2024-04-24T10:38:54,212  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.078 seconds
2024-04-24T10:38:54,213  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_parted_date
2024-04-24T10:38:54,214  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:54,217  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,217  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,217  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,217  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:54,217  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.004 seconds
2024-04-24T10:38:54,217  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,217  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,217  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_parted_date
2024-04-24T10:38:54,218  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,218  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:54,221  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,221  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,221  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.004 seconds
2024-04-24T10:38:54,221  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:54,222  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,225  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,225  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,225  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,225  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:54,225  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.004 seconds
2024-04-24T10:38:54,226  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,226  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,226  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:54,226  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,226  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,229  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,229  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,229  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.003 seconds
2024-04-24T10:38:54,230  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop database if exists junit_specific_db cascade
2024-04-24T10:38:54,230  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:54,231  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,231  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,232  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,232  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,232  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.002 seconds
2024-04-24T10:38:54,232  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,232  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,232  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2): drop database if exists junit_specific_db cascade
2024-04-24T10:38:54,232  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,232  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:54,233  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103853_93b95bcd-5aa8-43c6-9daf-6ad278d8d1a2); Time taken: 0.0 seconds
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:54,268  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:54,271  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:54,272  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:54,272  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@607fa188 will be shutdown
2024-04-24T10:38:54,272  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73145db2 created in the thread with id: 1
2024-04-24T10:38:54,274  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 19d26b80-e11d-4899-8de7-93b270be620e
2024-04-24T10:38:54,274  INFO [main] SessionState: Hive Session ID = 19d26b80-e11d-4899-8de7-93b270be620e
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:54,275  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:38:54,280  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/19d26b80-e11d-4899-8de7-93b270be620e
2024-04-24T10:38:54,283  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/19d26b80-e11d-4899-8de7-93b270be620e
2024-04-24T10:38:54,285  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/19d26b80-e11d-4899-8de7-93b270be620e/_tmp_space.db
2024-04-24T10:38:54,286  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,286  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,287  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,287  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,287  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,287  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,288  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.001 seconds
2024-04-24T10:38:54,288  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,288  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,288  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,288  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,288  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,290  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,290  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,291  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:54,291  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.002 seconds
2024-04-24T10:38:54,291  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,292  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,292  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,292  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=19d26b80-e11d-4899-8de7-93b270be620e, clientType=HIVECLI]
2024-04-24T10:38:54,292  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:38:54,292  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:54,292  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2604e76, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73145db2 will be shutdown
2024-04-24T10:38:54,293  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:54,293  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -21
2024-04-24T10:38:54,294  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:54,295  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:54,295  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:54,295  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f, with PersistenceManager: null will be shutdown
2024-04-24T10:38:54,295  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c9b96af created in the thread with id: 1
2024-04-24T10:38:54,297  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f from thread id: 1
2024-04-24T10:38:54,297  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:54,297  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:54,297  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:38:54,298  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:54,298  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:54,299  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c9b96af will be shutdown
2024-04-24T10:38:54,299  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35390561 created in the thread with id: 1
2024-04-24T10:38:54,301  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:54,301  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:54,301  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,302  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,302  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,302  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,302  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,302  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:38:54,302  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.011 seconds
2024-04-24T10:38:54,303  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,303  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,303  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,303  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,303  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:38:54,303  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:38:54,303  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@10d2411f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35390561 will be shutdown
2024-04-24T10:38:54,303  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:54,303  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -22
2024-04-24T10:38:54,305  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:54,306  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:54,306  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:54,306  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025, with PersistenceManager: null will be shutdown
2024-04-24T10:38:54,306  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@561419ce created in the thread with id: 1
2024-04-24T10:38:54,308  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025 from thread id: 1
2024-04-24T10:38:54,308  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:54,308  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:54,309  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numRows=0, rawDataSize=0, bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFiles=0, transactional=false, totalSize=0, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,313  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:38:54,334  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,334  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=26}
2024-04-24T10:38:54,334  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.031 seconds
2024-04-24T10:38:54,334  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,335  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,336  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,336  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,336  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,337  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:38:54,337  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.002 seconds
2024-04-24T10:38:54,337  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,337  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,337  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,337  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,337  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,339  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,340  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,340  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,341  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,341  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,342  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,342  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,342  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:38:54,342  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,344  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,344  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,344  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,344  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,344  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:54,344  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,345  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,345  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,345  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,345  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,346  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, bucketing_version=2, numRows=0, totalSize=0, numFilesErasureCoded=0, transactional=false, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,352  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:38:54,379  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,379  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=33, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,380  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.034 seconds
2024-04-24T10:38:54,380  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,381  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,383  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,383  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,383  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,383  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:54,384  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,384  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,384  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,384  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,384  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,384  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,386  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,389  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,389  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=4}
2024-04-24T10:38:54,389  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.005 seconds
2024-04-24T10:38:54,390  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,390  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,390  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,390  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:38:54,391  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,392  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,392  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,392  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,392  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,393  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:54,393  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,393  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,393  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,393  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,393  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,394  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,400  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:38:54,416  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,416  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=22, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,416  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.023 seconds
2024-04-24T10:38:54,417  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,417  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,420  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,420  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,420  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,420  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,420  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,420  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,420  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,421  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,421  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,421  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,423  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,425  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,425  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=4}
2024-04-24T10:38:54,425  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.005 seconds
2024-04-24T10:38:54,426  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,426  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,426  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,427  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:38:54,427  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,428  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,429  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,429  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,429  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,429  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,431  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{transactional=false, bucketing_version=2, numFilesErasureCoded=0, numFiles=0, rawDataSize=0, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,437  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:38:54,455  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,455  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=24}
2024-04-24T10:38:54,455  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.025 seconds
2024-04-24T10:38:54,455  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create database if not exists junit_specific_db
2024-04-24T10:38:54,456  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,456  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,456  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,456  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:54,457  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.001 seconds
2024-04-24T10:38:54,457  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,457  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,457  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create database if not exists junit_specific_db
2024-04-24T10:38:54,457  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,457  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:38:54,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:38:54,464  INFO [main] metastore.HMSHandler: Creating database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:38:54,464  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:38:54,467  INFO [main] metastore.HMSHandler: Created database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:38:54,470  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,471  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createDatabase_(Database)=12}
2024-04-24T10:38:54,471  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.013 seconds
2024-04-24T10:38:54,471  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use junit_specific_db
2024-04-24T10:38:54,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:54,473  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,473  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,473  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,474  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-24T10:38:54,474  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.002 seconds
2024-04-24T10:38:54,474  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,474  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,474  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use junit_specific_db
2024-04-24T10:38:54,474  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,475  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:54,476  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:54,478  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,478  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,478  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.004 seconds
2024-04-24T10:38:54,478  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,479  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,479  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,479  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:38:54,479  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:54,481  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,481  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,481  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,481  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,481  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:38:54,481  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,481  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,481  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,481  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,482  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,483  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, rawDataSize=0, numFiles=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, numRows=0, numFilesErasureCoded=0, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,488  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2
2024-04-24T10:38:54,506  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,506  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=24}
2024-04-24T10:38:54,506  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.025 seconds
2024-04-24T10:38:54,507  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,507  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,509  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,509  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,509  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): use default
2024-04-24T10:38:54,509  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,510  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,511  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,512  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,513  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,513  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,513  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,514  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,514  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,514  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_date position=13
2024-04-24T10:38:54,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,516  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,516  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,516  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,516  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,516  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:38:54,516  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.003 seconds
2024-04-24T10:38:54,516  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,516  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,516  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:38:54,517  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,518  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_date, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:date, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,524  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date
2024-04-24T10:38:54,537  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,537  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=19, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,537  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.021 seconds
2024-04-24T10:38:54,538  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:38:54,539  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,539  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:38:54,540  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_alltypes position=13
2024-04-24T10:38:54,540  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:38:54,542  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,542  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,542  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,542  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,542  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:38:54,542  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.004 seconds
2024-04-24T10:38:54,543  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,543  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,543  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:38:54,543  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:54,545  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_alltypes, dbName:default, owner:alex, createTime:1713980334, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c_boolean, type:boolean, comment:null), FieldSchema(name:c_tinyint, type:tinyint, comment:null), FieldSchema(name:c_smallint, type:smallint, comment:null), FieldSchema(name:c_int, type:int, comment:null), FieldSchema(name:c_bigint, type:bigint, comment:null), FieldSchema(name:c_float, type:float, comment:null), FieldSchema(name:c_double, type:double, comment:null), FieldSchema(name:c_decimal, type:decimal(5,2), comment:null), FieldSchema(name:c_string, type:string, comment:null), FieldSchema(name:c_char, type:char(10), comment:null), FieldSchema(name:c_varchar, type:varchar(20), comment:null), FieldSchema(name:c_binary, type:binary, comment:null), FieldSchema(name:c_date, type:date, comment:null), FieldSchema(name:c_timestamp, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, field.delim=	}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, numFilesErasureCoded=0, numRows=0, numFiles=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"c_bigint":"true","c_binary":"true","c_boolean":"true","c_char":"true","c_date":"true","c_decimal":"true","c_double":"true","c_float":"true","c_int":"true","c_smallint":"true","c_string":"true","c_timestamp":"true","c_tinyint":"true","c_varchar":"true"}}, rawDataSize=0, totalSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:38:54,552  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_alltypes
2024-04-24T10:38:54,574  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,574  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=30, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:54,574  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.031 seconds
2024-04-24T10:38:54,575  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:38:54,576  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,587  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,589  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e
2024-04-24T10:38:54,589  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:54,589  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:54,589  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:54,590  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=12, flushCache_()=0}
2024-04-24T10:38:54,590  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.014 seconds
2024-04-24T10:38:54,590  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:54,590  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:54,590  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:38:54,591  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.junit_unparted_alltypes
2024-04-24T10:38:54,591  INFO [main] exec.Task: Loading data to table default.junit_unparted_alltypes from file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data
2024-04-24T10:38:54,591  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,602  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,603  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,614  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,622  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:38:54,622  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:38:54,651  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T10:38:54,651  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,663  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,663  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T10:38:54,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:54,676  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,678  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:38:54,678  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:38:54,712  INFO [main] stats.BasicStatsTask: Table default.junit_unparted_alltypes stats: [numFiles=1, numRows=0, totalSize=329, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T10:38:54,712  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:54,712  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, alter_table_(String, String, String, Table, EnvironmentContext, String)=63, getTable_(GetTableRequest)=48}
2024-04-24T10:38:54,713  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.122 seconds
2024-04-24T10:38:54,731  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:38:54,732  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-88f80a0e-6ba7-4d63-9377-1b5134cf8ce6
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:54,925  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:54,926  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:54,926  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:54,927  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:54,931  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:54,940  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:54,960  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:54,961  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:54,961  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:54,961  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:54,978  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:54,979  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:54,979  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:54,979  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:54,979  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:54,979  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:54,980  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:54,981  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:54,983  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:54,992  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,012  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,013  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,013  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,013  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,013  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,013  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,031  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,031  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:55,032  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:55,033  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,035  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:38:55,044  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,068  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,068  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,088  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,088  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,088  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,088  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:55,089  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:55,090  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:55,102  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,125  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,125  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,143  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,143  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:55,144  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:38:55,145  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,147  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:55,157  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,180  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,181  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,200  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,201  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:55,202  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:55,203  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,205  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:55,215  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,240  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,241  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,241  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,241  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,242  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,262  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,263  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:55,264  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:38:55,265  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,267  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:55,277  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,297  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,298  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,298  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,298  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,298  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,302  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:38:55,310  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:38:55,323  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:38:55,327  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 8
2024-04-24T10:38:55,328  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 map-only splittees.
2024-04-24T10:38:55,328  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 out of total 6 MR operators.
2024-04-24T10:38:55,328  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 3
2024-04-24T10:38:55,335  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:55,337  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:38:55,337  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:38:55,338  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:38:55,338  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:38:55,338  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:38:55,338  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980335338-0
2024-04-24T10:38:55,345  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:38:55,345  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:38:55,346  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:38:55,347  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:38:55,347  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:38:55,347  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980335347-0
2024-04-24T10:38:55,354  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:38:55,354  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:38:55,355  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:38:55,356  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:38:55,356  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:38:55,356  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980335356-0
2024-04-24T10:38:55,362  INFO [main] mapReduceLayer.MapReduceLauncher: 3 map-reduce job(s) waiting for submission.
2024-04-24T10:38:55,365  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:55,381  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,382  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,382  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,383  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:55,384  INFO [JobControl] metastore.HMSHandler: 7: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:55,384  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:55,384  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3f1f9a73, with PersistenceManager: null will be shutdown
2024-04-24T10:38:55,384  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3f1f9a73, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@106f42d9 created in the thread with id: 469
2024-04-24T10:38:55,386  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3f1f9a73 from thread id: 469
2024-04-24T10:38:55,386  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:55,386  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:55,387  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,399  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:38:55,404  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:38:55,405  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,406  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:38:55,406  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:38:55,406  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:38:55,424  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:38:55,438  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1234608527_0004
2024-04-24T10:38:55,438  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:38:55,494  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:38:55,494  INFO [Thread-447] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:38:55,495  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:55,500  INFO [Thread-447] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:38:55,502  INFO [Thread-447] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:38:55,503  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1234608527_0004_m_000000_0
2024-04-24T10:38:55,511  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:38:55,513  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 65
Input split[0]:
   Length = 65
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:38:55,514  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,514  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data:0+65
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,516  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,517  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,517  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,520  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:38:55,520  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:38:55,521  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:38:55,525  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,525  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,535  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,535  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-14 15%3A10%3A15/_temporary/0/_temporary/attempt_local1234608527_0004_m_000000_0/part-m-00000
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,536  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,542  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,543  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,543  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,546  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@68d7ad10
2024-04-24T10:38:55,550  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,550  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,558  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,558  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-15 18%3A54%3A55/_temporary/0/_temporary/attempt_local1234608527_0004_m_000000_0/part-m-00000
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,559  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,567  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,567  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,568  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,570  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@57868bbf
2024-04-24T10:38:55,571  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:55,578  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Registering committer for TaskAttemptID:attempt_local1234608527_0004_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:38:55,578  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1234608527_0004_m_000000_0 is done. And is in the process of committing
2024-04-24T10:38:55,580  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:55,580  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1234608527_0004_m_000000_0 is allowed to commit now
2024-04-24T10:38:55,581  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Committing TaskAttempt:attempt_local1234608527_0004_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:38:55,581  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:38:55,582  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1234608527_0004_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:38:55,582  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:38:55,583  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1234608527_0004_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:38:55,584  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Discarding all cleanup for TaskAttemptID:attempt_local1234608527_0004_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.448983048406286/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:38:55,585  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:38:55,585  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1234608527_0004_m_000000_0' done.
2024-04-24T10:38:55,585  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1234608527_0004_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=13293
		FILE: Number of bytes written=2476082
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=494
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1052246016
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:38:55,585  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1234608527_0004_m_000000_0
2024-04-24T10:38:55,585  INFO [Thread-447] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,593  INFO [Thread-447] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,593  INFO [Thread-447] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,593  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,593  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,594  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,599  INFO [Thread-447] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,599  INFO [Thread-447] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,614  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,615  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,615  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,615  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,615  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,616  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,623  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:38:55,628  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:38:55,629  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,629  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:38:55,629  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:38:55,630  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,645  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,645  INFO [Thread-447] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,646  INFO [Thread-447] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:55,647  INFO [Thread-447] metastore.HMSHandler: 8: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:55,647  INFO [Thread-447] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:55,648  INFO [Thread-447] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@707ce5e7, with PersistenceManager: null will be shutdown
2024-04-24T10:38:55,648  INFO [Thread-447] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@707ce5e7, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@475b1d23 created in the thread with id: 489
2024-04-24T10:38:55,648  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:38:55,650  INFO [Thread-447] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@707ce5e7 from thread id: 489
2024-04-24T10:38:55,650  INFO [Thread-447] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:55,650  INFO [Thread-447] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:55,650  INFO [Thread-447] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,665  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1651786004_0005
2024-04-24T10:38:55,665  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,691  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,692  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,692  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,692  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,692  INFO [Thread-447] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_date has new partitions [{dt=2016-07-14 15:10:15}, {dt=2016-07-15 18:54:55}].
2024-04-24T10:38:55,713  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:38:55,714  INFO [Thread-489] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:38:55,714  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:55,721  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,721  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,725  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,725  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,729  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,729  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,732  INFO [Thread-447] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:38:55,732  INFO [Thread-447] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:38:55,734  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,734  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,737  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,738  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,738  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,738  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,738  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,740  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,740  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,741  INFO [Thread-489] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:38:55,745  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,745  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,747  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:38:55,751  INFO [HMSHandler #4] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:38:55,751  INFO [HMSHandler #5] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:38:55,751  INFO [HMSHandler #4] utils.MetaStoreServerUtils: Updated size to 476
2024-04-24T10:38:55,751  INFO [HMSHandler #5] utils.MetaStoreServerUtils: Updated size to 469
2024-04-24T10:38:55,753  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:38:55,756  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,757  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,757  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,757  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:38:55,757  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:38:55,758  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:38:55,767  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,767  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,777  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,777  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,778  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:38:55,790  INFO [Thread-489] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,790  INFO [Thread-489] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,793  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1835524214_0006
2024-04-24T10:38:55,793  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:38:55,801  INFO [Thread-489] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:38:55,801  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1651786004_0005_m_000000_0
2024-04-24T10:38:55,808  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,808  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,813  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,813  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,818  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,818  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,825  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,825  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,833  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,833  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,838  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,838  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,841  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,841  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,844  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,844  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,845  INFO [Thread-447] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:55,848  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,848  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,866  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1234608527_0004
2024-04-24T10:38:55,866  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases E,F
2024-04-24T10:38:55,866  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:38:55,867  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1651786004_0005
2024-04-24T10:38:55,867  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:38:55,867  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,B,B2,C,C2
2024-04-24T10:38:55,867  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:38:55,868  INFO [Thread-540] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:38:55,868  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:38:55,868  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -23
2024-04-24T10:38:55,869  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1835524214_0006
2024-04-24T10:38:55,869  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases D
2024-04-24T10:38:55,869  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:38:55,869  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,869  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,870  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:38:55,870  INFO [main] mapReduceLayer.MapReduceLauncher: 16% complete
2024-04-24T10:38:55,870  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1234608527_0004,job_local1651786004_0005,job_local1835524214_0006]
2024-04-24T10:38:55,871  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 54
Input split[0]:
   Length = 54
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:38:55,873  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,873  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data:0+54
2024-04-24T10:38:55,875  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:38:55,875  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:38:55,876  INFO [Thread-540] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,876  INFO [Thread-540] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,877  INFO [Thread-540] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:38:55,878  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:38:55,880  INFO [Thread-540] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,880  INFO [Thread-540] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,883  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,883  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,886  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,886  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,1953008740339105/_temporary/0/_temporary/attempt_local1651786004_0005_m_000000_0/part-m-00000
2024-04-24T10:38:55,886  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,886  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,887  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,887  INFO [Thread-540] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:38:55,888  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1835524214_0006_m_000000_0
2024-04-24T10:38:55,896  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,896  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,898  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@3056108c
2024-04-24T10:38:55,900  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,900  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,900  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:38:55,901  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 342
Input split[0]:
   Length = 342
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:38:55,902  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:38:55,903  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data:0+342
2024-04-24T10:38:55,904  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,904  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,14024880888986346/_temporary/0/_temporary/attempt_local1651786004_0005_m_000000_0/part-m-00000
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,907  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,908  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,908  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,910  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,6405042381755697/_temporary/0/_temporary/attempt_local1835524214_0006_m_000000_0/part-m-00000
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,911  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:55,917  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:55,918  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:55,918  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:55,918  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:55,918  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:55,918  WARN [Thread-447] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:55,918  INFO [Thread-447] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:55,919  INFO [Thread-447] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:55,919  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@40a6ec98
2024-04-24T10:38:55,921  INFO [Thread-447] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:55,923  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@4b88330b
2024-04-24T10:38:55,925  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:38:55,925  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:38:55,925  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,926  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,927  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:38:55,928  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,6589368329908963/_temporary/0/_temporary/attempt_local1651786004_0005_m_000000_0/part-m-00000
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,929  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,934  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:55,940  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@39e7e9fe
2024-04-24T10:38:55,946  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,946  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,43502105128188284/bkt=0/_temporary/0/_temporary/attempt_local1651786004_0005_m_000000_0/part-m-00000
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,948  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,949  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,952  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1835524214_0006_m_000000_0 is done. And is in the process of committing
2024-04-24T10:38:55,954  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,954  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,956  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:55,956  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1835524214_0006_m_000000_0 is allowed to commit now
2024-04-24T10:38:55,958  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,958  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,960  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1835524214_0006_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,6405042381755697
2024-04-24T10:38:55,961  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:38:55,961  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1835524214_0006_m_000000_0' done.
2024-04-24T10:38:55,961  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@e3081f6
2024-04-24T10:38:55,961  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1835524214_0006_m_000000_0: Counters: 16
	File System Counters
		FILE: Number of bytes read=17542
		FILE: Number of bytes written=3718472
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=488
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1056440320
	MultiStoreCounters
		Output records in _2_junit_specific_db.junit_specific_size2=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:38:55,961  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1835524214_0006_m_000000_0
2024-04-24T10:38:55,961  INFO [Thread-540] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:38:55,966  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:55,966  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,33489008483439775/bkt=1/_temporary/0/_temporary/attempt_local1651786004_0005_m_000000_0/part-m-00000
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:38:55,969  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:38:55,980  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@149f6de4
2024-04-24T10:38:56,012  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,012  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,012  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,013  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,013  INFO [Thread-540] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,014  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:56,014  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1651786004_0005_m_000000_0 is done. And is in the process of committing
2024-04-24T10:38:56,015  INFO [Thread-540] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:56,015  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,015  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,016  INFO [Thread-540] metastore.HMSHandler: 9: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:56,016  INFO [Thread-540] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:56,016  INFO [Thread-540] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b2c31bf, with PersistenceManager: null will be shutdown
2024-04-24T10:38:56,016  INFO [Thread-540] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b2c31bf, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2269560b created in the thread with id: 587
2024-04-24T10:38:56,018  INFO [Thread-540] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b2c31bf from thread id: 587
2024-04-24T10:38:56,018  INFO [Thread-540] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:56,018  INFO [Thread-540] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:56,018  INFO [Thread-540] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,021  INFO [Thread-540] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_complex newtbl=junit_unparted_complex	
2024-04-24T10:38:56,022  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:56,022  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1651786004_0005_m_000000_0 is allowed to commit now
2024-04-24T10:38:56,023  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,023  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,025  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1651786004_0005_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,1953008740339105
2024-04-24T10:38:56,026  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,026  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,028  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1651786004_0005_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,14024880888986346
2024-04-24T10:38:56,029  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,029  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,030  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1651786004_0005_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,6589368329908963
2024-04-24T10:38:56,032  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,032  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,033  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1651786004_0005_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,43502105128188284/bkt=0
2024-04-24T10:38:56,035  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:56,035  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:56,036  INFO [Thread-540] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_complex
2024-04-24T10:38:56,036  INFO [Thread-540] utils.MetaStoreServerUtils: Updated size of table junit_unparted_complex to 2660
2024-04-24T10:38:56,037  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1651786004_0005_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,33489008483439775/bkt=1
2024-04-24T10:38:56,037  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:38:56,037  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1651786004_0005_m_000000_0' done.
2024-04-24T10:38:56,038  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1651786004_0005_m_000000_0: Counters: 20
	File System Counters
		FILE: Number of bytes read=21448
		FILE: Number of bytes written=3721701
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=9
		Map output records=0
		Input split bytes=486
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=18
		Total committed heap usage (bytes)=1056440320
	MultiStoreCounters
		Output records in _0_junit_unparted_basic=9
		Output records in _1_junit_specific_size=9
		Output records in _2_junit_specific_db.junit_specific_size2=9
		Output records in _3_junit_parted_basic=3
		Output records in _4_junit_parted_basic=6
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:38:56,038  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1651786004_0005_m_000000_0
2024-04-24T10:38:56,038  INFO [Thread-489] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:38:56,046  INFO [Thread-540] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,085  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,085  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,087  INFO [Thread-489] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:38:56,087  INFO [Thread-489] metastore.HMSHandler: 10: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:38:56,087  INFO [Thread-489] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:38:56,088  INFO [Thread-489] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1cd3d8b0, with PersistenceManager: null will be shutdown
2024-04-24T10:38:56,088  INFO [Thread-489] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1cd3d8b0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6e229246 created in the thread with id: 533
2024-04-24T10:38:56,092  INFO [Thread-489] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1cd3d8b0 from thread id: 533
2024-04-24T10:38:56,092  INFO [Thread-489] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:38:56,092  INFO [Thread-489] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:38:56,092  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,095  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_basic newtbl=junit_unparted_basic	
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,103  WARN [Thread-540] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,103  INFO [Thread-540] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,104  INFO [Thread-540] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,105  INFO [Thread-540] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,110  INFO [Thread-489] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_basic
2024-04-24T10:38:56,110  INFO [Thread-489] utils.MetaStoreServerUtils: Updated size of table junit_unparted_basic to 647
2024-04-24T10:38:56,116  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,148  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,148  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,149  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,149  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,150  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,151  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,191  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,191  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,192  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,194  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_specific_size newtbl=junit_specific_size	
2024-04-24T10:38:56,207  INFO [Thread-489] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size
2024-04-24T10:38:56,207  INFO [Thread-489] utils.MetaStoreServerUtils: Updated size of table junit_specific_size to 647
2024-04-24T10:38:56,214  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,251  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,252  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,252  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,252  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,252  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,252  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,252  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,252  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,254  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,295  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,296  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,296  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,296  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,296  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,296  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,299  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.junit_specific_db.junit_specific_size2 newtbl=junit_specific_size2	
2024-04-24T10:38:56,313  INFO [Thread-489] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size2
2024-04-24T10:38:56,313  INFO [Thread-489] utils.MetaStoreServerUtils: Updated size of table junit_specific_size2 to 647
2024-04-24T10:38:56,320  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,351  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,351  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,351  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,352  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,352  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,352  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,354  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,371  INFO [main] mapReduceLayer.MapReduceLauncher: 83% complete
2024-04-24T10:38:56,371  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1651786004_0005]
2024-04-24T10:38:56,372  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,375  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,377  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,392  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,394  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,397  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,408  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,408  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,409  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,443  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,444  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,444  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=0}].
2024-04-24T10:38:56,445  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[0]	
2024-04-24T10:38:56,449  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:38:56,449  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:38:56,460  INFO [HMSHandler #6] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:38:56,460  INFO [HMSHandler #6] utils.MetaStoreServerUtils: Updated size to 597
2024-04-24T10:38:56,495  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,530  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,530  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,531  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,532  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,572  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,573  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,573  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,577  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=1}].
2024-04-24T10:38:56,579  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[1]	
2024-04-24T10:38:56,582  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:38:56,582  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:38:56,590  INFO [HMSHandler #7] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:38:56,590  INFO [HMSHandler #7] utils.MetaStoreServerUtils: Updated size to 635
2024-04-24T10:38:56,621  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,654  WARN [Thread-489] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,655  INFO [Thread-489] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:38:56,655  INFO [Thread-489] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,657  INFO [Thread-489] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:38:56,775  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,781  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,786  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,793  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:38:56,793  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_unparted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:56,794  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:38:56,794  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_size does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:56,795  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:38:56,795  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_db.junit_specific_size2 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:56,796  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:38:56,796  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:56,797  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:38:56,797  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:38:56,798  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:38:56,798  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:38:55	2024-04-24 10:38:56	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1234608527_0004	1	0	n/a	n/a	n/a	n/a	0	0	0	0	E,F	MAP_ONLY	junit_parted_date,
job_local1651786004_0005	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,B,B2,C,C2	MULTI_QUERY,MAP_ONLY	junit_unparted_basic,junit_specific_size,junit_specific_db.junit_specific_size2,junit_parted_basic,junit_parted_basic,
job_local1835524214_0006	1	0	n/a	n/a	n/a	n/a	0	0	0	0	D	MAP_ONLY	junit_unparted_complex,

Input(s):
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data"
Successfully read 9 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data"
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data"

Output(s):
Successfully stored 2 records in: "junit_parted_date"
Successfully stored 9 records in: "junit_unparted_basic"
Successfully stored 9 records in: "junit_specific_size"
Successfully stored 9 records in: "junit_specific_db.junit_specific_size2"
Successfully stored 3 records in: "junit_parted_basic"
Successfully stored 6 records in: "junit_parted_basic"
Successfully stored 2 records in: "junit_unparted_complex"

Counters:
Total records written : 40
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1234608527_0004
job_local1651786004_0005
job_local1835524214_0006


2024-04-24T10:38:56,802  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,807  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,812  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,815  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,821  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,823  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,824  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:38:56,837  INFO [main] pig.ExecTypeProvider: Trying ExecType : LOCAL
2024-04-24T10:38:56,837  INFO [main] pig.ExecTypeProvider: Picked LOCAL as the ExecType
log4j: Parsing for [root] with value=[INFO, PIGCONSOLE].
log4j: Level token is [INFO].
log4j: Category root set to INFO
log4j: Parsing appender named "PIGCONSOLE".
log4j: Parsing layout options for "PIGCONSOLE".
log4j: Setting property [conversionPattern] to [%d [%t] %-5p %c - %m%n].
log4j: End of parsing for "PIGCONSOLE".
log4j: Setting property [target] to [System.err].
log4j: Parsed "PIGCONSOLE" options.
log4j: Parsing for [org.apache.pig] with value=[INFO].
log4j: Level token is [INFO].
log4j: Category org.apache.pig set to INFO
log4j: Handling log4j.additivity.org.apache.pig=[null]
log4j: Finished configuring.
2024-04-24T10:38:56,856  INFO [main] pig.Main: Apache Pig version 0.16.0 (r1747271) compiled Jun 07 2016, 10:12:57
2024-04-24T10:38:56,856  INFO [main] pig.Main: Logging error messages to: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/pig_1713980336837.log
2024-04-24T10:38:56,866  INFO [main] util.Utils: Default bootup file /home/alex/.pigbootup not found
2024-04-24T10:38:56,892  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:38:56,893  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-test.pig-2af0a00b-2220-4eaa-85c7-c6a682f58a64
2024-04-24T10:38:56,893  WARN [main] pig.PigServer: ATS is disabled since yarn.timeline-service.enabled set to false
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.test.console.log.level does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,922  WARN [main] conf.HiveConf: HiveConf of name hive.version does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.root does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,923  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,923  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,926  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:56,936  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:56,947  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:38:56,953  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-24T10:38:56,954  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:38:56,958  INFO [main] rules.ColumnPruneVisitor: Columns pruned for A: $2, $3, $4, $5
2024-04-24T10:38:56,960  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:38:56,961  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-24T10:38:56,961  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-24T10:38:56,967  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:56,969  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:38:56,970  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:38:56,983  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:38:56,983  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:38:56,983  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.root does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.version does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.test.console.log.level does not exist
2024-04-24T10:38:56,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:38:56,984  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:38:56,985  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:38:56,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:38:56,988  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:56,998  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,007  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:38:57,007  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:38:57,007  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:38:57,007  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980337007-0
2024-04-24T10:38:57,018  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-24T10:38:57,020  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,033  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:38:57,037  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, mapreduce.jobtracker.staging.root.dir=${test.tmp.dir}/cli/mapred/staging, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-04-24T10:38:57,046  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-04-24T10:38:57,050  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:38:57,067  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:38:57,080  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local834486134_0007
2024-04-24T10:38:57,080  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:38:57,127  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:38:57,128  INFO [Thread-639] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:38:57,133  INFO [Thread-639] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:57,134  INFO [Thread-639] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:57,134  INFO [Thread-639] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:38:57,142  INFO [Thread-639] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:38:57,142  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local834486134_0007_m_000000_0
2024-04-24T10:38:57,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:57,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:57,152  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:38:57,153  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 2660
Input split[0]:
   Length = 2660
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-04-24T10:38:57,158  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@38e58056
2024-04-24T10:38:57,162  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:38:57,162  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:38:57,222  INFO [LocalJobRunner Map Task Executor #0] hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2 records.
2024-04-24T10:38:57,222  INFO [LocalJobRunner Map Task Executor #0] hadoop.InternalParquetRecordReader: at row 0. reading next block
2024-04-24T10:38:57,229  INFO [LocalJobRunner Map Task Executor #0] hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 2
2024-04-24T10:38:57,246  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe with properties {name=default.junit_unparted_complex, numFiles=1, columns.types=string,int,struct<phno:string,email:string>,array<string>,map<string,string>,array<struct<phno:string,type:string>>, numFilesErasureCoded=0, serialization.format=1, columns=name,studentid,contact,currently_registered_courses,current_grades,phnos, rawDataSize=0, columns.comments=null null null null null null, numRows=0, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, totalSize=2660, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1713980334}
2024-04-24T10:38:57,247  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:38:57,247  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
log4j: Could not find root logger information. Is this OK?
log4j: Parsing for [org.apache.pig] with value=[INFO].
log4j: Level token is [INFO].
log4j: Category org.apache.pig set to INFO
log4j: Handling log4j.additivity.org.apache.pig=[null]
log4j: Finished configuring.
2024-04-24T10:38:57,249  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],C[3,4],B[2,4] C:  R: 
2024-04-24T10:38:57,250  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:57,252  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local834486134_0007_m_000000_0 is done. And is in the process of committing
2024-04-24T10:38:57,254  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:38:57,254  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local834486134_0007_m_000000_0 is allowed to commit now
2024-04-24T10:38:57,256  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local834486134_0007_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/colpushdownop
2024-04-24T10:38:57,257  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:38:57,257  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local834486134_0007_m_000000_0' done.
2024-04-24T10:38:57,257  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local834486134_0007_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=32290
		FILE: Number of bytes written=4344374
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=2350
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1056440320
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:38:57,257  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local834486134_0007_m_000000_0
2024-04-24T10:38:57,257  INFO [Thread-639] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:38:57,429  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local834486134_0007
2024-04-24T10:38:57,429  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,B,C
2024-04-24T10:38:57,429  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],C[3,4],B[2,4] C:  R: 
2024-04-24T10:38:57,437  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,444  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,450  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,457  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:38:57,458  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:38:56	2024-04-24 10:38:57	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local834486134_0007	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,B,C	MAP_ONLY	/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/colpushdownop,

Input(s):
Successfully read 2 records from: "junit_unparted_complex"

Output(s):
Successfully stored 2 records in: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/colpushdownop"

Counters:
Total records written : 2
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local834486134_0007


2024-04-24T10:38:57,462  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,469  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,474  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:38:57,485  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:38:57,497  INFO [main] pig.Main: Pig script completed in 654 milliseconds (654 ms)
2024-04-24T10:38:57,499  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_basic
2024-04-24T10:38:57,499  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:57,510  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,510  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,510  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,510  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,511  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=11}
2024-04-24T10:38:57,511  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.011 seconds
2024-04-24T10:38:57,511  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,511  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,511  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_basic
2024-04-24T10:38:57,511  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,511  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:57,521  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,521  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:57,531  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,531  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:38:57,571  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:57,571  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=10, dropTable_(String, String, boolean, boolean, boolean)=50, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:57,571  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.06 seconds
2024-04-24T10:38:57,572  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_complex
2024-04-24T10:38:57,572  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:57,580  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,581  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,581  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,581  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,581  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:57,581  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.009 seconds
2024-04-24T10:38:57,581  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,581  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,581  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_complex
2024-04-24T10:38:57,582  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,582  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:57,590  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,591  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:57,604  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,604  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:38:57,636  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:57,636  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8, dropTable_(String, String, boolean, boolean, boolean)=46}
2024-04-24T10:38:57,637  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.055 seconds
2024-04-24T10:38:57,637  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_parted_basic
2024-04-24T10:38:57,637  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:57,646  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,647  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,647  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.01 seconds
2024-04-24T10:38:57,647  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_parted_basic
2024-04-24T10:38:57,647  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,647  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:57,656  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,656  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:57,664  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,665  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:38:57,788  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:57,788  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=132}
2024-04-24T10:38:57,789  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.141 seconds
2024-04-24T10:38:57,789  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_specific_size
2024-04-24T10:38:57,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:57,823  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,823  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,823  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,823  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,823  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=34, isCompatibleWith_(Configuration)=0}
2024-04-24T10:38:57,823  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.034 seconds
2024-04-24T10:38:57,824  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,824  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,824  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_specific_size
2024-04-24T10:38:57,824  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,824  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:57,834  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,834  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:57,844  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,844  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:38:57,879  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:57,879  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=10, isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=45}
2024-04-24T10:38:57,879  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.055 seconds
2024-04-24T10:38:57,879  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_parted_date
2024-04-24T10:38:57,880  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:57,889  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,889  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,890  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,890  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,890  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, flushCache_()=0}
2024-04-24T10:38:57,890  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.011 seconds
2024-04-24T10:38:57,890  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,890  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,890  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_parted_date
2024-04-24T10:38:57,890  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,891  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:57,900  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,900  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:57,909  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,909  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:38:57,986  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:57,986  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=86, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:38:57,986  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.096 seconds
2024-04-24T10:38:57,986  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:57,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:57,996  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:57,996  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:57,996  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:57,996  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:57,996  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, flushCache_()=0}
2024-04-24T10:38:57,996  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.01 seconds
2024-04-24T10:38:57,997  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:57,997  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:57,997  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop table if exists junit_unparted_alltypes
2024-04-24T10:38:57,997  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:57,997  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:58,007  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:58,007  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:58,017  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:38:58,017  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:38:58,051  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:38:58,051  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=44, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:38:58,052  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.054 seconds
2024-04-24T10:38:58,052  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop database if exists junit_specific_db cascade
2024-04-24T10:38:58,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:58,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:38:58,062  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:38:58,062  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:38:58,062  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:38:58,063  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:38:58,063  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTables_(String, String)=7, getTableObjectsByName_(String, List)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:38:58,063  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 0.011 seconds
2024-04-24T10:38:58,063  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:38:58,063  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:38:58,063  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e): drop database if exists junit_specific_db cascade
2024-04-24T10:38:58,063  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:38:58,063  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:38:58,065  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:58,065  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:58,070  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:58,074  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:38:58,080  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:38:58,081  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:38:58,082  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:38:58,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:38:58,091  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:38:58,095  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:38:58,100  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:38:58,118  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:38:58,118  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:38:58,120 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:00,121  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:00,125  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:00,126  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:00,128  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:00,129  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:00,131  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:00,135  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:00,141  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:00,141  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:00,142 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:00,595  INFO [Metastore Scheduled Worker 2] metastore.HMSHandler: 11: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:00,595  INFO [Metastore Scheduled Worker 2] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64211c23, with PersistenceManager: null will be shutdown
2024-04-24T10:39:00,595  INFO [Metastore Scheduled Worker 2] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64211c23, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@601e847f created in the thread with id: 28
2024-04-24T10:39:00,620  INFO [Metastore Scheduled Worker 2] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64211c23 from thread id: 28
2024-04-24T10:39:02,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:02,150  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:02,151  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:02,153  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:02,155  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:02,156  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:02,160  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:02,167  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:02,167  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:02,168 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:04,169  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:04,172  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:04,173  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:04,175  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:04,176  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:04,177  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:04,181  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:04,188  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:04,189  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:04,190 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:06,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:06,196  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:06,197  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:06,199  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:06,200  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:06,201  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:06,205  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:06,211  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:06,211  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:06,213 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:08,214  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:08,219  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:08,222  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:08,229  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:08,234  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:08,239  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:08,252  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:08,265  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:08,265  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:08,266 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:09,980  INFO [HiveClientCache-cleaner-0] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:09,980  INFO [HiveClientCache-cleaner-0] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-24T10:39:09,980  INFO [HiveClientCache-cleaner-0] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:09,980  INFO [HiveClientCache-cleaner-0] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-24T10:39:09,981  INFO [HiveClientCache-cleaner-0] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:09,981  INFO [HiveClientCache-cleaner-0] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -26
2024-04-24T10:39:09,981  INFO [HiveClientCache-cleaner-0] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:09,981  INFO [HiveClientCache-cleaner-0] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -27
2024-04-24T10:39:10,267  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:10,272  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:10,273  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:10,276  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:10,278  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:10,280  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:10,285  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:10,293  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:10,293  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:10,294 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:12,294  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:12,300  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:12,301  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:12,303  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:12,304  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:12,306  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:12,310  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:12,317  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:12,317  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:12,318 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:14,320  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:14,325  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:14,325  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:14,327  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:14,329  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:14,330  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:14,334  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:14,341  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:14,341  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:14,342 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:16,343  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:16,348  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:16,351  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:16,358  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:16,363  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:16,368  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:16,383  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:16,394  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:16,394  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:16,395 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:18,396  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:18,401  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:18,402  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:18,404  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:18,405  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:18,407  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:18,411  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:18,417  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:18,417  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:18,418 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:18,419 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:39:18,422 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:39:18,425  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:39:18,425  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:39:18,425  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.)
2024-04-24T10:39:18,425 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.)
2024-04-24T10:39:18,425  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,425  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,426  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103854_e64617f7-34fd-43d0-a294-897b2073fc5e); Time taken: 20.362 seconds
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:18,458  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:18,462  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:18,462  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:18,462  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@561419ce will be shutdown
2024-04-24T10:39:18,463  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@249e3cb2 created in the thread with id: 1
2024-04-24T10:39:18,464  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = a51068c8-def0-48e4-9d74-bbfbaddfa9bf
2024-04-24T10:39:18,465  INFO [main] SessionState: Hive Session ID = a51068c8-def0-48e4-9d74-bbfbaddfa9bf
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:18,465  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:18,470  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/a51068c8-def0-48e4-9d74-bbfbaddfa9bf
2024-04-24T10:39:18,472  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/a51068c8-def0-48e4-9d74-bbfbaddfa9bf
2024-04-24T10:39:18,475  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/a51068c8-def0-48e4-9d74-bbfbaddfa9bf/_tmp_space.db
2024-04-24T10:39:18,475  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,476  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,477  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,477  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,477  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,477  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:18,477  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.002 seconds
2024-04-24T10:39:18,477  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,477  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,477  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,478  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,478  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,479  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,480  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,480  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:39:18,480  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,480  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,481  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,481  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:18,481  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a51068c8-def0-48e4-9d74-bbfbaddfa9bf, clientType=HIVECLI]
2024-04-24T10:39:18,481  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:39:18,481  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:18,481  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1e21c025, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@249e3cb2 will be shutdown
2024-04-24T10:39:18,482  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:18,482  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -28
2024-04-24T10:39:18,483  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:18,483  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:18,483  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:18,484  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33, with PersistenceManager: null will be shutdown
2024-04-24T10:39:18,484  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c7f3eae created in the thread with id: 1
2024-04-24T10:39:18,485  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33 from thread id: 1
2024-04-24T10:39:18,485  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:18,485  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:18,485  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:39:18,486  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:18,486  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:18,487  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1c7f3eae will be shutdown
2024-04-24T10:39:18,487  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5574f42b created in the thread with id: 1
2024-04-24T10:39:18,488  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:18,488  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:18,489  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,490  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,490  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,490  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.01 seconds
2024-04-24T10:39:18,490  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,490  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,490  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:39:18,491  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:18,491  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@455d5b33, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5574f42b will be shutdown
2024-04-24T10:39:18,491  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:18,491  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -29
2024-04-24T10:39:18,492  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:18,493  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:18,493  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:18,493  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377, with PersistenceManager: null will be shutdown
2024-04-24T10:39:18,493  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@484af6c1 created in the thread with id: 1
2024-04-24T10:39:18,494  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377 from thread id: 1
2024-04-24T10:39:18,494  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:18,495  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:18,495  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980358, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, transactional=false, numFiles=0, bucketing_version=2, rawDataSize=0, totalSize=0, numRows=0, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:18,501  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:39:18,537  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,537  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=41}
2024-04-24T10:39:18,537  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.047 seconds
2024-04-24T10:39:18,538  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,539  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,540  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,540  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,540  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,541  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:39:18,541  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.002 seconds
2024-04-24T10:39:18,541  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,541  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,541  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,541  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,542  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,543  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,545  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,545  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:18,545  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,546  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,546  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,547  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:18,547  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:39:18,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,548  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,548  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,549  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,549  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,549  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:18,549  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,549  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,549  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,549  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,550  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,551  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980358, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numRows=0, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, numFiles=0, bucketing_version=2, transactional=false, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:18,556  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:39:18,579  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,580  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=28, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,580  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.03 seconds
2024-04-24T10:39:18,580  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,582  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,582  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,583  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,583  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:18,583  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,583  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,583  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,583  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,584  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,584  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,585  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,587  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,587  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:18,587  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,588  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,588  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,588  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:18,588  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:39:18,588  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,590  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,590  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,590  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,590  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,590  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,591  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.002 seconds
2024-04-24T10:39:18,591  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,591  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,591  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,591  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,592  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980358, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:18,597  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:39:18,612  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,612  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=20, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,612  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.021 seconds
2024-04-24T10:39:18,613  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,613  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,615  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,615  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,615  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,615  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:39:18,615  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.002 seconds
2024-04-24T10:39:18,616  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,616  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,616  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use default
2024-04-24T10:39:18,616  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,618  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,619  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,619  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:18,620  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,620  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,621  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,621  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:18,621  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:39:18,621  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:18,622  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,622  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,623  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,623  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,623  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,623  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,623  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,623  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,623  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,623  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,624  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980358, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0, rawDataSize=0, bucketing_version=2, totalSize=0, numFiles=0, transactional=false, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:18,630  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:39:18,651  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:18,651  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -30
2024-04-24T10:39:18,668  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,668  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=44, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,668  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.045 seconds
2024-04-24T10:39:18,669  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create database if not exists junit_specific_db
2024-04-24T10:39:18,670  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,670  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,670  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,670  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,670  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.001 seconds
2024-04-24T10:39:18,671  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,671  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,671  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create database if not exists junit_specific_db
2024-04-24T10:39:18,671  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,671  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:39:18,671  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:39:18,673 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Database junit_specific_db already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:1151)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:605)
	at org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.execute(CreateDatabaseOperation.java:68)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:129)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:39:18,674  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,674  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,674  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,675  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use junit_specific_db
2024-04-24T10:39:18,676  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,678  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,678  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,678  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,678  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=3}
2024-04-24T10:39:18,678  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,679  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,679  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,679  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): use junit_specific_db
2024-04-24T10:39:18,679  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,679  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,681  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,683  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,684  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=4}
2024-04-24T10:39:18,684  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,684  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,685  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,685  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:18,685  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:39:18,685  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,688  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77
2024-04-24T10:39:18,688  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,688  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,688  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,688  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,688  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,689  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,689  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,689  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:18,689  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,690  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980358, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numRows=0, numFilesErasureCoded=0, bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFiles=0, transactional=false, rawDataSize=0, totalSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:18,737  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:18,740 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:39:18,740 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:39:18,743 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:39:18,745  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:39:18,746  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
2024-04-24T10:39:18,746  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:39:18,746 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:39:18,746  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,746  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,746  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.057 seconds
2024-04-24T10:39:18,747  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_basic
2024-04-24T10:39:18,747  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:39:18,750  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,750  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,750  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,750  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,750  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,751  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,751  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,751  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_basic
2024-04-24T10:39:18,751  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,751  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:39:18,754  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,754  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,755  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,755  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_complex
2024-04-24T10:39:18,755  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:39:18,759  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,759  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,759  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,759  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:18,759  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,759  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,759  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,759  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_complex
2024-04-24T10:39:18,760  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,760  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:39:18,764  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,764  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,764  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.005 seconds
2024-04-24T10:39:18,765  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_parted_basic
2024-04-24T10:39:18,765  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:39:18,768  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,768  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,769  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,769  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,769  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,769  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,769  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,769  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_parted_basic
2024-04-24T10:39:18,769  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,770  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:39:18,773  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,773  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-24T10:39:18,773  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,774  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_specific_size
2024-04-24T10:39:18,774  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,778  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,778  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,778  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_specific_size
2024-04-24T10:39:18,778  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,779  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:39:18,782  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,782  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,782  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,783  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_parted_date
2024-04-24T10:39:18,783  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:39:18,786  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,786  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,786  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,786  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:18,786  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,786  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,786  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,786  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_parted_date
2024-04-24T10:39:18,787  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,787  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:39:18,790  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,790  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,790  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.004 seconds
2024-04-24T10:39:18,791  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:18,791  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:39:18,794  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,794  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,794  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,795  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,795  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,795  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,795  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,795  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:18,795  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,795  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:39:18,798  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:18,798  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:18,799  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.003 seconds
2024-04-24T10:39:18,799  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop database if exists junit_specific_db cascade
2024-04-24T10:39:18,800  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,802  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:39:18,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:18,808  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:18,808  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, getTables_(String, String)=5, getTableObjectsByName_(String, List)=1, isCompatibleWith_(Configuration)=1, flushCache_()=0}
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 0.009 seconds
2024-04-24T10:39:18,808  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77): drop database if exists junit_specific_db cascade
2024-04-24T10:39:18,808  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:18,809  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:18,811  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:18,811  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:18,813  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:18,815  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:39:18,819  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:18,820  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:18,821  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:18,823  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:18,824  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:18,826  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:18,830  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:18,839  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:18,839  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:18,840 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:20,841  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:20,846  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:20,846  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:20,849  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:20,850  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:20,851  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:20,856  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:20,863  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:20,863  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:20,864 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:22,865  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:22,870  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:22,874  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:22,882  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:22,886  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:22,891  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:22,903  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:22,913  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:22,913  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:22,914 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:24,915  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:24,919  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:24,920  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:24,922  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:24,923  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:24,925  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:24,930  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:24,936  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:24,936  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:24,938 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:26,939  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:26,942  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:26,943  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:26,945  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:26,946  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:26,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:26,952  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:26,960  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:26,960  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:26,961 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:28,962  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:28,967  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:28,967  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:28,969  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:28,970  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:28,972  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:28,976  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:28,982  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:28,982  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:28,983 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:30,984  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:30,989  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:30,990  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:30,991  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:30,993  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:30,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:30,998  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:31,005  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:31,005  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:31,006 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:33,007  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:33,012  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:33,013  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:33,015  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:33,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:33,018  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:33,022  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:33,028  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:33,029  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:33,030 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:35,031  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:35,036  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:35,036  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:35,038  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:35,039  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:35,041  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:35,044  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:35,051  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:35,051  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:35,053 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (3).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:37,054  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:37,058  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:37,059  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:37,061  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:37,062  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:37,063  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:37,081  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:39:37,167  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:37,175  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:37,175  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:37,176  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:39:37,176 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:37,177  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:39:37,177 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:37,177  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,177  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropDatabase_(String, boolean, boolean, boolean)=18368, isCompatibleWith_(Configuration)=1}
2024-04-24T10:39:37,177  INFO [main] metadata.Hive: Total time spent in this metastore function was greater than 1000ms : dropDatabase_(String, boolean, boolean, boolean)=18368
2024-04-24T10:39:37,177  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103918_6f1e5e27-0ee1-42ce-838d-3b53230a9c77); Time taken: 18.369 seconds
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:37,209  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:37,211  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,211  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,211  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@484af6c1 will be shutdown
2024-04-24T10:39:37,212  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@258925f4 created in the thread with id: 1
2024-04-24T10:39:37,213  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = e11ffa81-5b7c-4a99-90ed-11fb958f456a
2024-04-24T10:39:37,213  INFO [main] SessionState: Hive Session ID = e11ffa81-5b7c-4a99-90ed-11fb958f456a
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:37,213  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:37,219  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e11ffa81-5b7c-4a99-90ed-11fb958f456a
2024-04-24T10:39:37,221  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/e11ffa81-5b7c-4a99-90ed-11fb958f456a
2024-04-24T10:39:37,223  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e11ffa81-5b7c-4a99-90ed-11fb958f456a/_tmp_space.db
2024-04-24T10:39:37,224  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): use default
2024-04-24T10:39:37,224  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,225  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,225  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,225  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,225  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,226  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.001 seconds
2024-04-24T10:39:37,226  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,226  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,226  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): use default
2024-04-24T10:39:37,226  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,226  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,227  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,228  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,228  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:39:37,229  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.002 seconds
2024-04-24T10:39:37,229  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,230  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7
2024-04-24T10:39:37,230  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,230  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e11ffa81-5b7c-4a99-90ed-11fb958f456a, clientType=HIVECLI]
2024-04-24T10:39:37,230  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:39:37,230  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:37,230  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5c74f377, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@258925f4 will be shutdown
2024-04-24T10:39:37,230  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:37,230  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -31
2024-04-24T10:39:37,231  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,232  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:37,232  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,232  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de, with PersistenceManager: null will be shutdown
2024-04-24T10:39:37,233  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73090cd8 created in the thread with id: 1
2024-04-24T10:39:37,234  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de from thread id: 1
2024-04-24T10:39:37,234  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,234  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,234  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:39:37,235  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,235  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,236  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73090cd8 will be shutdown
2024-04-24T10:39:37,236  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33638e3f created in the thread with id: 1
2024-04-24T10:39:37,237  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,237  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,238  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,239  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,239  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,239  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.01 seconds
2024-04-24T10:39:37,239  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,239  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,239  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:39:37,240  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:37,240  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6373f5de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33638e3f will be shutdown
2024-04-24T10:39:37,240  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:37,240  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -32
2024-04-24T10:39:37,241  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,242  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:37,242  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,242  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026, with PersistenceManager: null will be shutdown
2024-04-24T10:39:37,243  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1cf9430a created in the thread with id: 1
2024-04-24T10:39:37,244  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026 from thread id: 1
2024-04-24T10:39:37,244  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,244  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,245  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, totalSize=0, rawDataSize=0, numFiles=0, bucketing_version=2, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,254  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:37,257 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:39:37,257 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:39:37,259 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:39:37,262  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:39:37,262  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
2024-04-24T10:39:37,262  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:39:37,262 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:39:37,262  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,262  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:37,262  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.023 seconds
2024-04-24T10:39:37,263  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_basic
2024-04-24T10:39:37,263  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:37,272  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,272  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,272  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,272  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,273  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,273  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.009 seconds
2024-04-24T10:39:37,273  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,273  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,273  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_basic
2024-04-24T10:39:37,273  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,273  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:37,282  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,282  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:37,290  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,291  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:37,321  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic does not exist; Force to delete it.
2024-04-24T10:39:37,322 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:39:37,322  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,322  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=40, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9}
2024-04-24T10:39:37,322  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.049 seconds
2024-04-24T10:39:37,322  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_complex
2024-04-24T10:39:37,323  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:37,331  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,331  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,331  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,331  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,331  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8}
2024-04-24T10:39:37,332  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.009 seconds
2024-04-24T10:39:37,332  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,332  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,332  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_complex
2024-04-24T10:39:37,332  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,332  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:37,341  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,341  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:37,350  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,350  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:37,381  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex does not exist; Force to delete it.
2024-04-24T10:39:37,381 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:39:37,381  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,381  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=40, getTable_(GetTableRequest)=9}
2024-04-24T10:39:37,381  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.049 seconds
2024-04-24T10:39:37,382  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_parted_basic
2024-04-24T10:39:37,382  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:37,391  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,391  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,391  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,391  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,391  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,391  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.009 seconds
2024-04-24T10:39:37,392  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,392  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,392  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_parted_basic
2024-04-24T10:39:37,392  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,392  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:37,401  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,401  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:37,409  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,410  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:37,439  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic does not exist; Force to delete it.
2024-04-24T10:39:37,439 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:39:37,439  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,439  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, dropTable_(String, String, boolean, boolean, boolean)=38, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,439  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.047 seconds
2024-04-24T10:39:37,440  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_specific_size
2024-04-24T10:39:37,440  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:37,450  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,450  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,450  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,450  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,450  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=10, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,451  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.01 seconds
2024-04-24T10:39:37,451  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,451  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,451  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_specific_size
2024-04-24T10:39:37,451  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,451  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:37,460  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,460  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:37,468  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,468  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:37,498  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size does not exist; Force to delete it.
2024-04-24T10:39:37,498 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:39:37,499  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,499  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=39, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,499  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.048 seconds
2024-04-24T10:39:37,499  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_parted_date
2024-04-24T10:39:37,500  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:37,502  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,502  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,502  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,503  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,503  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.003 seconds
2024-04-24T10:39:37,503  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,503  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,503  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_parted_date
2024-04-24T10:39:37,503  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,503  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:37,506  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,506  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,506  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.003 seconds
2024-04-24T10:39:37,507  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:37,507  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,510  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,510  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.003 seconds
2024-04-24T10:39:37,510  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:37,510  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,511  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,513  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,513  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,513  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.003 seconds
2024-04-24T10:39:37,514  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop database if exists junit_specific_db cascade
2024-04-24T10:39:37,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:37,515  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,515  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,515  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,515  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,515  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.001 seconds
2024-04-24T10:39:37,516  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,516  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,516  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7): drop database if exists junit_specific_db cascade
2024-04-24T10:39:37,516  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,516  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:37,516  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_5af8abdd-7eeb-49ff-a655-78aeb3b9d7c7); Time taken: 0.0 seconds
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:37,548  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:37,550  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,550  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,550  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1cf9430a will be shutdown
2024-04-24T10:39:37,550  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@30ad0a65 created in the thread with id: 1
2024-04-24T10:39:37,551  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1
2024-04-24T10:39:37,552  INFO [main] SessionState: Hive Session ID = d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:37,552  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:37,557  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1
2024-04-24T10:39:37,560  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1
2024-04-24T10:39:37,562  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1/_tmp_space.db
2024-04-24T10:39:37,563  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,563  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,564  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,564  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,564  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,564  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:37,564  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.001 seconds
2024-04-24T10:39:37,565  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,565  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,565  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,565  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,565  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,566  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,567  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,567  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:39:37,568  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,568  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,569  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,569  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,569  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d4905fb3-d4aa-4786-b4b9-7a57b9c4efc1, clientType=HIVECLI]
2024-04-24T10:39:37,569  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:39:37,569  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:37,569  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ce2e026, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@30ad0a65 will be shutdown
2024-04-24T10:39:37,569  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:37,569  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -33
2024-04-24T10:39:37,570  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,571  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:37,571  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,572  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5, with PersistenceManager: null will be shutdown
2024-04-24T10:39:37,572  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b50afb4 created in the thread with id: 1
2024-04-24T10:39:37,573  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5 from thread id: 1
2024-04-24T10:39:37,573  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,573  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,573  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:39:37,574  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,574  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,574  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7b50afb4 will be shutdown
2024-04-24T10:39:37,575  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7a1ed74 created in the thread with id: 1
2024-04-24T10:39:37,576  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,576  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,577  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,577  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,577  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,577  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,577  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:37,577  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.009 seconds
2024-04-24T10:39:37,578  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,578  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,578  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,578  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,578  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:39:37,578  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:37,578  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3e971bd5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7a1ed74 will be shutdown
2024-04-24T10:39:37,578  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:37,578  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -34
2024-04-24T10:39:37,580  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:37,581  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:37,581  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:37,581  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31, with PersistenceManager: null will be shutdown
2024-04-24T10:39:37,581  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5efdf532 created in the thread with id: 1
2024-04-24T10:39:37,582  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31 from thread id: 1
2024-04-24T10:39:37,582  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:37,582  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:37,583  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0, numRows=0, numFiles=0, bucketing_version=2, rawDataSize=0, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,586  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:39:37,606  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,607  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=24}
2024-04-24T10:39:37,607  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.028 seconds
2024-04-24T10:39:37,607  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,607  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,609  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,609  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,609  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,609  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,609  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,609  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,609  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,609  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,610  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,610  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,611  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,613  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,613  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:39:37,613  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,613  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,614  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,614  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,614  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:39:37,614  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,616  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,616  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,616  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,616  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,616  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,617  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,617  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,617  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,617  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,617  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,618  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, bucketing_version=2, numFilesErasureCoded=0, rawDataSize=0, numFiles=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,623  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:39:37,638  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,639  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=20}
2024-04-24T10:39:37,639  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.021 seconds
2024-04-24T10:39:37,639  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,641  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,641  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,641  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,641  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,641  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,642  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,642  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,642  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,642  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,642  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,643  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,645  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,645  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,645  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,646  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,646  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,646  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,646  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:39:37,646  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,648  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,648  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,648  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,648  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,648  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,648  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,649  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,649  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,649  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,649  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,649  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,654  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:39:37,669  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,669  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=20, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,669  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.02 seconds
2024-04-24T10:39:37,670  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,670  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,672  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,672  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,672  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,672  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,672  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,672  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,672  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,672  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,673  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,673  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,674  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,676  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,676  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,676  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.004 seconds
2024-04-24T10:39:37,676  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,677  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,677  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,677  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:39:37,677  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,678  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,678  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,678  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,679  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,679  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-24T10:39:37,679  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,679  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,679  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,679  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,679  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,680  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, numRows=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, transactional=false, rawDataSize=0, numFiles=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,685  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:39:37,699  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,699  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=19}
2024-04-24T10:39:37,700  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.02 seconds
2024-04-24T10:39:37,700  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create database if not exists junit_specific_db
2024-04-24T10:39:37,700  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,700  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,700  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,700  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,701  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.0 seconds
2024-04-24T10:39:37,701  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,701  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,701  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create database if not exists junit_specific_db
2024-04-24T10:39:37,701  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,701  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:39:37,701  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:39:37,705  INFO [main] metastore.HMSHandler: Creating database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:37,705  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:37,707  INFO [main] metastore.HMSHandler: Created database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:37,710  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,710  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createDatabase_(Database)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,710  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.009 seconds
2024-04-24T10:39:37,711  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use junit_specific_db
2024-04-24T10:39:37,711  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:37,712  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,712  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,712  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,712  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,713  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.001 seconds
2024-04-24T10:39:37,713  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,713  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,713  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use junit_specific_db
2024-04-24T10:39:37,713  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:37,715  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:37,716  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,716  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,716  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,717  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,717  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,717  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,717  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:39:37,717  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:37,719  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,719  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,719  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,719  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,719  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:37,720  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,720  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,720  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,720  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,720  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,721  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, totalSize=0, rawDataSize=0, transactional=false, bucketing_version=2, numRows=0, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,726  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2
2024-04-24T10:39:37,739  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,739  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=18, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,739  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.019 seconds
2024-04-24T10:39:37,740  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,740  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,742  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,742  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2, flushCache_()=0}
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,742  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): use default
2024-04-24T10:39:37,742  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,742  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,744  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,745  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,745  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,745  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,746  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,746  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,746  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,746  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_date position=13
2024-04-24T10:39:37,746  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,748  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,748  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,748  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,748  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,748  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:39:37,748  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.002 seconds
2024-04-24T10:39:37,748  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,749  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,749  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:37,749  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,749  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_date, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:date, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,754  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date
2024-04-24T10:39:37,767  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,767  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=18}
2024-04-24T10:39:37,767  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.018 seconds
2024-04-24T10:39:37,768  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:39:37,769  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,769  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:37,769  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_alltypes position=13
2024-04-24T10:39:37,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:37,770  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,770  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,771  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,771  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,771  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:37,771  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.003 seconds
2024-04-24T10:39:37,771  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,771  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,771  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:39:37,771  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:37,772  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_alltypes, dbName:default, owner:alex, createTime:1713980377, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c_boolean, type:boolean, comment:null), FieldSchema(name:c_tinyint, type:tinyint, comment:null), FieldSchema(name:c_smallint, type:smallint, comment:null), FieldSchema(name:c_int, type:int, comment:null), FieldSchema(name:c_bigint, type:bigint, comment:null), FieldSchema(name:c_float, type:float, comment:null), FieldSchema(name:c_double, type:double, comment:null), FieldSchema(name:c_decimal, type:decimal(5,2), comment:null), FieldSchema(name:c_string, type:string, comment:null), FieldSchema(name:c_char, type:char(10), comment:null), FieldSchema(name:c_varchar, type:varchar(20), comment:null), FieldSchema(name:c_binary, type:binary, comment:null), FieldSchema(name:c_date, type:date, comment:null), FieldSchema(name:c_timestamp, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, field.delim=	}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"c_bigint":"true","c_binary":"true","c_boolean":"true","c_char":"true","c_date":"true","c_decimal":"true","c_double":"true","c_float":"true","c_int":"true","c_smallint":"true","c_string":"true","c_timestamp":"true","c_tinyint":"true","c_varchar":"true"}}, numRows=0, numFiles=0, rawDataSize=0, numFilesErasureCoded=0, totalSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:37,777  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_alltypes
2024-04-24T10:39:37,796  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,796  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=23, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,796  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.025 seconds
2024-04-24T10:39:37,796  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:39:37,797  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,797  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,806  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,808  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4
2024-04-24T10:39:37,808  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:37,808  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:37,808  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:37,808  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,808  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.012 seconds
2024-04-24T10:39:37,808  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:37,808  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:37,808  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:39:37,809  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.junit_unparted_alltypes
2024-04-24T10:39:37,809  INFO [main] exec.Task: Loading data to table default.junit_unparted_alltypes from file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data
2024-04-24T10:39:37,809  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,818  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,819  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,829  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,835  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:39:37,835  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:39:37,860  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T10:39:37,860  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,869  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,869  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T10:39:37,869  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:37,878  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:37,879  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:39:37,880  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:39:37,909  INFO [main] stats.BasicStatsTask: Table default.junit_unparted_alltypes stats: [numFiles=1, numRows=0, totalSize=329, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T10:39:37,909  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:37,909  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=37, alter_table_(String, String, String, Table, EnvironmentContext, String)=55, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:37,910  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.101 seconds
2024-04-24T10:39:37,928  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:39:37,929  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-test.pig-2af0a00b-2220-4eaa-85c7-c6a682f58a64
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,087  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,088  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:38,089  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,094  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:38,103  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,124  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,124  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,125  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,140  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,141  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,141  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,142  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.ignore.mapjoin.hint=false, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:38,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,144  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:38,152  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,173  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,174  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,174  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,190  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,190  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,190  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,190  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,191  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,191  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,192  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:38,193  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,195  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:39:38,203  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,223  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,223  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,224  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,239  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,239  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,240  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,240  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,241  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:38,241  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,244  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:38,252  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,282  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,282  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,298  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,299  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,299  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:38,300  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,302  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:38,311  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,331  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,331  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,332  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,347  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,348  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,348  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,348  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,348  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,348  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,348  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,349  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:38,349  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,351  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:38,359  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,380  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,380  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,381  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,381  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,381  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,397  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:38,398  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:38,399  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,401  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:38,409  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:38,428  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,428  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,428  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,429  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,429  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,429  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,433  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:39:38,440  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:39:38,464  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:39:38,468  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 8
2024-04-24T10:39:38,469  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 map-only splittees.
2024-04-24T10:39:38,469  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 out of total 6 MR operators.
2024-04-24T10:39:38,469  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 3
2024-04-24T10:39:38,475  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:38,477  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:38,477  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:38,478  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:39:38,478  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:38,478  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:38,478  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980378478-0
2024-04-24T10:39:38,484  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:38,484  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:38,485  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:39:38,485  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:38,485  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:38,486  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980378485-0
2024-04-24T10:39:38,492  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:38,492  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:38,493  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:39:38,494  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:38,494  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:38,494  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980378494-0
2024-04-24T10:39:38,501  INFO [main] mapReduceLayer.MapReduceLauncher: 3 map-reduce job(s) waiting for submission.
2024-04-24T10:39:38,503  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,519  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,520  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,521  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:38,522  INFO [JobControl] metastore.HMSHandler: 13: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:38,522  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:38,522  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb50966, with PersistenceManager: null will be shutdown
2024-04-24T10:39:38,522  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb50966, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@70cd5cc created in the thread with id: 802
2024-04-24T10:39:38,523  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6eb50966 from thread id: 802
2024-04-24T10:39:38,523  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:38,524  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:38,524  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,534  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:38,539  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:38,540  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:38,540  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:38,541  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:38,541  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:38,558  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:38,572  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1631061732_0008
2024-04-24T10:39:38,572  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:38,626  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:38,627  INFO [Thread-762] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:38,628  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:38,633  INFO [Thread-762] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:38,636  INFO [Thread-762] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:38,636  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1631061732_0008_m_000000_0
2024-04-24T10:39:38,644  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:38,645  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 65
Input split[0]:
   Length = 65
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:38,646  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:38,646  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data:0+65
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,648  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,648  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,649  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,651  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:38,652  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:38,653  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:39:38,657  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,657  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,658  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:38,663  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:38,664  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:38,665  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:38,665  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:38,666  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:38,667  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,667  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-14 15%3A10%3A15/_temporary/0/_temporary/attempt_local1631061732_0008_m_000000_0/part-m-00000
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:38,668  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:38,669  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:38,669  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:38,669  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:38,669  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:38,669  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:38,678  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@5b5ace8
2024-04-24T10:39:38,682  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,682  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,685  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:38,690  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,690  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-15 18%3A54%3A55/_temporary/0/_temporary/attempt_local1631061732_0008_m_000000_0/part-m-00000
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:38,691  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:38,699  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local293788078_0009
2024-04-24T10:39:38,699  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:38,702  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@76e2a5a8
2024-04-24T10:39:38,702  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:38,709  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Registering committer for TaskAttemptID:attempt_local1631061732_0008_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:38,710  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1631061732_0008_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:38,712  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:38,712  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1631061732_0008_m_000000_0 is allowed to commit now
2024-04-24T10:39:38,714  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Committing TaskAttempt:attempt_local1631061732_0008_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:38,714  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:39:38,716  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1631061732_0008_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:39:38,716  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:39:38,717  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1631061732_0008_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:39:38,718  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Discarding all cleanup for TaskAttemptID:attempt_local1631061732_0008_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.8324568281482818/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:38,718  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:38,719  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1631061732_0008_m_000000_0' done.
2024-04-24T10:39:38,719  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1631061732_0008_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=34859
		FILE: Number of bytes written=5159811
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=494
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1040711680
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:38,719  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1631061732_0008_m_000000_0
2024-04-24T10:39:38,719  INFO [Thread-762] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:38,730  INFO [Thread-762] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,730  INFO [Thread-762] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,736  INFO [Thread-762] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,736  INFO [Thread-762] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,756  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:38,756  INFO [Thread-799] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:38,757  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:38,763  INFO [Thread-799] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,763  INFO [Thread-799] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,764  INFO [Thread-799] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:38,766  INFO [Thread-799] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,766  INFO [Thread-799] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,774  INFO [Thread-799] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:38,774  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local293788078_0009_m_000000_0
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,778  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,778  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,779  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,783  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,783  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,786  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,786  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,787  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:38,787  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 342
Input split[0]:
   Length = 342
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:38,788  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:38,788  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data:0+342
2024-04-24T10:39:38,792  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,792  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,792  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,792  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,793  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,793  INFO [Thread-762] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,028509306254604683/_temporary/0/_temporary/attempt_local293788078_0009_m_000000_0/part-m-00000
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:38,795  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:38,795  INFO [Thread-762] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:38,796  INFO [Thread-762] metastore.HMSHandler: 14: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:38,796  INFO [Thread-762] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:38,797  INFO [Thread-762] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8d6108a, with PersistenceManager: null will be shutdown
2024-04-24T10:39:38,797  INFO [Thread-762] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8d6108a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2cb20007 created in the thread with id: 821
2024-04-24T10:39:38,799  INFO [Thread-762] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@8d6108a from thread id: 821
2024-04-24T10:39:38,799  INFO [Thread-762] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:38,799  INFO [Thread-762] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:38,800  INFO [Thread-762] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,802  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,803  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,803  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,807  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@6a138575
2024-04-24T10:39:38,809  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:38,809  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:38,810  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:39:38,814  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:38,825  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local293788078_0009_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:38,827  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,827  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,827  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,827  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,827  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,828  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,828  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,829  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,829  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:38,829  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local293788078_0009_m_000000_0 is allowed to commit now
2024-04-24T10:39:38,832  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:38,832  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:38,834  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local293788078_0009_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,028509306254604683
2024-04-24T10:39:38,834  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:38,834  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local293788078_0009_m_000000_0' done.
2024-04-24T10:39:38,835  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local293788078_0009_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=37529
		FILE: Number of bytes written=5579977
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=488
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1040711680
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:38,835  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local293788078_0009_m_000000_0
2024-04-24T10:39:38,835  INFO [Thread-799] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,849  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,849  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,850  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,859  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,859  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,860  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,860  INFO [Thread-762] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_date has new partitions [{dt=2016-07-14 15:10:15}, {dt=2016-07-15 18:54:55}].
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,871  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,872  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,872  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,881  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,886  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,886  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,887  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,887  INFO [Thread-799] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,888  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:38,888  INFO [Thread-799] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:38,888  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:38,889  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:38,889  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:38,889  INFO [Thread-799] metastore.HMSHandler: 15: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:38,889  INFO [Thread-799] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:38,890  INFO [Thread-799] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@15fc7ac6, with PersistenceManager: null will be shutdown
2024-04-24T10:39:38,890  INFO [Thread-799] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@15fc7ac6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@786b5f67 created in the thread with id: 860
2024-04-24T10:39:38,895  INFO [Thread-799] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@15fc7ac6 from thread id: 860
2024-04-24T10:39:38,895  INFO [Thread-799] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:38,895  INFO [Thread-799] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:38,895  INFO [Thread-799] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,898  INFO [Thread-799] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_complex newtbl=junit_unparted_complex	
2024-04-24T10:39:38,899  INFO [Thread-762] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:38,899  INFO [Thread-762] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:38,921  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:38,921  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -35
2024-04-24T10:39:38,921  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:38,921  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -36
2024-04-24T10:39:38,928  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:38,932  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:39:38,932  INFO [HMSHandler #9] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:39:38,932  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 476
2024-04-24T10:39:38,933  INFO [HMSHandler #9] utils.MetaStoreServerUtils: Updated size to 469
2024-04-24T10:39:38,936  INFO [Thread-799] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_complex
2024-04-24T10:39:38,936  INFO [Thread-799] utils.MetaStoreServerUtils: Updated size of table junit_unparted_complex to 2660
2024-04-24T10:39:38,945  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1319328820_0010
2024-04-24T10:39:38,945  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:38,945  INFO [Thread-799] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:38,994  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:38,995  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:38,995  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:38,995  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:38,995  WARN [Thread-799] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:38,995  INFO [Thread-799] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:38,996  INFO [Thread-799] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:38,997  INFO [Thread-799] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,003  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:39,003  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1631061732_0008
2024-04-24T10:39:39,003  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases E,F
2024-04-24T10:39:39,003  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:39:39,003  INFO [Thread-847] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:39,004  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local293788078_0009
2024-04-24T10:39:39,004  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases D
2024-04-24T10:39:39,004  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:39:39,005  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1319328820_0010
2024-04-24T10:39:39,005  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,B,B2,C,C2
2024-04-24T10:39:39,005  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:39:39,008  INFO [main] mapReduceLayer.MapReduceLauncher: 33% complete
2024-04-24T10:39:39,008  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1631061732_0008,job_local293788078_0009,job_local1319328820_0010]
2024-04-24T10:39:39,012  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,012  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,015  INFO [Thread-762] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,016  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,016  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,019  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,019  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,023  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,023  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,026  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,026  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,028  INFO [Thread-847] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:39,029  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,029  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,037  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,038  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,046  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,046  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,053  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,053  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,064  INFO [Thread-847] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,064  INFO [Thread-847] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,073  INFO [Thread-847] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:39,074  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1319328820_0010_m_000000_0
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,076  WARN [Thread-762] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,076  INFO [Thread-762] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,077  INFO [Thread-762] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,079  INFO [Thread-762] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,082  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,082  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,086  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,086  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,089  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,089  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,093  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,093  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,097  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,097  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,100  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,100  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,102  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,102  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,104  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,104  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,106  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,106  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,108  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,108  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,108  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:39,109  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 54
Input split[0]:
   Length = 54
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:39,110  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:39,110  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data:0+54
2024-04-24T10:39:39,111  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:39,111  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:39,113  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:39:39,116  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,116  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,7953665655804875/_temporary/0/_temporary/attempt_local1319328820_0010_m_000000_0/part-m-00000
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:39,118  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:39,128  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@2e73ea94
2024-04-24T10:39:39,134  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,134  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,36608618363295164/_temporary/0/_temporary/attempt_local1319328820_0010_m_000000_0/part-m-00000
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:39,136  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:39,146  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@799dd62a
2024-04-24T10:39:39,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,5366803988030945/_temporary/0/_temporary/attempt_local1319328820_0010_m_000000_0/part-m-00000
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:39,152  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:39,162  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@1cb17b72
2024-04-24T10:39:39,166  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,166  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,3975427307779329/bkt=0/_temporary/0/_temporary/attempt_local1319328820_0010_m_000000_0/part-m-00000
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:39,167  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:39,168  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:39,177  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@7c4c0ed3
2024-04-24T10:39:39,181  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,181  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,1856691370633018/bkt=1/_temporary/0/_temporary/attempt_local1319328820_0010_m_000000_0/part-m-00000
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:39,183  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:39,192  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@42772db1
2024-04-24T10:39:39,216  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:39,216  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1319328820_0010_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:39,217  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,217  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,223  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:39,223  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1319328820_0010_m_000000_0 is allowed to commit now
2024-04-24T10:39:39,224  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,224  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,225  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1319328820_0010_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,7953665655804875
2024-04-24T10:39:39,226  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,226  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,227  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1319328820_0010_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,36608618363295164
2024-04-24T10:39:39,229  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,229  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,230  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1319328820_0010_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,5366803988030945
2024-04-24T10:39:39,231  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,231  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,233  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1319328820_0010_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,3975427307779329/bkt=0
2024-04-24T10:39:39,234  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:39,234  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:39,235  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1319328820_0010_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,1856691370633018/bkt=1
2024-04-24T10:39:39,236  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:39,236  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1319328820_0010_m_000000_0' done.
2024-04-24T10:39:39,236  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1319328820_0010_m_000000_0: Counters: 20
	File System Counters
		FILE: Number of bytes read=43014
		FILE: Number of bytes written=6200195
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=9
		Map output records=0
		Input split bytes=486
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1022885888
	MultiStoreCounters
		Output records in _0_junit_unparted_basic=9
		Output records in _1_junit_specific_size=9
		Output records in _2_junit_specific_db.junit_specific_size2=9
		Output records in _3_junit_parted_basic=3
		Output records in _4_junit_parted_basic=6
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:39,236  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1319328820_0010_m_000000_0
2024-04-24T10:39:39,236  INFO [Thread-847] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,276  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,276  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,277  INFO [Thread-847] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:39,278  INFO [Thread-847] metastore.HMSHandler: 16: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:39,278  INFO [Thread-847] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:39,278  INFO [Thread-847] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27e55748, with PersistenceManager: null will be shutdown
2024-04-24T10:39:39,278  INFO [Thread-847] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27e55748, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@44e5b9ee created in the thread with id: 912
2024-04-24T10:39:39,280  INFO [Thread-847] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@27e55748 from thread id: 912
2024-04-24T10:39:39,280  INFO [Thread-847] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:39,280  INFO [Thread-847] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:39,280  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,282  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_basic newtbl=junit_unparted_basic	
2024-04-24T10:39:39,294  INFO [Thread-847] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_basic
2024-04-24T10:39:39,294  INFO [Thread-847] utils.MetaStoreServerUtils: Updated size of table junit_unparted_basic to 647
2024-04-24T10:39:39,300  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,337  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,337  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,338  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,339  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,382  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,382  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,383  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,385  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_specific_size newtbl=junit_specific_size	
2024-04-24T10:39:39,397  INFO [Thread-847] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size
2024-04-24T10:39:39,397  INFO [Thread-847] utils.MetaStoreServerUtils: Updated size of table junit_specific_size to 647
2024-04-24T10:39:39,403  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,438  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,438  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,439  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,440  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,482  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,483  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,483  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,483  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,485  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.junit_specific_db.junit_specific_size2 newtbl=junit_specific_size2	
2024-04-24T10:39:39,497  INFO [Thread-847] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size2
2024-04-24T10:39:39,497  INFO [Thread-847] utils.MetaStoreServerUtils: Updated size of table junit_specific_size2 to 647
2024-04-24T10:39:39,503  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,508  INFO [main] mapReduceLayer.MapReduceLauncher: 83% complete
2024-04-24T10:39:39,508  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1319328820_0010]
2024-04-24T10:39:39,509  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,512  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,514  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,529  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,531  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,533  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,543  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,543  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,544  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,545  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,588  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,588  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,589  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,626  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,627  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,627  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,627  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,627  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,627  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=0}].
2024-04-24T10:39:39,628  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[0]	
2024-04-24T10:39:39,631  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:39,631  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:39,640  INFO [HMSHandler #10] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:39:39,640  INFO [HMSHandler #10] utils.MetaStoreServerUtils: Updated size to 597
2024-04-24T10:39:39,668  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,705  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,706  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,706  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,707  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,747  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,748  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,748  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,752  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=1}].
2024-04-24T10:39:39,753  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[1]	
2024-04-24T10:39:39,756  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:39,756  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:39,762  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:39:39,763  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 635
2024-04-24T10:39:39,791  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:39,825  WARN [Thread-847] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:39,825  INFO [Thread-847] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:39,826  INFO [Thread-847] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:39,827  INFO [Thread-847] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:40,010  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,013  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,014  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,017  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:40,017  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_unparted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:40,018  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:40,018  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_size does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:40,018  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:40,018  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_db.junit_specific_size2 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:40,019  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:40,019  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:40,019  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:40,019  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:40,019  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:39:40,020  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:39:38	2024-04-24 10:39:40	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1319328820_0010	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,B,B2,C,C2	MULTI_QUERY,MAP_ONLY	junit_unparted_basic,junit_specific_size,junit_specific_db.junit_specific_size2,junit_parted_basic,junit_parted_basic,
job_local1631061732_0008	1	0	n/a	n/a	n/a	n/a	0	0	0	0	E,F	MAP_ONLY	junit_parted_date,
job_local293788078_0009	1	0	n/a	n/a	n/a	n/a	0	0	0	0	D	MAP_ONLY	junit_unparted_complex,

Input(s):
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data"
Successfully read 9 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data"
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data"

Output(s):
Successfully stored 2 records in: "junit_unparted_complex"
Successfully stored 9 records in: "junit_unparted_basic"
Successfully stored 9 records in: "junit_specific_size"
Successfully stored 9 records in: "junit_specific_db.junit_specific_size2"
Successfully stored 3 records in: "junit_parted_basic"
Successfully stored 6 records in: "junit_parted_basic"
Successfully stored 2 records in: "junit_parted_date"

Counters:
Total records written : 40
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local293788078_0009
job_local1319328820_0010
job_local1631061732_0008


2024-04-24T10:39:40,021  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,023  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,026  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,028  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,032  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,034  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:40,035  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:39:40,056  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:39:40,056  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-test.pig-2af0a00b-2220-4eaa-85c7-c6a682f58a64
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:40,077  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:40,078  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:40,080  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,089  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:40,103  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:40,104  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:40,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:40,107  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,116  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
X: {c_boolean: boolean,c_tinyint: int,c_smallint: int,c_int: int,c_bigint: long,c_float: float,c_double: double,c_decimal: bigdecimal,c_string: chararray,c_char: chararray,c_varchar: chararray,c_binary: bytearray,c_date: datetime,c_timestamp: datetime}
2024-04-24T10:39:40,117  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_basic
2024-04-24T10:39:40,118  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:40,125  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,126  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,126  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=9}
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.009 seconds
2024-04-24T10:39:40,126  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_basic
2024-04-24T10:39:40,126  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,127  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:40,134  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,135  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:40,143  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:40,175  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,176  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=40, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8}
2024-04-24T10:39:40,176  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.049 seconds
2024-04-24T10:39:40,176  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_complex
2024-04-24T10:39:40,177  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:40,184  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,184  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,185  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,185  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,185  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=7, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:40,185  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.009 seconds
2024-04-24T10:39:40,185  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,185  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,185  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_complex
2024-04-24T10:39:40,185  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:40,194  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,194  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:40,203  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,203  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:40,232  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,232  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=8, dropTable_(String, String, boolean, boolean, boolean)=38}
2024-04-24T10:39:40,233  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.047 seconds
2024-04-24T10:39:40,233  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_parted_basic
2024-04-24T10:39:40,234  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:40,243  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,243  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,243  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,243  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,243  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=10}
2024-04-24T10:39:40,243  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.01 seconds
2024-04-24T10:39:40,243  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,243  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,244  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_parted_basic
2024-04-24T10:39:40,244  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,244  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:40,253  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,253  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:40,262  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,262  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:40,329  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,329  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=76, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9}
2024-04-24T10:39:40,329  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.086 seconds
2024-04-24T10:39:40,330  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_specific_size
2024-04-24T10:39:40,330  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:40,338  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,338  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,338  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,338  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,338  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8, flushCache_()=0}
2024-04-24T10:39:40,339  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.008 seconds
2024-04-24T10:39:40,339  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,339  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,339  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_specific_size
2024-04-24T10:39:40,339  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,339  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:40,347  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,348  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:40,356  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,356  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:40,385  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,385  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=36, getTable_(GetTableRequest)=9}
2024-04-24T10:39:40,385  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.046 seconds
2024-04-24T10:39:40,385  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_parted_date
2024-04-24T10:39:40,386  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:40,394  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,394  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,394  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,394  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,394  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=8}
2024-04-24T10:39:40,395  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.009 seconds
2024-04-24T10:39:40,395  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,395  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,395  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_parted_date
2024-04-24T10:39:40,395  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,395  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:40,404  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,404  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:40,413  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,413  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:40,482  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,482  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, dropTable_(String, String, boolean, boolean, boolean)=78}
2024-04-24T10:39:40,483  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.087 seconds
2024-04-24T10:39:40,483  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:40,483  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,492  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,493  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,493  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=10, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.01 seconds
2024-04-24T10:39:40,493  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:40,493  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,494  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,503  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,503  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,513  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:40,513  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:40,545  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:40,545  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=10, isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=42}
2024-04-24T10:39:40,545  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.052 seconds
2024-04-24T10:39:40,546  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop database if exists junit_specific_db cascade
2024-04-24T10:39:40,546  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:40,549  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:39:40,556  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:39:40,556  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:40,556  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:40,556  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:40,556  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTables_(String, String)=7, getTableObjectsByName_(String, List)=1, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:40,557  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 0.01 seconds
2024-04-24T10:39:40,557  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:40,557  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:40,557  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4): drop database if exists junit_specific_db cascade
2024-04-24T10:39:40,557  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:40,557  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:40,559  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:40,559  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:40,563  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:40,568  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:39:40,573  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:40,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:40,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:40,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:40,583  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:40,587  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:40,592  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:40,608  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:40,608  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:40,610 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:42,611  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:42,615  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:42,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:42,618  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:42,619  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:42,620  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:42,625  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:42,632  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:42,632  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:42,633 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:44,634  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:44,639  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:44,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:44,642  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:44,643  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:44,644  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:44,648  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:44,655  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:44,655  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:44,656 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (4).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:46,657  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:46,660  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:46,661  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:46,663  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:46,664  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:46,665  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:46,676  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:39:46,703  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:46,710  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:46,710  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:46,712  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:39:46,712 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:46,713  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,713  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropDatabase_(String, boolean, boolean, boolean)=6156}
2024-04-24T10:39:46,713  INFO [main] metadata.Hive: Total time spent in this metastore function was greater than 1000ms : dropDatabase_(String, boolean, boolean, boolean)=6156
2024-04-24T10:39:46,713  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103937_841bf050-dbf9-437e-8361-2f4a48c452c4); Time taken: 6.156 seconds
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:46,747  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:46,750  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:46,750  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:46,750  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5efdf532 will be shutdown
2024-04-24T10:39:46,751  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3530930 created in the thread with id: 1
2024-04-24T10:39:46,771  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 5bb33cf1-c4d2-40d6-be1c-87ca990c9d17
2024-04-24T10:39:46,771  INFO [main] SessionState: Hive Session ID = 5bb33cf1-c4d2-40d6-be1c-87ca990c9d17
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:46,771  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:39:46,777  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/5bb33cf1-c4d2-40d6-be1c-87ca990c9d17
2024-04-24T10:39:46,779  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/5bb33cf1-c4d2-40d6-be1c-87ca990c9d17
2024-04-24T10:39:46,782  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/5bb33cf1-c4d2-40d6-be1c-87ca990c9d17/_tmp_space.db
2024-04-24T10:39:46,782  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,783  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,784  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,784  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,784  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,784  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:39:46,784  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,784  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,784  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,784  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,785  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,785  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,786  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,787  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,787  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,787  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,788  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,788  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,788  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,789  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5bb33cf1-c4d2-40d6-be1c-87ca990c9d17, clientType=HIVECLI]
2024-04-24T10:39:46,789  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:39:46,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:46,789  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7d3a7c31, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3530930 will be shutdown
2024-04-24T10:39:46,789  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:46,789  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -37
2024-04-24T10:39:46,790  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:46,791  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:46,791  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:46,792  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe, with PersistenceManager: null will be shutdown
2024-04-24T10:39:46,792  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@74386944 created in the thread with id: 1
2024-04-24T10:39:46,793  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe from thread id: 1
2024-04-24T10:39:46,793  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:46,793  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:46,793  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:39:46,795  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:46,795  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:46,795  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@74386944 will be shutdown
2024-04-24T10:39:46,795  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5460c665 created in the thread with id: 1
2024-04-24T10:39:46,797  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:46,797  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:46,797  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,798  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,798  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,798  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,798  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,798  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:39:46,798  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.01 seconds
2024-04-24T10:39:46,799  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,799  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,799  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,799  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,799  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:39:46,799  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:39:46,799  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b7cf3fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5460c665 will be shutdown
2024-04-24T10:39:46,799  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:46,799  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -38
2024-04-24T10:39:46,801  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:46,802  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:46,802  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:46,803  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e, with PersistenceManager: null will be shutdown
2024-04-24T10:39:46,803  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@378c66fb created in the thread with id: 1
2024-04-24T10:39:46,804  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e from thread id: 1
2024-04-24T10:39:46,804  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:46,804  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:46,805  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{rawDataSize=0, totalSize=0, transactional=false, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFiles=0, numRows=0, bucketing_version=2, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:46,809  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:39:46,845  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,845  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=41}
2024-04-24T10:39:46,845  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.046 seconds
2024-04-24T10:39:46,846  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,846  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,848  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,848  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,848  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,848  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:46,848  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,848  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,848  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,848  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,849  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,849  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,850  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,852  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,852  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,852  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.004 seconds
2024-04-24T10:39:46,853  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,853  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,854  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,854  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:39:46,854  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,856  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,856  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,856  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,856  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,856  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,858  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, numFilesErasureCoded=0, numRows=0, transactional=false, rawDataSize=0, bucketing_version=2, totalSize=0, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:46,863  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:39:46,879  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,879  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=22}
2024-04-24T10:39:46,879  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.023 seconds
2024-04-24T10:39:46,880  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,880  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,881  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,881  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,881  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,881  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-24T10:39:46,882  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.001 seconds
2024-04-24T10:39:46,882  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,882  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,882  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,882  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,882  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,884  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,885  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,885  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,885  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,886  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,886  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,886  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,886  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:39:46,886  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,888  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,888  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,888  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,888  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,888  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:39:46,888  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,888  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,889  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,889  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,889  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,890  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:46,895  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:39:46,909  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,910  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=20}
2024-04-24T10:39:46,910  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.02 seconds
2024-04-24T10:39:46,910  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,911  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,912  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,912  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,912  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,912  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,913  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,913  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,913  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,913  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,913  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,913  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,915  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,916  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,916  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:46,916  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,916  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,917  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,917  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,917  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:39:46,917  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,919  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,919  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,919  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,919  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,919  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:39:46,919  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,920  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,920  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,920  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,920  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,921  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numRows=0, transactional=false, numFiles=0, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, rawDataSize=0, totalSize=0, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:46,927  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:39:46,942  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,943  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=21, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,943  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.022 seconds
2024-04-24T10:39:46,943  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create database if not exists junit_specific_db
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,944  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,944  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, flushCache_()=0}
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.001 seconds
2024-04-24T10:39:46,944  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create database if not exists junit_specific_db
2024-04-24T10:39:46,944  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,944  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:39:46,945  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:39:46,949  INFO [main] metastore.HMSHandler: Creating database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:46,949  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:46,951  INFO [main] metastore.HMSHandler: Created database path in external directory file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:39:46,954  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,954  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createDatabase_(Database)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,954  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.01 seconds
2024-04-24T10:39:46,954  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use junit_specific_db
2024-04-24T10:39:46,955  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:46,956  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,956  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,956  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,956  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:46,957  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,957  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,957  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,957  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use junit_specific_db
2024-04-24T10:39:46,957  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,957  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:46,959  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:46,960  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,960  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:46,961  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,961  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,961  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,961  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,962  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:39:46,962  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:46,963  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,963  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,963  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,963  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,963  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:46,964  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.002 seconds
2024-04-24T10:39:46,964  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,964  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,964  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,964  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,965  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, rawDataSize=0, bucketing_version=2, numRows=0, numFilesErasureCoded=0, numFiles=0, transactional=false, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:46,971  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2
2024-04-24T10:39:46,984  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,984  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=19, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,984  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.02 seconds
2024-04-24T10:39:46,985  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,985  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,986  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,986  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,986  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,987  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:46,987  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.001 seconds
2024-04-24T10:39:46,987  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,987  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,987  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): use default
2024-04-24T10:39:46,987  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,989  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,990  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:46,990  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:39:46,991  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,991  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,992  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,992  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:46,992  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_date position=13
2024-04-24T10:39:46,992  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:46,993  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:46,993  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:46,993  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:46,994  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:46,994  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getDatabase_(String)=1}
2024-04-24T10:39:46,994  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:46,994  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:46,994  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:46,994  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_parted_date(b string) partitioned by (dt date) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:39:46,994  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:46,995  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_date, dbName:default, owner:alex, createTime:1713980386, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:date, comment:null)], parameters:{transactional=false, bucketing_version=2}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:47,001  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date
2024-04-24T10:39:47,014  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:47,014  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, createTable_(Table)=19}
2024-04-24T10:39:47,014  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.02 seconds
2024-04-24T10:39:47,015  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:39:47,016  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:47,016  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:39:47,016  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_alltypes position=13
2024-04-24T10:39:47,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:39:47,018  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:47,018  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:47,018  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:47,018  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:47,018  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2, flushCache_()=0}
2024-04-24T10:39:47,018  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.003 seconds
2024-04-24T10:39:47,019  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:47,019  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:47,019  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): create table junit_unparted_alltypes( c_boolean boolean, c_tinyint tinyint, c_smallint smallint, c_int int, c_bigint bigint, c_float float, c_double double, c_decimal decimal(5,2), c_string string, c_char char(10), c_varchar varchar(20), c_binary binary, c_date date, c_timestamp timestamp)ROW FORMAT DELIMITED FIELDS TERMINATED BY '	' STORED AS TEXTFILE
2024-04-24T10:39:47,019  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:47,020  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_alltypes, dbName:default, owner:alex, createTime:1713980387, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c_boolean, type:boolean, comment:null), FieldSchema(name:c_tinyint, type:tinyint, comment:null), FieldSchema(name:c_smallint, type:smallint, comment:null), FieldSchema(name:c_int, type:int, comment:null), FieldSchema(name:c_bigint, type:bigint, comment:null), FieldSchema(name:c_float, type:float, comment:null), FieldSchema(name:c_double, type:double, comment:null), FieldSchema(name:c_decimal, type:decimal(5,2), comment:null), FieldSchema(name:c_string, type:string, comment:null), FieldSchema(name:c_char, type:char(10), comment:null), FieldSchema(name:c_varchar, type:varchar(20), comment:null), FieldSchema(name:c_binary, type:binary, comment:null), FieldSchema(name:c_date, type:date, comment:null), FieldSchema(name:c_timestamp, type:timestamp, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, field.delim=	}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFilesErasureCoded=0, totalSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"c_bigint":"true","c_binary":"true","c_boolean":"true","c_char":"true","c_date":"true","c_decimal":"true","c_double":"true","c_float":"true","c_int":"true","c_smallint":"true","c_string":"true","c_timestamp":"true","c_tinyint":"true","c_varchar":"true"}}, numRows=0, bucketing_version=2, rawDataSize=0, numFiles=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:39:47,025  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_alltypes
2024-04-24T10:39:47,047  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:47,047  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=27, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:47,047  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.028 seconds
2024-04-24T10:39:47,048  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:39:47,048  INFO [main] parse.LoadSemanticAnalyzer: Starting caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:47,048  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:47,060  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,062  INFO [main] parse.LoadSemanticAnalyzer: Ending caching scope for: alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802
2024-04-24T10:39:47,062  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:47,062  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:47,062  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:47,062  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=12}
2024-04-24T10:39:47,062  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.014 seconds
2024-04-24T10:39:47,063  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:47,063  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:47,063  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): load data local inpath '/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data' into table junit_unparted_alltypes
2024-04-24T10:39:47,063  INFO [main] ql.Driver: Starting task [Stage-0:MOVE] in serial mode
Loading data to table default.junit_unparted_alltypes
2024-04-24T10:39:47,063  INFO [main] exec.Task: Loading data to table default.junit_unparted_alltypes from file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/alltypes.input.data
2024-04-24T10:39:47,063  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:47,074  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,075  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:47,085  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,092  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:39:47,092  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:39:47,116  INFO [main] ql.Driver: Starting task [Stage-1:STATS] in serial mode
2024-04-24T10:39:47,117  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:47,127  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,127  INFO [main] stats.BasicStatsTask: Executing stats task
2024-04-24T10:39:47,127  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:47,137  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,138  WARN [main] metadata.Hive: Cannot get a table snapshot for junit_unparted_alltypes
2024-04-24T10:39:47,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_alltypes newtbl=junit_unparted_alltypes	
2024-04-24T10:39:47,169  INFO [main] stats.BasicStatsTask: Table default.junit_unparted_alltypes stats: [numFiles=1, numRows=0, totalSize=329, rawDataSize=0, numFilesErasureCoded=0]
2024-04-24T10:39:47,169  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:47,170  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=42, alter_table_(String, String, String, Table, EnvironmentContext, String)=55}
2024-04-24T10:39:47,170  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.106 seconds
2024-04-24T10:39:47,192  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:39:47,193  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-test.pig-2af0a00b-2220-4eaa-85c7-c6a682f58a64
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,361  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,361  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,362  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:47,363  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,368  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:47,376  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,395  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,395  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,396  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,411  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,411  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,412  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.conf.restricted.list=dummy.config.value, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:47,413  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,415  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:47,423  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,442  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,442  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,443  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,459  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,459  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,460  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:47,461  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,463  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:39:47,475  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,497  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,497  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,498  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,513  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,513  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,514  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.stats.key.prefix.reserve.length=0, hive.metastore.client.cache.enabled=true, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:47,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:47,526  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,560  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:39:47,560  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -39
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,561  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,562  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,562  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,578  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,578  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,579  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, hive.stats.column.autogather=true, javax.jdo.option.ConnectionUserName=APP, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:47,580  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,582  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:47,590  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,608  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,608  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,608  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,608  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,609  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,609  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,609  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,625  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,626  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,626  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,626  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,626  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,626  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,627  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.stats.column.autogather=true, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.query.results.cache.enabled=false, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.use.lrfu=true, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.scheduled.queries.executor.enabled=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, hive.strict.timestamp.conversion=false, iceberg.hive.keep.stats=true}
2024-04-24T10:39:47,628  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,630  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:47,638  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,659  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,660  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,660  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,660  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,660  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,660  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,660  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,679  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,680  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,680  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,680  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,680  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-24T10:39:47,681  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, test.property1=value1, javax.jdo.option.ConnectionUserName=APP, hive.stats.column.autogather=true, hive.llap.io.cache.orc.alloc.max=2097152, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, test.data.scripts=${hive.root}/data/scripts, datanucleus.schema.autoCreateAll=true, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, hive.exec.mode.local.auto=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.materializedview.rewriting=true, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.in.test=true, hive.support.concurrency=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.scheduled.queries.executor.enabled=false, hive.llap.cache.allow.synthetic.fileid=true, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-24T10:39:47,682  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,684  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:47,692  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,712  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,712  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,713  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,716  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-24T10:39:47,723  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-24T10:39:47,735  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-24T10:39:47,739  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 8
2024-04-24T10:39:47,740  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 map-only splittees.
2024-04-24T10:39:47,740  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 5 out of total 6 MR operators.
2024-04-24T10:39:47,740  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 3
2024-04-24T10:39:47,746  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:47,748  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:47,748  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:47,749  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:39:47,749  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:47,749  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:47,749  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980387749-0
2024-04-24T10:39:47,755  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:47,755  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:47,756  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-24T10:39:47,756  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:47,756  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:47,756  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980387756-0
2024-04-24T10:39:47,762  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-24T10:39:47,763  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-24T10:39:47,763  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-24T10:39:47,764  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-24T10:39:47,764  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-24T10:39:47,764  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1713980387764-0
2024-04-24T10:39:47,771  INFO [main] mapReduceLayer.MapReduceLauncher: 3 map-reduce job(s) waiting for submission.
2024-04-24T10:39:47,774  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,791  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,791  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,792  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:47,793  INFO [JobControl] metastore.HMSHandler: 17: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:47,793  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:47,793  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7f5e5452, with PersistenceManager: null will be shutdown
2024-04-24T10:39:47,793  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7f5e5452, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@247b94fe created in the thread with id: 1053
2024-04-24T10:39:47,795  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7f5e5452 from thread id: 1053
2024-04-24T10:39:47,795  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:47,795  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:47,795  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,808  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:47,812  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:47,813  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:47,814  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:47,814  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:47,814  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:47,831  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:47,844  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local705690793_0011
2024-04-24T10:39:47,844  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:47,897  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:47,898  INFO [Thread-1002] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:47,898  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:47,904  INFO [Thread-1002] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,904  INFO [Thread-1002] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,905  INFO [Thread-1002] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:47,907  INFO [Thread-1002] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,907  INFO [Thread-1002] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,915  INFO [Thread-1002] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:47,915  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local705690793_0011_m_000000_0
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:47,917  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:47,917  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:47,918  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:47,922  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,922  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,925  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:47,926  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,926  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,926  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:47,927  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 342
Input split[0]:
   Length = 342
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:47,928  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:47,928  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data:0+342
2024-04-24T10:39:47,930  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:47,931  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:47,931  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:47,931  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:47,932  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:47,932  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,932  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,5515454099612607/_temporary/0/_temporary/attempt_local705690793_0011_m_000000_0/part-m-00000
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:47,934  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:47,945  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@58dd94a5
2024-04-24T10:39:47,948  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:47,948  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:47,949  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:39:47,950  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:47,956  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:47,966  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1597369099_0012
2024-04-24T10:39:47,966  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:47,971  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local705690793_0011_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:47,973  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,973  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,974  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:47,974  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local705690793_0011_m_000000_0 is allowed to commit now
2024-04-24T10:39:47,975  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:47,975  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:47,976  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local705690793_0011_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex/_SCRATCH0,5515454099612607
2024-04-24T10:39:47,977  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:47,977  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local705690793_0011_m_000000_0' done.
2024-04-24T10:39:47,977  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local705690793_0011_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=46671
		FILE: Number of bytes written=7019965
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=488
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1001390080
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:47,977  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local705690793_0011_m_000000_0
2024-04-24T10:39:47,977  INFO [Thread-1002] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:48,015  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:48,015  INFO [Thread-1032] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:48,016  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,021  INFO [Thread-1032] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:48,023  INFO [Thread-1032] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:48,023  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1597369099_0012_m_000000_0
2024-04-24T10:39:48,023  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,024  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,024  INFO [Thread-1002] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,025  INFO [Thread-1002] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:48,026  INFO [Thread-1002] metastore.HMSHandler: 18: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:48,026  INFO [Thread-1002] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:48,026  INFO [Thread-1002] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@663870dc, with PersistenceManager: null will be shutdown
2024-04-24T10:39:48,026  INFO [Thread-1002] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@663870dc, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4165aced created in the thread with id: 1073
2024-04-24T10:39:48,027  INFO [Thread-1002] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@663870dc from thread id: 1073
2024-04-24T10:39:48,027  INFO [Thread-1002] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:48,027  INFO [Thread-1002] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:48,028  INFO [Thread-1002] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,030  INFO [Thread-1002] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_complex newtbl=junit_unparted_complex	
2024-04-24T10:39:48,032  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:48,032  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 65
Input split[0]:
   Length = 65
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:48,034  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:48,034  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data:0+65
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,035  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,036  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,036  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,039  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:48,039  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:48,040  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:39:48,043  INFO [Thread-1002] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_complex
2024-04-24T10:39:48,043  INFO [Thread-1002] utils.MetaStoreServerUtils: Updated size of table junit_unparted_complex to 2660
2024-04-24T10:39:48,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,045  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,050  INFO [Thread-1002] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,056  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,056  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-14 15%3A10%3A15/_temporary/0/_temporary/attempt_local1597369099_0012_m_000000_0/part-m-00000
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,057  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,057  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,058  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,058  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,068  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@57f2388d
2024-04-24T10:39:48,073  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,073  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,077  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,077  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,078  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,081  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,081  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-15 18%3A54%3A55/_temporary/0/_temporary/attempt_local1597369099_0012_m_000000_0/part-m-00000
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,082  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,083  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,083  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,083  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,093  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@339b4a14
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,093  WARN [Thread-1002] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,094  INFO [Thread-1002] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,094  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:48,094  INFO [Thread-1002] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,096  INFO [Thread-1002] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,100  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Registering committer for TaskAttemptID:attempt_local1597369099_0012_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:48,100  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1597369099_0012_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:48,103  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:48,103  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1597369099_0012_m_000000_0 is allowed to commit now
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,103  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,104  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,104  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,105  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Committing TaskAttempt:attempt_local1597369099_0012_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:48,105  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:39:48,106  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1597369099_0012_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-14 15%3A10%3A15
2024-04-24T10:39:48,106  INFO [LocalJobRunner Map Task Executor #0] mapreduce.DynamicPartitionFileRecordWriterContainer: Committing task-attempt for file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:39:48,107  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1597369099_0012_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=2016-07-15 18%3A54%3A55
2024-04-24T10:39:48,108  INFO [LocalJobRunner Map Task Executor #0] mapreduce.TaskCommitContextRegistry: Discarding all cleanup for TaskAttemptID:attempt_local1597369099_0012_m_000000_0@file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_date/_DYN0.47004109438264785/dt=__HIVE_DEFAULT_PARTITION__
2024-04-24T10:39:48,108  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:48,109  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1597369099_0012_m_000000_0' done.
2024-04-24T10:39:48,109  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1597369099_0012_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=48253
		FILE: Number of bytes written=7433322
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=494
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1001390080
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:48,109  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1597369099_0012_m_000000_0
2024-04-24T10:39:48,109  INFO [Thread-1032] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:48,116  INFO [Thread-1032] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,116  INFO [Thread-1032] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,121  INFO [Thread-1032] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,121  INFO [Thread-1032] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,129  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,130  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,130  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,130  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,138  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-24T10:39:48,143  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-24T10:39:48,144  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:48,144  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-24T10:39:48,144  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-24T10:39:48,145  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-24T10:39:48,163  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,169  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,170  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,170  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,170  INFO [Thread-1032] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,171  INFO [Thread-1032] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:48,172  INFO [Thread-1032] metastore.HMSHandler: 19: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:48,172  INFO [Thread-1032] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:48,172  INFO [Thread-1032] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@50478edb, with PersistenceManager: null will be shutdown
2024-04-24T10:39:48,172  INFO [Thread-1032] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@50478edb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3bd7f840 created in the thread with id: 1105
2024-04-24T10:39:48,174  INFO [Thread-1032] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@50478edb from thread id: 1105
2024-04-24T10:39:48,174  INFO [Thread-1032] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:48,174  INFO [Thread-1032] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:48,175  INFO [Thread-1032] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,179  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local955960542_0013
2024-04-24T10:39:48,179  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-24T10:39:48,226  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,227  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,227  INFO [Thread-1032] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_date has new partitions [{dt=2016-07-14 15:10:15}, {dt=2016-07-15 18:54:55}].
2024-04-24T10:39:48,240  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-24T10:39:48,241  INFO [Thread-1073] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-24T10:39:48,247  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,247  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,250  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,250  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,254  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,254  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,258  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,258  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,261  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,261  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,262  INFO [Thread-1073] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-24T10:39:48,264  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,264  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,265  INFO [Thread-1032] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:48,266  INFO [Thread-1032] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:48,273  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,273  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,273  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local705690793_0011
2024-04-24T10:39:48,273  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases D
2024-04-24T10:39:48,273  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: D[11,4],D[-1,-1] C:  R: 
2024-04-24T10:39:48,275  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1597369099_0012
2024-04-24T10:39:48,275  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases E,F
2024-04-24T10:39:48,275  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: E[13,4],F[-1,-1] C:  R: 
2024-04-24T10:39:48,276  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local955960542_0013
2024-04-24T10:39:48,276  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,B,B2,C,C2
2024-04-24T10:39:48,276  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:39:48,278  INFO [main] mapReduceLayer.MapReduceLauncher: 33% complete
2024-04-24T10:39:48,278  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local705690793_0011,job_local1597369099_0012,job_local955960542_0013]
2024-04-24T10:39:48,280  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:39:48,281  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_date
2024-04-24T10:39:48,281  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 476
2024-04-24T10:39:48,281  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 469
2024-04-24T10:39:48,283  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,283  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,292  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,292  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,303  INFO [Thread-1073] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,303  INFO [Thread-1073] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,314  INFO [Thread-1073] mapred.LocalJobRunner: Waiting for map tasks
2024-04-24T10:39:48,314  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local955960542_0013_m_000000_0
2024-04-24T10:39:48,322  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,322  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,327  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,327  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,331  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,331  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,336  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,336  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,340  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,340  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,344  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,344  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,346  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,346  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,349  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,349  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,351  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,351  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,353  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,353  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,353  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-24T10:39:48,354  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 54
Input split[0]:
   Length = 54
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-24T10:39:48,356  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-24T10:39:48,356  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data:0+54
2024-04-24T10:39:48,357  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-24T10:39:48,357  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
2024-04-24T10:39:48,360  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],B2[6,5],B[5,4],C2[9,5],C[8,4] C:  R: 
2024-04-24T10:39:48,361  INFO [Thread-1032] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,364  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,364  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,07225058876319168/_temporary/0/_temporary/attempt_local955960542_0013_m_000000_0/part-m-00000
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,366  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,376  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@64924cd
2024-04-24T10:39:48,381  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,381  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,5110230697454675/_temporary/0/_temporary/attempt_local955960542_0013_m_000000_0/part-m-00000
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,384  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,394  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@7c7d174
2024-04-24T10:39:48,399  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,400  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,24575149544365738/_temporary/0/_temporary/attempt_local955960542_0013_m_000000_0/part-m-00000
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,402  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,402  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,403  WARN [Thread-1032] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,403  INFO [Thread-1032] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,403  INFO [Thread-1032] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,405  INFO [Thread-1032] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,412  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@604bc8ee
2024-04-24T10:39:48,416  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,416  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,417  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,417  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,6491461599376249/bkt=0/_temporary/0/_temporary/attempt_local955960542_0013_m_000000_0/part-m-00000
2024-04-24T10:39:48,417  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,418  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,427  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@606ae274
2024-04-24T10:39:48,431  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,431  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: initialize serde with table properties.
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: creating real writer to write at file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,5112536715842684/bkt=1/_temporary/0/_temporary/attempt_local955960542_0013_m_000000_0/part-m-00000
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression set to false
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] codec.CodecConfig: Compression: UNCOMPRESSED
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet block size to 134217728
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet page size to 1048576
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Dictionary is on
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Validation is off
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page size checking is: estimated
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Min row count for page size check is: 100
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Page row count limit to 20000
2024-04-24T10:39:48,432  INFO [LocalJobRunner Map Task Executor #0] hadoop.ParquetOutputFormat: Writing page checksums is: on
2024-04-24T10:39:48,442  INFO [LocalJobRunner Map Task Executor #0] write.ParquetRecordWriterWrapper: real writer: org.apache.parquet.hadoop.ParquetRecordWriter@6cc73abe
2024-04-24T10:39:48,465  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:48,466  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local955960542_0013_m_000000_0 is done. And is in the process of committing
2024-04-24T10:39:48,467  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,467  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,472  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-24T10:39:48,472  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local955960542_0013_m_000000_0 is allowed to commit now
2024-04-24T10:39:48,473  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,473  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,474  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local955960542_0013_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic/_SCRATCH0,07225058876319168
2024-04-24T10:39:48,475  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,475  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,477  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local955960542_0013_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size/_SCRATCH0,5110230697454675
2024-04-24T10:39:48,478  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,478  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,479  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local955960542_0013_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db/junit_specific_size2/_SCRATCH0,24575149544365738
2024-04-24T10:39:48,481  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,481  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,482  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local955960542_0013_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,6491461599376249/bkt=0
2024-04-24T10:39:48,483  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-24T10:39:48,483  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-24T10:39:48,484  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local955960542_0013_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic/_SCRATCH0,5112536715842684/bkt=1
2024-04-24T10:39:48,485  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-24T10:39:48,485  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local955960542_0013_m_000000_0' done.
2024-04-24T10:39:48,485  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local955960542_0013_m_000000_0: Counters: 20
	File System Counters
		FILE: Number of bytes read=53738
		FILE: Number of bytes written=8050578
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=9
		Map output records=0
		Input split bytes=486
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1001390080
	MultiStoreCounters
		Output records in _0_junit_unparted_basic=9
		Output records in _1_junit_specific_size=9
		Output records in _2_junit_specific_db.junit_specific_size2=9
		Output records in _3_junit_parted_basic=3
		Output records in _4_junit_parted_basic=6
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-24T10:39:48,485  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local955960542_0013_m_000000_0
2024-04-24T10:39:48,486  INFO [Thread-1073] mapred.LocalJobRunner: map task executor complete.
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,527  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,527  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,528  INFO [Thread-1073] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:39:48,529  INFO [Thread-1073] metastore.HMSHandler: 20: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:39:48,529  INFO [Thread-1073] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:39:48,529  INFO [Thread-1073] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@76628dea, with PersistenceManager: null will be shutdown
2024-04-24T10:39:48,529  INFO [Thread-1073] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@76628dea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65e4d903 created in the thread with id: 1148
2024-04-24T10:39:48,530  INFO [Thread-1073] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@76628dea from thread id: 1148
2024-04-24T10:39:48,531  INFO [Thread-1073] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:39:48,531  INFO [Thread-1073] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:39:48,531  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,533  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_unparted_basic newtbl=junit_unparted_basic	
2024-04-24T10:39:48,544  INFO [Thread-1073] utils.MetaStoreServerUtils: Updating table stats for junit_unparted_basic
2024-04-24T10:39:48,544  INFO [Thread-1073] utils.MetaStoreServerUtils: Updated size of table junit_unparted_basic to 647
2024-04-24T10:39:48,550  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,583  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,583  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,584  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,585  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,624  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,624  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,625  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,628  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.default.junit_specific_size newtbl=junit_specific_size	
2024-04-24T10:39:48,640  INFO [Thread-1073] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size
2024-04-24T10:39:48,640  INFO [Thread-1073] utils.MetaStoreServerUtils: Updated size of table junit_specific_size to 647
2024-04-24T10:39:48,646  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,678  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,679  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,679  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,679  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,679  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,680  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,721  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,721  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,722  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,725  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=alter_table: hive.junit_specific_db.junit_specific_size2 newtbl=junit_specific_size2	
2024-04-24T10:39:48,739  INFO [Thread-1073] utils.MetaStoreServerUtils: Updating table stats for junit_specific_size2
2024-04-24T10:39:48,739  INFO [Thread-1073] utils.MetaStoreServerUtils: Updated size of table junit_specific_size2 to 647
2024-04-24T10:39:48,745  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,778  INFO [main] mapReduceLayer.MapReduceLauncher: 83% complete
2024-04-24T10:39:48,778  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local955960542_0013]
2024-04-24T10:39:48,780  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,782  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,783  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,783  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,783  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,785  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,785  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,801  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,803  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,805  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:48,829  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,830  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,830  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,830  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,864  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,865  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,865  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,865  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,865  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,865  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,865  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=0}].
2024-04-24T10:39:48,866  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[0]	
2024-04-24T10:39:48,870  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:48,870  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:48,876  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:39:48,877  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 597
2024-04-24T10:39:48,903  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,937  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,953  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,955  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:48,956  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:48,996  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:48,997  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:48,997  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:48,997  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:48,997  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:49,001  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_parted_basic has new partitions [{bkt=1}].
2024-04-24T10:39:49,002  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_parted_basic[1]	
2024-04-24T10:39:49,005  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-24T10:39:49,005  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-24T10:39:49,012  INFO [HMSHandler #1] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_parted_basic
2024-04-24T10:39:49,012  INFO [HMSHandler #1] utils.MetaStoreServerUtils: Updated size to 635
2024-04-24T10:39:49,041  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-24T10:39:49,075  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:49,076  WARN [Thread-1073] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:49,076  INFO [Thread-1073] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-24T10:39:49,076  INFO [Thread-1073] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:49,078  INFO [Thread-1073] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-24T10:39:49,260  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,268  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,274  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,285  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:49,285  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_unparted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:49,286  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:49,286  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_size does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:49,287  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:49,287  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_specific_db.junit_specific_size2 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:49,288  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:49,289  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:49,290  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-24T10:39:49,290  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File junit_parted_basic does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:226) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:39:49,291  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-24T10:39:49,291  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-24 10:39:47	2024-04-24 10:39:49	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1597369099_0012	1	0	n/a	n/a	n/a	n/a	0	0	0	0	E,F	MAP_ONLY	junit_parted_date,
job_local705690793_0011	1	0	n/a	n/a	n/a	n/a	0	0	0	0	D	MAP_ONLY	junit_unparted_complex,
job_local955960542_0013	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,B,B2,C,C2	MULTI_QUERY,MAP_ONLY	junit_unparted_basic,junit_specific_size,junit_specific_db.junit_specific_size2,junit_parted_basic,junit_parted_basic,

Input(s):
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/datetimestamp.input.data"
Successfully read 2 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/complex.input.data"
Successfully read 9 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/basic.input.data"

Output(s):
Successfully stored 2 records in: "junit_parted_date"
Successfully stored 2 records in: "junit_unparted_complex"
Successfully stored 9 records in: "junit_unparted_basic"
Successfully stored 9 records in: "junit_specific_size"
Successfully stored 9 records in: "junit_specific_db.junit_specific_size2"
Successfully stored 3 records in: "junit_parted_basic"
Successfully stored 6 records in: "junit_parted_basic"

Counters:
Total records written : 40
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1597369099_0012
job_local705690793_0011
job_local955960542_0013


2024-04-24T10:39:49,295  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,299  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,304  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,307  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,312  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,315  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-24T10:39:49,316  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-24T10:39:49,337  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-24T10:39:49,338  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-test.pig-2af0a00b-2220-4eaa-85c7-c6a682f58a64
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:49,357  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:49,358  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:49,360  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,371  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,384  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:39:49,384  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:39:49,384  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:39:49,384  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:39:49,385  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:39:49,385  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-24T10:39:49,387  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,395  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
X: {a: int,b: chararray}
2024-04-24T10:39:49,396  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_basic
2024-04-24T10:39:49,397  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,406  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,407  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,407  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,407  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,407  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0, getTable_(GetTableRequest)=11}
2024-04-24T10:39:49,407  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.011 seconds
2024-04-24T10:39:49,407  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,407  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,407  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_basic
2024-04-24T10:39:49,408  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,408  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,421  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,422  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,436  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:39:49,515  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,515  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=13, isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=93}
2024-04-24T10:39:49,516  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.108 seconds
2024-04-24T10:39:49,516  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_complex
2024-04-24T10:39:49,516  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:49,526  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,526  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,526  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,526  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,526  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=10, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:49,527  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.01 seconds
2024-04-24T10:39:49,527  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,527  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,527  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_complex
2024-04-24T10:39:49,527  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,527  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:49,538  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,539  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:49,546  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:39:49,578  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,578  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=12, dropTable_(String, String, boolean, boolean, boolean)=39, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:49,578  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.051 seconds
2024-04-24T10:39:49,579  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_parted_basic
2024-04-24T10:39:49,579  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:49,588  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,588  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,588  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,588  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,588  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:49,588  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.009 seconds
2024-04-24T10:39:49,588  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,588  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,589  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_parted_basic
2024-04-24T10:39:49,589  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,589  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:49,597  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,598  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:49,606  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,606  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:39:49,677  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,677  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=78, getTable_(GetTableRequest)=9, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:49,677  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.089 seconds
2024-04-24T10:39:49,677  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_specific_size
2024-04-24T10:39:49,678  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:49,687  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,687  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,687  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, flushCache_()=0}
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.01 seconds
2024-04-24T10:39:49,687  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_specific_size
2024-04-24T10:39:49,687  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,688  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:49,696  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,697  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:49,704  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,705  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:39:49,749  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,750  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=52, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9}
2024-04-24T10:39:49,750  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.062 seconds
2024-04-24T10:39:49,750  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_parted_date
2024-04-24T10:39:49,751  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:49,758  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,758  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,758  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,758  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,758  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getTable_(GetTableRequest)=7, isCompatibleWith_(Configuration)=0}
2024-04-24T10:39:49,759  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.008 seconds
2024-04-24T10:39:49,759  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,759  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,759  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_parted_date
2024-04-24T10:39:49,759  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,759  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:49,769  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:49,777  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,777  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:39:49,845  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,845  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropTable_(String, String, boolean, boolean, boolean)=76, isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10}
2024-04-24T10:39:49,846  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.086 seconds
2024-04-24T10:39:49,846  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:49,847  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:49,859  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,860  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,860  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=13, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.014 seconds
2024-04-24T10:39:49,860  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop table if exists junit_unparted_alltypes
2024-04-24T10:39:49,860  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,861  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:49,870  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,870  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:49,881  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:39:49,881  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:39:49,914  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:39:49,914  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, dropTable_(String, String, boolean, boolean, boolean)=44}
2024-04-24T10:39:49,914  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.054 seconds
2024-04-24T10:39:49,915  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop database if exists junit_specific_db cascade
2024-04-24T10:39:49,915  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:49,918  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:39:49,933  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:39:49,938  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:39:49,938  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:39:49,938  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:39:49,938  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, getTables_(String, String)=15, getTableObjectsByName_(String, List)=6}
2024-04-24T10:39:49,939  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 0.023 seconds
2024-04-24T10:39:49,939  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:39:49,939  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:39:49,939  INFO [main] ql.Driver: Executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802): drop database if exists junit_specific_db cascade
2024-04-24T10:39:49,939  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:39:49,939  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:39:49,941  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:49,941  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:49,945  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:49,949  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:39:49,952  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:49,953  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:49,954  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:49,959  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:49,962  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:49,965  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:49,969  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:49,985  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:49,985  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:49,987 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:51,988  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:51,993  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:51,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:51,995  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:51,996  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:51,998  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:52,002  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:52,008  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:52,008  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:52,009 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:54,010  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:54,015  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:54,016  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:54,017  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:54,019  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:54,020  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:54,024  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:54,030  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:54,030  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:54,032 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:56,032  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:56,037  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:56,038  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:56,040  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:56,041  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:56,042  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:56,046  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:56,053  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:56,053  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:56,054 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:39:58,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:39:58,058  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:39:58,059  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:39:58,061  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:39:58,062  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:39:58,063  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:39:58,068  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:39:58,075  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:39:58,075  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:39:58,077 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:00,078  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:00,081  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:00,081  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:00,083  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:00,084  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:00,085  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:00,089  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:00,096  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:00,096  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:00,097 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 6 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:00,594  INFO [Metastore Scheduled Worker 4] metastore.HMSHandler: 21: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:40:00,595  INFO [Metastore Scheduled Worker 4] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c7cc445, with PersistenceManager: null will be shutdown
2024-04-24T10:40:00,595  INFO [Metastore Scheduled Worker 4] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c7cc445, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@68434d12 created in the thread with id: 34
2024-04-24T10:40:00,613  INFO [Metastore Scheduled Worker 4] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6c7cc445 from thread id: 34
2024-04-24T10:40:02,098  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:02,103  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:02,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:02,105  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:02,106  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:02,108  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:02,112  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:02,118  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:02,118  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:02,119 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 7 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:04,121  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:04,124  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:04,125  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:04,127  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:04,128  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:04,130  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:04,134  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:04,140  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:04,140  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:04,141 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 8 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:06,143  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:06,148  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:06,151  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:06,157  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:06,162  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:06,167  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:06,172  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:06,179  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:06,179  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:06,180 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 9 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:08,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:08,186  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:08,187  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:08,189  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:08,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:08,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:08,195  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:08,203  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:08,203  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:08,204 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 10 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:10,205  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:10,210  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:10,211  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:10,212  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:10,214  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:10,215  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:10,219  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:10,225  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:10,225  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:10,226 ERROR [main] metastore.RetryingHMSHandler: HMSHandler Fatal error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:10,226 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:40:10,229 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:666) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:209) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
Caused by: org.apache.derby.iapi.error.StandardException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source) ~[derby-10.14.1.0.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128) ~[HikariCP-java7-2.4.12.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-java7-2.4.12.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729) ~[datanucleus-rdbms-5.2.4.jar:?]
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286) ~[datanucleus-core-5.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99) ~[datanucleus-api-jdo-5.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[classes/:?]
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662) ~[classes/:?]
	... 57 more
2024-04-24T10:40:10,232  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:40:10,232  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.) retryPossible: false
2024-04-24T10:40:10,232  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.)
2024-04-24T10:40:10,232 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
NestedThrowables:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.)
2024-04-24T10:40:10,232  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,232  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,232  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424103946_222cf05b-f253-4e06-8da5-397ba262c802); Time taken: 20.293 seconds
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:40:10,264  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:40:10,266  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:10,266  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:10,266  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@378c66fb will be shutdown
2024-04-24T10:40:10,267  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@41ca4c9d created in the thread with id: 1
2024-04-24T10:40:10,268  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 1a0557db-b08d-4c37-b989-ec8e21dae9a1
2024-04-24T10:40:10,269  INFO [main] SessionState: Hive Session ID = 1a0557db-b08d-4c37-b989-ec8e21dae9a1
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:40:10,269  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:40:10,274  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/1a0557db-b08d-4c37-b989-ec8e21dae9a1
2024-04-24T10:40:10,277  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/1a0557db-b08d-4c37-b989-ec8e21dae9a1
2024-04-24T10:40:10,279  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/1a0557db-b08d-4c37-b989-ec8e21dae9a1/_tmp_space.db
2024-04-24T10:40:10,280  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,280  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,281  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,281  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,281  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,281  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-24T10:40:10,281  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.001 seconds
2024-04-24T10:40:10,281  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,282  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,282  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,282  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,282  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,283  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,284  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,284  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-24T10:40:10,284  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,284  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,285  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,285  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:10,285  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1a0557db-b08d-4c37-b989-ec8e21dae9a1, clientType=HIVECLI]
2024-04-24T10:40:10,285  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:40:10,285  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:40:10,285  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@66b2416e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@41ca4c9d will be shutdown
2024-04-24T10:40:10,285  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:10,285  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -40
2024-04-24T10:40:10,286  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:10,287  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:40:10,287  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:10,288  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247, with PersistenceManager: null will be shutdown
2024-04-24T10:40:10,288  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@67a86535 created in the thread with id: 1
2024-04-24T10:40:10,289  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247 from thread id: 1
2024-04-24T10:40:10,289  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:10,289  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:10,289  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:40:10,290  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:10,290  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:10,291  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@67a86535 will be shutdown
2024-04-24T10:40:10,291  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4c1a818f created in the thread with id: 1
2024-04-24T10:40:10,292  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:10,292  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:10,292  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,293  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,293  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,293  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,293  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,293  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:40:10,293  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.009 seconds
2024-04-24T10:40:10,294  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,294  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,294  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,294  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,294  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:40:10,294  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:40:10,294  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2fd47247, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4c1a818f will be shutdown
2024-04-24T10:40:10,294  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:10,294  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -41
2024-04-24T10:40:10,296  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:10,296  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:40:10,296  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:10,297  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42, with PersistenceManager: null will be shutdown
2024-04-24T10:40:10,297  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@c03da6b created in the thread with id: 1
2024-04-24T10:40:10,298  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42 from thread id: 1
2024-04-24T10:40:10,298  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:10,298  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:10,299  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980410, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{transactional=false, totalSize=0, numFiles=0, bucketing_version=2, numRows=0, rawDataSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numFilesErasureCoded=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:10,302  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:40:10,339  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,339  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=41}
2024-04-24T10:40:10,339  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.045 seconds
2024-04-24T10:40:10,340  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,340  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,342  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,342  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,342  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,342  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:10,342  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,342  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,342  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,342  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,343  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,343  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,344  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,346  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,346  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=3}
2024-04-24T10:40:10,346  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,347  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,348  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,348  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:10,348  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_complex position=13
2024-04-24T10:40:10,348  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,350  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,350  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,350  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,351  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,351  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,351  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,351  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,351  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,351  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_unparted_complex(name string, studentid int, contact struct<phno:string,email:string>, currently_registered_courses array<string>, current_grades map<string,string>, phnos array<struct<phno:string,type:string>>) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,351  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,352  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_complex, dbName:default, owner:alex, createTime:1713980410, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:studentid, type:int, comment:null), FieldSchema(name:contact, type:struct<phno:string,email:string>, comment:null), FieldSchema(name:currently_registered_courses, type:array<string>, comment:null), FieldSchema(name:current_grades, type:map<string,string>, comment:null), FieldSchema(name:phnos, type:array<struct<phno:string,type:string>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"contact":"true","current_grades":"true","currently_registered_courses":"true","name":"true","phnos":"true","studentid":"true"}}, totalSize=0, transactional=false, numFilesErasureCoded=0, numFiles=0, rawDataSize=0, bucketing_version=2, numRows=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:10,359  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:40:10,378  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,378  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=25, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,378  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.027 seconds
2024-04-24T10:40:10,378  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,379  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,380  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,380  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,380  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,381  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,381  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,381  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,381  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,381  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,381  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,381  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,383  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,384  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,385  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,385  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,385  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,386  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,386  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:10,386  INFO [main] parse.CalcitePlanner: Creating table default.junit_parted_basic position=13
2024-04-24T10:40:10,386  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,388  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,388  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,388  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,388  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_parted_basic(a int, b string) partitioned by (bkt string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,388  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,389  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_parted_basic, dbName:default, owner:alex, createTime:1713980410, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:bkt, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:10,395  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:40:10,410  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,411  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, createTable_(Table)=21}
2024-04-24T10:40:10,411  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.022 seconds
2024-04-24T10:40:10,411  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,412  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,413  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,413  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,413  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,413  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,413  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,414  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,414  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,414  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use default
2024-04-24T10:40:10,414  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,414  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,415  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,417  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,417  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,417  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,417  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,418  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,418  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:10,418  INFO [main] parse.CalcitePlanner: Creating table default.junit_specific_size position=13
2024-04-24T10:40:10,418  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:10,420  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,420  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,420  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,420  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,420  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=2, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,420  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,420  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,421  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,421  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_specific_size(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,421  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,422  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size, dbName:default, owner:alex, createTime:1713980410, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, rawDataSize=0, numRows=0, numFiles=0, transactional=false, bucketing_version=2, numFilesErasureCoded=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:10,427  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:40:10,442  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,442  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=21, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,442  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.021 seconds
2024-04-24T10:40:10,443  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create database if not exists junit_specific_db
2024-04-24T10:40:10,443  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,443  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,443  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,444  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,444  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.0 seconds
2024-04-24T10:40:10,444  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,444  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,444  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create database if not exists junit_specific_db
2024-04-24T10:40:10,444  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,444  WARN [main] exec.DDLTask: metastore.warehouse.external.dir is not set, falling back to metastore.warehouse.dir. This could cause external tables to use to managed tablespace.
2024-04-24T10:40:10,445  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_database: Database(name:junit_specific_db, description:null, locationUri:file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db, parameters:null, ownerName:alex, ownerType:USER, catalogName:hive, type:NATIVE)	
2024-04-24T10:40:10,446 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Database junit_specific_db already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_database(HMSHandler.java:1472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:1151)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:605)
	at org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.execute(CreateDatabaseOperation.java:68)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:129)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:40:10,446  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,447  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,447  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,447  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use junit_specific_db
2024-04-24T10:40:10,448  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,449  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,449  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,449  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,449  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-24T10:40:10,450  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,450  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,450  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,450  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): use junit_specific_db
2024-04-24T10:40:10,450  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,450  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,452  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,453  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,453  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=3, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,453  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,454  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,454  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,454  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:10,454  INFO [main] parse.CalcitePlanner: Creating table junit_specific_db.junit_specific_size2 position=13
2024-04-24T10:40:10,454  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,456  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d
2024-04-24T10:40:10,456  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,456  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,456  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,456  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=2, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:10,456  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.002 seconds
2024-04-24T10:40:10,457  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,457  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,457  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): create table junit_specific_size2(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:10,457  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_specific_size2, dbName:junit_specific_db, owner:alex, createTime:1713980410, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}, numRows=0, totalSize=0, numFiles=0, bucketing_version=2, transactional=false, numFilesErasureCoded=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:10,470  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:40:10,473 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:40:10,473 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:40:10,476 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:138) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:172) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.junit_specific_db.junit_specific_size2 already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:40:10,478  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:40:10,479  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
2024-04-24T10:40:10,479  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:40:10,479 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.junit_specific_db.junit_specific_size2 already exists)
2024-04-24T10:40:10,479  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,479  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,479  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.022 seconds
2024-04-24T10:40:10,479  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_basic
2024-04-24T10:40:10,480  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,483  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,483  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,483  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_basic
2024-04-24T10:40:10,483  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,484  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_basic	
2024-04-24T10:40:10,487  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,487  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,487  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,487  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_complex
2024-04-24T10:40:10,488  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:40:10,490  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,490  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,491  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,491  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:10,491  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,491  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,491  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,491  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_complex
2024-04-24T10:40:10,491  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,492  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_complex	
2024-04-24T10:40:10,495  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,495  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,496  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,496  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_parted_basic
2024-04-24T10:40:10,496  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:40:10,499  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,499  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,499  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,499  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:10,500  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,500  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,500  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,500  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_parted_basic
2024-04-24T10:40:10,500  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,500  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_basic	
2024-04-24T10:40:10,503  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,503  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,503  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,503  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_specific_size
2024-04-24T10:40:10,504  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:40:10,506  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,506  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,506  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,507  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,507  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,507  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,507  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,507  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_specific_size
2024-04-24T10:40:10,507  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,507  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_specific_size	
2024-04-24T10:40:10,510  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,510  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,510  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,510  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_parted_date
2024-04-24T10:40:10,511  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,514  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,514  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.004 seconds
2024-04-24T10:40:10,514  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_parted_date
2024-04-24T10:40:10,514  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,514  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_parted_date	
2024-04-24T10:40:10,517  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,517  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,517  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,518  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_alltypes
2024-04-24T10:40:10,518  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,521  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,521  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,521  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop table if exists junit_unparted_alltypes
2024-04-24T10:40:10,521  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,521  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.junit_specific_db.junit_unparted_alltypes	
2024-04-24T10:40:10,524  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:10,524  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:10,524  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.003 seconds
2024-04-24T10:40:10,525  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop database if exists junit_specific_db cascade
2024-04-24T10:40:10,525  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,528  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=null	
2024-04-24T10:40:10,533  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=	
2024-04-24T10:40:10,533  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:10,533  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:10,534  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:10,534  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTables_(String, String)=5, getTableObjectsByName_(String, List)=1, flushCache_()=0, getDatabase_(String)=2}
2024-04-24T10:40:10,534  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 0.008 seconds
2024-04-24T10:40:10,534  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:10,534  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:10,534  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d): drop database if exists junit_specific_db cascade
2024-04-24T10:40:10,534  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:10,535  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:10,536  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_tables_by_type: db=@hive#junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:10,536  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:10,538  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:10,540  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_multi_table : db=junit_specific_db tbls=junit_specific_size2	
2024-04-24T10:40:10,543  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:10,544  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:10,545  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:10,547  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:10,548  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:10,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:10,554  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:10,561  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:10,561  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:10,563 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:12,563  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:12,568  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:12,570  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:12,572  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:12,573  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:12,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:12,579  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:12,585  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:12,585  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:12,587 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 2 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:14,587  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:14,589  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:14,591  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:14,593  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:14,594  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:14,595  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:14,600  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:14,608  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:14,608  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:14,609 ERROR [main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 3 of 10) with error: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:162)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
NestedThrowablesStackTrace:
java.sql.BatchUpdateException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeBatch(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeBatch(Unknown Source)
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366)
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:675)
	at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:643)
	at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:729)
	at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:95)
	at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:528)
	at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:222)
	at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:286)
	at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:99)
	at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:615)
	at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
	at com.sun.proxy.$Proxy32.commitTransaction(Unknown Source)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database_core(HMSHandler.java:1790)
	at org.apache.hadoop.hive.metastore.HMSHandler.drop_database(HMSHandler.java:1869)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.drop_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1601)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1532)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.dropDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:662)
	at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:52)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.tearDown(AbstractHCatLoaderTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.derby.shared.common.error.DerbySQLIntegrityConstraintViolationException: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeBatchElement(Unknown Source)
	... 94 more
Caused by: ERROR 23503: DELETE on table 'DBS' caused a violation of foreign key constraint 'TBLS_FK1' for key (5).  The statement has been rolled back.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker.doCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.RISetChecker.doPKCheck(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.collectAffectedRows(Unknown Source)
	at org.apache.derby.impl.sql.execute.DeleteResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 96 more

2024-04-24T10:40:16,610  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_database: @hive#junit_specific_db	
2024-04-24T10:40:16,615  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_tables: db=@hive#junit_specific_db	
2024-04-24T10:40:16,616  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_functions: db=@hive#junit_specific_db pat=*	
2024-04-24T10:40:16,618  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_stored_procedures	
2024-04-24T10:40:16,620  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_packages	
2024-04-24T10:40:16,621  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=getTablesByTypeCore: catName=hive: db=junit_specific_db pat=.*,type=MATERIALIZED_VIEW	
2024-04-24T10:40:16,631  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.junit_specific_db.junit_specific_size2	
2024-04-24T10:40:16,702  INFO [main] metastore.ObjectStore: Dropping database hive.junit_specific_db along with all tables
2024-04-24T10:40:16,709  INFO [main] txn.TxnHandler: Non-retryable error in cleanupRecords : Table/View 'TXN_COMPONENTS' does not exist. (SQLState=42X05, ErrorCode=20000)
2024-04-24T10:40:16,709  WARN [main] txn.TxnHandler: Cannot perform cleanup since metastore table does not exist
2024-04-24T10:40:16,710  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:40:16,711 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:40:16,711  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db does not exist; Force to delete it.
2024-04-24T10:40:16,711 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_db.db
2024-04-24T10:40:16,711  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,712  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {dropDatabase_(String, boolean, boolean, boolean)=6177, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:16,712  INFO [main] metadata.Hive: Total time spent in this metastore function was greater than 1000ms : dropDatabase_(String, boolean, boolean, boolean)=6177
2024-04-24T10:40:16,712  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104010_cf440e91-8a9b-41b9-b2b1-d7997728b02d); Time taken: 6.177 seconds
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-24T10:40:16,746  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-24T10:40:16,748  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:16,748  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:16,748  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@c03da6b will be shutdown
2024-04-24T10:40:16,749  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72e2192d created in the thread with id: 1
2024-04-24T10:40:16,750  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 73a67569-f489-4e69-921a-11baa990c4e8
2024-04-24T10:40:16,750  INFO [main] SessionState: Hive Session ID = 73a67569-f489-4e69-921a-11baa990c4e8
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:40:16,750  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-24T10:40:16,756  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/73a67569-f489-4e69-921a-11baa990c4e8
2024-04-24T10:40:16,758  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/73a67569-f489-4e69-921a-11baa990c4e8
2024-04-24T10:40:16,760  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/73a67569-f489-4e69-921a-11baa990c4e8/_tmp_space.db
2024-04-24T10:40:16,760  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): use default
2024-04-24T10:40:16,761  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:16,762  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,762  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,762  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,762  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:16,762  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.002 seconds
2024-04-24T10:40:16,762  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,762  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,762  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): use default
2024-04-24T10:40:16,763  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,763  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:16,764  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:16,764  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,765  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getDatabase_(String)=1, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:16,765  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.002 seconds
2024-04-24T10:40:16,765  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:16,766  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d
2024-04-24T10:40:16,766  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-24T10:40:16,766  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=73a67569-f489-4e69-921a-11baa990c4e8, clientType=HIVECLI]
2024-04-24T10:40:16,766  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-24T10:40:16,766  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:40:16,766  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@57d6ac42, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72e2192d will be shutdown
2024-04-24T10:40:16,766  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:16,766  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -42
2024-04-24T10:40:16,767  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:16,768  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:40:16,768  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:16,768  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251, with PersistenceManager: null will be shutdown
2024-04-24T10:40:16,768  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73ce59fd created in the thread with id: 1
2024-04-24T10:40:16,769  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251 from thread id: 1
2024-04-24T10:40:16,769  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:16,769  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:16,770  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted_basic position=13
2024-04-24T10:40:16,770  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:16,770  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:16,771  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@73ce59fd will be shutdown
2024-04-24T10:40:16,771  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a5f7e16 created in the thread with id: 1
2024-04-24T10:40:16,772  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:16,772  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:16,772  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-24T10:40:16,773  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d
2024-04-24T10:40:16,773  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,773  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,773  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,773  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:40:16,773  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.008 seconds
2024-04-24T10:40:16,774  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,774  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,774  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): create table junit_unparted_basic(a int, b string) stored as PARQUET TBLPROPERTIES ('transactional'='false')
2024-04-24T10:40:16,774  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,774  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-24T10:40:16,774  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-24T10:40:16,774  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@46062251, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3a5f7e16 will be shutdown
2024-04-24T10:40:16,774  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:16,774  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -43
2024-04-24T10:40:16,776  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-24T10:40:16,776  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-24T10:40:16,776  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-24T10:40:16,777  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2c91de4e, with PersistenceManager: null will be shutdown
2024-04-24T10:40:16,777  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2c91de4e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ace6e05 created in the thread with id: 1
2024-04-24T10:40:16,778  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2c91de4e from thread id: 1
2024-04-24T10:40:16,778  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-24T10:40:16,778  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-24T10:40:16,779  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted_basic, dbName:default, owner:alex, createTime:1713980416, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{bucketing_version=2, totalSize=0, numFilesErasureCoded=0, numFiles=0, transactional=false, numRows=0, rawDataSize=0, COLUMN_STATS_ACCURATE={"BASIC_STATS":"true","COLUMN_STATS":{"a":"true","b":"true"}}}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-24T10:40:16,787  WARN [main] metastore.HMSHandler: create_table_req got 
org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) [classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) [classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) [classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) [classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) [classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-24T10:40:16,790 ERROR [main] metastore.RetryingHMSHandler: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339)
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
	at com.sun.proxy.$Proxy45.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

2024-04-24T10:40:16,790 ERROR [main] exec.Task: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:40:16,792 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1298) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1306) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.createTableNonReplaceMode(CreateTableOperation.java:140) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:98) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:104) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:142) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.setUpTest(AbstractHCatLoaderTest.java:162) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.jar:4.13]
	at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) [surefire-junit47-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Table hive.default.junit_unparted_basic already exists
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_core(HMSHandler.java:2339) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HMSHandler.create_table_req(HMSHandler.java:2598) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[classes/:?]
	at com.sun.proxy.$Proxy35.create_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:4408) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:182) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1260) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1245) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[classes/:?]
	at com.sun.proxy.$Proxy45.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1290) ~[classes/:?]
	... 63 more
2024-04-24T10:40:16,795  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-24T10:40:16,795  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
2024-04-24T10:40:16,795  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists) retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:40:16,795 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. AlreadyExistsException(message:Table hive.default.junit_unparted_basic already exists)
2024-04-24T10:40:16,795  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,795  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:40:16,795  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.021 seconds
2024-04-24T10:40:16,796  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_basic
2024-04-24T10:40:16,796  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:40:16,804  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,804  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,804  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,804  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,804  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=8, isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:16,804  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.008 seconds
2024-04-24T10:40:16,805  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,805  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,805  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_basic
2024-04-24T10:40:16,805  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,805  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:40:16,814  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,814  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:40:16,822  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,822  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_basic	
2024-04-24T10:40:16,852  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic does not exist; Force to delete it.
2024-04-24T10:40:16,852 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_basic
2024-04-24T10:40:16,852  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,852  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=9, dropTable_(String, String, boolean, boolean, boolean)=38}
2024-04-24T10:40:16,853  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.047 seconds
2024-04-24T10:40:16,853  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_complex
2024-04-24T10:40:16,853  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:40:16,864  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,864  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,864  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=11, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.011 seconds
2024-04-24T10:40:16,864  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_complex
2024-04-24T10:40:16,864  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,865  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:40:16,874  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,874  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:40:16,884  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,884  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_unparted_complex	
2024-04-24T10:40:16,918  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex does not exist; Force to delete it.
2024-04-24T10:40:16,918 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_unparted_complex
2024-04-24T10:40:16,919  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,919  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, dropTable_(String, String, boolean, boolean, boolean)=44, isCompatibleWith_(Configuration)=1}
2024-04-24T10:40:16,919  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.055 seconds
2024-04-24T10:40:16,919  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_parted_basic
2024-04-24T10:40:16,920  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:40:16,929  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,929  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,929  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=1, getTable_(GetTableRequest)=9}
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.01 seconds
2024-04-24T10:40:16,929  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_parted_basic
2024-04-24T10:40:16,929  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,930  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:40:16,939  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,939  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:40:16,948  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,948  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_parted_basic	
2024-04-24T10:40:16,982  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic does not exist; Force to delete it.
2024-04-24T10:40:16,982 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_parted_basic
2024-04-24T10:40:16,983  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:16,983  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=44, getTable_(GetTableRequest)=9}
2024-04-24T10:40:16,983  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.054 seconds
2024-04-24T10:40:16,983  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_specific_size
2024-04-24T10:40:16,984  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:40:16,993  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:16,993  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:16,993  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:16,993  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:16,994  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {getTable_(GetTableRequest)=9, flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:16,994  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.01 seconds
2024-04-24T10:40:16,994  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:16,994  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:16,994  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_specific_size
2024-04-24T10:40:16,994  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:16,994  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:40:17,004  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:17,004  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:40:17,014  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-24T10:40:17,014  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junit_specific_size	
2024-04-24T10:40:17,047  WARN [main] utils.FileUtils: File file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size does not exist; Force to delete it.
2024-04-24T10:40:17,047 ERROR [main] utils.FileUtils: Failed to delete file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1713980215316/warehouse/junit_specific_size
2024-04-24T10:40:17,047  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:17,047  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=10, dropTable_(String, String, boolean, boolean, boolean)=43}
2024-04-24T10:40:17,047  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.053 seconds
2024-04-24T10:40:17,048  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_parted_date
2024-04-24T10:40:17,048  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:40:17,051  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:17,051  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:17,051  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:17,051  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:17,052  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.003 seconds
2024-04-24T10:40:17,052  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:17,052  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:17,052  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_parted_date
2024-04-24T10:40:17,052  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:17,052  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_parted_date	
2024-04-24T10:40:17,055  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:17,056  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:17,056  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.003 seconds
2024-04-24T10:40:17,056  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_alltypes
2024-04-24T10:40:17,057  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:17,060  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:17,060  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.004 seconds
2024-04-24T10:40:17,060  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop table if exists junit_unparted_alltypes
2024-04-24T10:40:17,060  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-24T10:40:17,060  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted_alltypes	
2024-04-24T10:40:17,064  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:17,064  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:17,064  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.004 seconds
2024-04-24T10:40:17,064  INFO [main] ql.Driver: Compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop database if exists junit_specific_db cascade
2024-04-24T10:40:17,065  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: junit_specific_db	
2024-04-24T10:40:17,066  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-24T10:40:17,066  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-24T10:40:17,066  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-24T10:40:17,066  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {flushCache_()=0, isCompatibleWith_(Configuration)=0}
2024-04-24T10:40:17,066  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.002 seconds
2024-04-24T10:40:17,066  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-24T10:40:17,066  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-24T10:40:17,066  INFO [main] ql.Driver: Executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d): drop database if exists junit_specific_db cascade
2024-04-24T10:40:17,066  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-24T10:40:17,067  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-24T10:40:17,067  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240424104016_9ffe5e34-2c1c-487d-af00-e9b553fdd54d); Time taken: 0.0 seconds
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -51
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -52
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -53
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -54
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -55
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-24T10:40:17,491  INFO [pool-3-thread-1] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -56
