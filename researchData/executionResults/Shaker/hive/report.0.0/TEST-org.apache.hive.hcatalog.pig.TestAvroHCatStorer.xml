<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="44.722" tests="29" errors="4" skipped="5" failures="2">
  <properties>
    <property name="sun.desktop" value="gnome"/>
    <property name="build.test.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="test.tmp.dir.uri" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="qfile" value=""/>
    <property name="log4j.configurationFile" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="test.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/warehouse"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="run_disabled" value=""/>
    <property name="java.vm.vendor" value="Private Build"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="hadoop.version" value="3.1.0"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="user.timezone" value=""/>
    <property name="user.country.format" value="PT"/>
    <property name="test.local.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/localfs/warehouse"/>
    <property name="os.name" value="Linux"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="test.tmp.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="test.build.data" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="clustermode" value=""/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="user.country" value="US"/>
    <property name="test.data.files" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter4209218354287344082.jar /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire 2024-05-01T00-40-33_663-jvmRun1 surefire3892172834181780717tmp surefire_9458236861732205379343tmp"/>
    <property name="surefire.test.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/alex"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/java-8-openjdk-amd64/jre"/>
    <property name="java.security.krb5.conf" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/krb5.conf"/>
    <property name="file.separator" value="/"/>
    <property name="basedir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="test.dfs.mkdir" value=""/>
    <property name="derby.stream.error.file" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/derby.log"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="surefire.real.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter4209218354287344082.jar"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes"/>
    <property name="hadoop.log.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="log4j.debug" value="true"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="spark.home" value=""/>
    <property name="java.runtime.version" value="1.8.0_402-8u402-ga-2ubuntu1~22.04-b06"/>
    <property name="user.name" value="alex"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="test.src.tables" value=""/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="6.5.0-28-generic"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="hive.test.console.log.level" value="INFO"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="qfile_regex" value=""/>
    <property name="hadoop.bin.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../testutils/hadoop"/>
    <property name="localRepository" value="/home/alex/.m2/repository"/>
    <property name="build.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target"/>
    <property name="mapred.job.tracker" value="local"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="java.io.tmpdir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="maven.local.repository" value="/home/alex/.m2/repository"/>
    <property name="java.version" value="1.8.0_402"/>
    <property name="user.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="os.arch" value="amd64"/>
    <property name="hive.version" value="4.0.0-SNAPSHOT"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="user.language.format" value="pt"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="antlr.version" value="3.5.2"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="derby.version" value="10.14.1.0"/>
    <property name="test.data.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib"/>
    <property name="java.vendor" value="Private Build"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.402-b06"/>
    <property name="java.specification.maintenance.version" value="5"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="initScript" value=""/>
    <property name="java.class.version" value="52.0"/>
    <property name="test.output.overwrite" value=""/>
    <property name="hive.root" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../"/>
  </properties>
  <testcase name="testWriteTinyint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
    <system-err><![CDATA[SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,115061 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@7f31245a
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916]...
DEBUG StatusLogger Reconfiguration started for context[name=7f31245a] at URI null (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@233795b6
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,020939 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", modulate="true", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(max="30", min="null", fileIndex="null", compressionLevel="null", ={}, stopCustomActionsOnError="null", tempCompressedFilePattern="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileName="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log", filePattern="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertise="null", advertiseURI="null", filePermissions="null", fileOwner="null", fileGroup="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log seek to 63279237
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-04-30T20:36:45.408-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/01-00:00:00.000, nextFileTime=2024/04/30-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/04/30-20:36:47.072, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/01-00:00:00.000, nextFileTime=2024/04/30-00:00:00.000, prevFileTime=2024/04/30-00:00:00.000, current=2024/04/30-20:36:47.073, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@609db546
TRACE StatusLogger Reregistering context (1/1): '7f31245a' org.apache.logging.log4j.core.LoggerContext@2b71e916
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=7f31245a] at URI /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916] started OK.
2024-04-30T20:36:47,266  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
]]></system-err>
  </testcase>
  <testcase name="testWriteSmallint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDateCharTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteTimestamp" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteDecimalXY" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDynamicPartitioningMultiPartColsInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="14.317"/>
  <testcase name="testEmptyStore" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.16"/>
  <testcase name="testStoreFuncAllSimpleTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.38"/>
  <testcase name="testDynamicPartitioningMultiPartColsInDataPartialSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.463"/>
  <testcase name="testWriteDate2" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="4.708"/>
  <testcase name="testWriteDate3" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="2.123"/>
  <testcase name="testStoreInPartiitonedTbl" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.212">
    <failure message="expected:&lt;1&gt; but was:&lt;2&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testStoreInPartiitonedTbl(AbstractHCatStorerTest.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-30T20:37:12,462  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,463  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,464  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,464  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,465  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1e33dda will be shutdown
2024-04-30T20:37:12,465  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65d870be created in the thread with id: 1
2024-04-30T20:37:12,467  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 7a7128a0-fd13-44d2-b4eb-83390832ed0d
2024-04-30T20:37:12,467  INFO [main] SessionState: Hive Session ID = 7a7128a0-fd13-44d2-b4eb-83390832ed0d
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:12,467  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:12,473  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/7a7128a0-fd13-44d2-b4eb-83390832ed0d
2024-04-30T20:37:12,476  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/7a7128a0-fd13-44d2-b4eb-83390832ed0d
2024-04-30T20:37:12,479  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/7a7128a0-fd13-44d2-b4eb-83390832ed0d/_tmp_space.db
2024-04-30T20:37:12,479  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): drop table if exists junit_unparted
2024-04-30T20:37:12,480  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:12,481  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:12,481  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:12,481  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:12,481  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-30T20:37:12,481  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.002 seconds
2024-04-30T20:37:12,481  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:12,481  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:12,481  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): drop table if exists junit_unparted
2024-04-30T20:37:12,482  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:12,482  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:12,483  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:12,483  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-30T20:37:12,483  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.002 seconds
2024-04-30T20:37:12,484  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): use default
2024-04-30T20:37:12,484  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:12,485  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:12,485  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.001 seconds
2024-04-30T20:37:12,485  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): use default
2024-04-30T20:37:12,485  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:12,486  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:12,486  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:12,487  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:12,487  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-30T20:37:12,487  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.002 seconds
2024-04-30T20:37:12,488  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:12,488  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1
2024-04-30T20:37:12,488  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:12,488  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7a7128a0-fd13-44d2-b4eb-83390832ed0d, clientType=HIVECLI]
2024-04-30T20:37:12,488  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:12,489  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:12,489  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65d870be will be shutdown
2024-04-30T20:37:12,489  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:12,489  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-04-30T20:37:12,489  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,490  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:12,490  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,490  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee, with PersistenceManager: null will be shutdown
2024-04-30T20:37:12,490  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@328b60fb created in the thread with id: 1
2024-04-30T20:37:12,492  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee from thread id: 1
2024-04-30T20:37:12,492  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:12,492  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:12,492  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted position=13
2024-04-30T20:37:12,493  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,493  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,493  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@328b60fb will be shutdown
2024-04-30T20:37:12,493  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@28336feb created in the thread with id: 1
2024-04-30T20:37:12,495  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:12,495  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:12,495  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:12,496  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1
2024-04-30T20:37:12,496  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:12,496  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:12,496  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:12,496  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:12,496  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.009 seconds
2024-04-30T20:37:12,496  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:12,496  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:12,496  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:12,497  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:12,497  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:12,497  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:12,497  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@39a9b3ee, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@28336feb will be shutdown
2024-04-30T20:37:12,497  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:12,497  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-04-30T20:37:12,498  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,498  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:12,498  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,499  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751, with PersistenceManager: null will be shutdown
2024-04-30T20:37:12,499  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5bbfd031 created in the thread with id: 1
2024-04-30T20:37:12,501  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751 from thread id: 1
2024-04-30T20:37:12,501  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:12,501  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:12,501  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted, dbName:default, owner:alex, createTime:1714534632, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:b, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-30T20:37:12,503  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/junit_unparted
2024-04-30T20:37:12,515  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:12,515  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=14}
2024-04-30T20:37:12,516  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203712_3eeb945e-fbb2-4dca-85d6-2fae825eecd1); Time taken: 0.019 seconds
2024-04-30T20:37:12,538  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-30T20:37:12,538  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-88b97e84-47b0-4a8a-a579-34e1c6a52411
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,572  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,573  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:12,573  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:12,574  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:12,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:12,581  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:12,600  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,600  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,600  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,600  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,601  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,601  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:12,601  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:12,603  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-30T20:37:12,611  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-30T20:37:12,611  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-30T20:37:12,612  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-30T20:37:12,612  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-30T20:37:12,612  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-30T20:37:12,618  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:12,619  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-30T20:37:12,619  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-30T20:37:12,619  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-30T20:37:12,619  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-30T20:37:12,619  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-30T20:37:12,620  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714534632619-0
2024-04-30T20:37:12,623  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-30T20:37:12,625  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:12,643  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,643  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,644  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,644  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:12,645  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,646  INFO [JobControl] metastore.HMSHandler: 26: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:12,646  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,646  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e6a69ea, with PersistenceManager: null will be shutdown
2024-04-30T20:37:12,646  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e6a69ea, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1a240293 created in the thread with id: 1060
2024-04-30T20:37:12,648  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4e6a69ea from thread id: 1060
2024-04-30T20:37:12,648  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:12,648  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:12,648  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:12,662  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-30T20:37:12,667  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-30T20:37:12,668  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-30T20:37:12,668  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:12,668  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-30T20:37:12,668  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-30T20:37:12,688  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-30T20:37:12,703  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1307699259_0018
2024-04-30T20:37:12,703  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-30T20:37:12,760  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-30T20:37:12,760  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-30T20:37:12,764  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,764  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,765  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-30T20:37:12,766  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,766  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,777  INFO [Thread-994] mapred.LocalJobRunner: Waiting for map tasks
2024-04-30T20:37:12,777  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1307699259_0018_m_000000_0
2024-04-30T20:37:12,782  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,782  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,784  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,784  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,784  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-30T20:37:12,785  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 23
Input split[0]:
   Length = 23
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-30T20:37:12,785  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-30T20:37:12,786  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/input.data:0+23
2024-04-30T20:37:12,788  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,788  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,790  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-30T20:37:12,791  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-30T20:37:12,791  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1] C:  R: 
2024-04-30T20:37:12,803  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:12,803  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1307699259_0018_m_000000_0 is done. And is in the process of committing
2024-04-30T20:37:12,804  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,804  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,805  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:12,806  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1307699259_0018_m_000000_0 is allowed to commit now
2024-04-30T20:37:12,807  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:12,807  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:12,808  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1307699259_0018_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/junit_unparted/_SCRATCH0,9746405896438506/b=1
2024-04-30T20:37:12,808  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-30T20:37:12,808  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1307699259_0018_m_000000_0' done.
2024-04-30T20:37:12,808  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1307699259_0018_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=61554
		FILE: Number of bytes written=10282612
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=997195776
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-30T20:37:12,808  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1307699259_0018_m_000000_0
2024-04-30T20:37:12,809  INFO [Thread-994] mapred.LocalJobRunner: map task executor complete.
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,853  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,853  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:12,854  INFO [Thread-994] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:12,854  INFO [Thread-994] metastore.HMSHandler: 27: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:12,854  INFO [Thread-994] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:12,855  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a524379, with PersistenceManager: null will be shutdown
2024-04-30T20:37:12,855  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a524379, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@21a40e2b created in the thread with id: 1080
2024-04-30T20:37:12,856  INFO [Thread-994] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6a524379 from thread id: 1080
2024-04-30T20:37:12,856  INFO [Thread-994] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:12,856  INFO [Thread-994] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:12,857  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,896  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,897  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_unparted has new partitions [{b=1}].
2024-04-30T20:37:12,898  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_unparted[1]	
2024-04-30T20:37:12,905  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-30T20:37:12,905  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-30T20:37:12,911  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_unparted
2024-04-30T20:37:12,911  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 265
2024-04-30T20:37:12,949  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:12,986  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:12,987  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:12,987  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:12,988  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-30T20:37:13,124  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1307699259_0018
2024-04-30T20:37:13,124  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A
2024-04-30T20:37:13,124  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1] C:  R: 
2024-04-30T20:37:13,126  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,127  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,127  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,142  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-30T20:37:13,142  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-30 20:37:12	2024-04-30 20:37:13	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1307699259_0018	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A	MAP_ONLY	default.junit_unparted,

Input(s):
Successfully read 11 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/input.data"

Output(s):
Successfully stored 11 records in: "default.junit_unparted"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1307699259_0018


2024-04-30T20:37:13,143  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,144  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,145  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:13,178  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:13,179  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:13,180  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:13,185  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:13,208  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:13,208  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:13,209  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:13,214  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,215  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-04-30T20:37:13,224  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-30T20:37:13,224  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-30T20:37:13,224  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-30T20:37:13,225  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-04-30T20:37:13,225  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-30T20:37:13,231  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,232  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-30T20:37:13,232  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:13,245  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:13,245  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:13,246  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:13,247  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:13,248  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:13,253  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,253  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_by_filter : tbl=hive.default.junit_unparted	
2024-04-30T20:37:13,286  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-04-30T20:37:13,290  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-30T20:37:13,291  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,298  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-30T20:37:13,303  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-30T20:37:13,325  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, test.data.files=${hive.root}/data/files, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-04-30T20:37:13,330  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:13,330  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-30T20:37:13,350  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-30T20:37:13,364  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1384846818_0019
2024-04-30T20:37:13,364  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-30T20:37:13,412  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-30T20:37:13,412  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-30T20:37:13,415  INFO [Thread-1036] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:13,415  INFO [Thread-1036] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:13,416  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-30T20:37:13,425  INFO [Thread-1036] mapred.LocalJobRunner: Waiting for map tasks
2024-04-30T20:37:13,425  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1384846818_0019_m_000000_0
2024-04-30T20:37:13,428  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:13,428  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:13,429  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-30T20:37:13,430  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 265
Input split[0]:
   Length = 265
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-04-30T20:37:13,432  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@8992397
2024-04-30T20:37:13,433  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:13,433  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:13,446  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.avro.AvroSerDe with properties {name=default.junit_unparted, columns.types=int, serialization.format=1, columns=a, columns.comments=, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.avro.AvroSerDe, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1714534632}
2024-04-30T20:37:13,446  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-30T20:37:13,446  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-30T20:37:13,446  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: B[3,4] C:  R: 
2024-04-30T20:37:13,447  WARN [LocalJobRunner Map Task Executor #0] avro.AvroDeserializer: Received different schemas.  Have to re-encode: {"type":"record","name":"junit_unparted","fields":[{"name":"a","type":["null","int"],"default":null}]}
SIZE{-8e29414:18f323a201c:-7fed=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@70385bdd} ID -8e29414:18f323a201c:-7fed
2024-04-30T20:37:13,448  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:13,448  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1384846818_0019_m_000000_0 is done. And is in the process of committing
2024-04-30T20:37:13,449  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:13,449  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1384846818_0019_m_000000_0 is allowed to commit now
2024-04-30T20:37:13,450  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1384846818_0019_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714534622873/pig/temp/temp-290106102/tmp-946473410
2024-04-30T20:37:13,451  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-30T20:37:13,451  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1384846818_0019_m_000000_0' done.
2024-04-30T20:37:13,451  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1384846818_0019_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=65145
		FILE: Number of bytes written=10846700
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=2102
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=999292928
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-30T20:37:13,451  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1384846818_0019_m_000000_0
2024-04-30T20:37:13,451  INFO [Thread-1036] mapred.LocalJobRunner: map task executor complete.
2024-04-30T20:37:13,612  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1384846818_0019
2024-04-30T20:37:13,612  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases B
2024-04-30T20:37:13,612  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: B[3,4] C:  R: 
2024-04-30T20:37:13,614  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,615  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,615  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,617  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-30T20:37:13,617  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-30 20:37:13	2024-04-30 20:37:13	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1384846818_0019	1	0	n/a	n/a	n/a	n/a	0	0	0	0	B	MAP_ONLY	file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714534622873/pig/temp/temp-290106102/tmp-946473410,

Input(s):
Successfully read 11 records from: "default.junit_unparted"

Output(s):
Successfully stored 11 records in: "file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714534622873/pig/temp/temp-290106102/tmp-946473410"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1384846818_0019


2024-04-30T20:37:13,618  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,619  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:13,619  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-30T20:37:13,619  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-30T20:37:13,626  INFO [main] input.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:13,626  INFO [main] util.MapRedUtil: Total input paths to process : 1
2024-04-30T20:37:13,627  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-04-30T20:37:13,631  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testWriteChar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.128">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:13,675  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:13,677  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:13,677  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:13,677  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5bbfd031 will be shutdown
2024-04-30T20:37:13,678  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7445fdb2 created in the thread with id: 1
2024-04-30T20:37:13,680  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 335555bb-e85b-4c64-b3cc-9f63c6ae8ce9
2024-04-30T20:37:13,680  INFO [main] SessionState: Hive Session ID = 335555bb-e85b-4c64-b3cc-9f63c6ae8ce9
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:13,680  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:13,686  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/335555bb-e85b-4c64-b3cc-9f63c6ae8ce9
2024-04-30T20:37:13,689  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/335555bb-e85b-4c64-b3cc-9f63c6ae8ce9
2024-04-30T20:37:13,691  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/335555bb-e85b-4c64-b3cc-9f63c6ae8ce9/_tmp_space.db
2024-04-30T20:37:13,692  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): drop table if exists junitTypeTest1
2024-04-30T20:37:13,693  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:13,700  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,700  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:13,700  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:13,700  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:13,700  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getTable_(GetTableRequest)=7, flushCache_()=0}
2024-04-30T20:37:13,700  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.008 seconds
2024-04-30T20:37:13,700  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:13,700  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:13,700  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): drop table if exists junitTypeTest1
2024-04-30T20:37:13,701  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:13,701  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:13,707  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,708  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:13,714  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:13,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:13,734  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:13,734  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=26, getTable_(GetTableRequest)=6}
2024-04-30T20:37:13,734  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.034 seconds
2024-04-30T20:37:13,734  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): use default
2024-04-30T20:37:13,735  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:13,736  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:13,736  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getDatabase_(String)=1, flushCache_()=0}
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.002 seconds
2024-04-30T20:37:13,736  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): use default
2024-04-30T20:37:13,736  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:13,737  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:13,738  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:13,738  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:13,739  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-04-30T20:37:13,739  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.002 seconds
2024-04-30T20:37:13,739  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:13,740  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e
2024-04-30T20:37:13,740  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:13,740  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=335555bb-e85b-4c64-b3cc-9f63c6ae8ce9, clientType=HIVECLI]
2024-04-30T20:37:13,740  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:13,741  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:13,741  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@58409751, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7445fdb2 will be shutdown
2024-04-30T20:37:13,741  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:13,741  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-04-30T20:37:13,742  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:13,743  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:13,743  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:13,743  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da, with PersistenceManager: null will be shutdown
2024-04-30T20:37:13,743  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2090d3d1 created in the thread with id: 1
2024-04-30T20:37:13,745  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da from thread id: 1
2024-04-30T20:37:13,745  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:13,745  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:13,745  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-30T20:37:13,746  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:13,746  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:13,746  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2090d3d1 will be shutdown
2024-04-30T20:37:13,746  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7755382a created in the thread with id: 1
2024-04-30T20:37:13,748  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:13,748  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:13,748  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:13,749  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:13,749  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:13,749  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.01 seconds
2024-04-30T20:37:13,749  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:13,749  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:13,749  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:13,750  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:13,750  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4ecbd1da, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7755382a will be shutdown
2024-04-30T20:37:13,750  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:13,750  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-04-30T20:37:13,760 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:13,761 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:13,763  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-30T20:37:13,763  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-30T20:37:13,764  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:13,764 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:13,764  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:13,764  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:13,764  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203713_d87b0f98-93ea-48c4-95e8-5fde86bc443e); Time taken: 0.015 seconds
]]></system-err>
  </testcase>
  <testcase name="testWriteDate" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.502"/>
  <testcase name="testDynamicPartitioningMultiPartColsNoDataInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.759"/>
  <testcase name="testWriteVarchar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.109">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:20,063  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:20,064  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:20,064  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:20,064  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:20,064  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:20,064  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:20,065  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:20,065  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:20,066  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@270706e3 will be shutdown
2024-04-30T20:37:20,066  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@499a93a9 created in the thread with id: 1
2024-04-30T20:37:20,067  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = b4ee50d4-f82a-4120-9333-c7d9d918f76b
2024-04-30T20:37:20,068  INFO [main] SessionState: Hive Session ID = b4ee50d4-f82a-4120-9333-c7d9d918f76b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:20,068  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:20,074  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/b4ee50d4-f82a-4120-9333-c7d9d918f76b
2024-04-30T20:37:20,076  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/b4ee50d4-f82a-4120-9333-c7d9d918f76b
2024-04-30T20:37:20,079  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/b4ee50d4-f82a-4120-9333-c7d9d918f76b/_tmp_space.db
2024-04-30T20:37:20,080  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): drop table if exists junitTypeTest1
2024-04-30T20:37:20,080  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:20,086  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:20,086  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:20,086  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=6, flushCache_()=0}
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.006 seconds
2024-04-30T20:37:20,086  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): drop table if exists junitTypeTest1
2024-04-30T20:37:20,086  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:20,087  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:20,093  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:20,093  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:20,097  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:20,097  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:20,117  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:20,117  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=24, getTable_(GetTableRequest)=6}
2024-04-30T20:37:20,117  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.031 seconds
2024-04-30T20:37:20,117  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): use default
2024-04-30T20:37:20,118  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:20,119  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:20,119  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:20,119  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:20,119  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-30T20:37:20,119  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.002 seconds
2024-04-30T20:37:20,119  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:20,119  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:20,119  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): use default
2024-04-30T20:37:20,120  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:20,120  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:20,121  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:20,121  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:20,121  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-30T20:37:20,121  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.002 seconds
2024-04-30T20:37:20,122  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:20,123  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36
2024-04-30T20:37:20,123  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:20,123  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b4ee50d4-f82a-4120-9333-c7d9d918f76b, clientType=HIVECLI]
2024-04-30T20:37:20,123  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:20,123  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:20,123  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@499a93a9 will be shutdown
2024-04-30T20:37:20,123  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:20,123  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-04-30T20:37:20,124  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:20,125  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:20,125  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:20,125  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135, with PersistenceManager: null will be shutdown
2024-04-30T20:37:20,126  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3c2e5cb6 created in the thread with id: 1
2024-04-30T20:37:20,128  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135 from thread id: 1
2024-04-30T20:37:20,128  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:20,128  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:20,128  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-30T20:37:20,129  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:20,129  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:20,130  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3c2e5cb6 will be shutdown
2024-04-30T20:37:20,130  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c0e260 created in the thread with id: 1
2024-04-30T20:37:20,132  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:20,132  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:20,132  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:20,133  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:20,133  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:20,133  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.011 seconds
2024-04-30T20:37:20,133  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:20,133  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:20,134  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:20,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:20,134  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@21247135, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2c0e260 will be shutdown
2024-04-30T20:37:20,134  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:20,134  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-04-30T20:37:20,135 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:20,135 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:20,136  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-30T20:37:20,136  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-30T20:37:20,136  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:20,136 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:20,136  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:20,136  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:20,136  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203720_41621aa5-8e33-44b4-977e-75fd26d48e36); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testBagNStruct" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.003"/>
  <testcase name="testStaticPartitioningMultiPartCols" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.018"/>
  <testcase name="testPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.228"/>
  <testcase name="testStoreWithNoCtorArgs" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.011"/>
  <testcase name="testWriteDecimalX" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.078">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:24,436  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:24,439  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:24,439  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:24,439  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b4efe16, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@148aeed will be shutdown
2024-04-30T20:37:24,439  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b4efe16, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@54a9cb38 created in the thread with id: 1
2024-04-30T20:37:24,441  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 9e669ab1-b2c1-4743-9ff8-e2550097cd50
2024-04-30T20:37:24,441  INFO [main] SessionState: Hive Session ID = 9e669ab1-b2c1-4743-9ff8-e2550097cd50
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:24,441  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:24,448  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/9e669ab1-b2c1-4743-9ff8-e2550097cd50
2024-04-30T20:37:24,450  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/9e669ab1-b2c1-4743-9ff8-e2550097cd50
2024-04-30T20:37:24,453  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/9e669ab1-b2c1-4743-9ff8-e2550097cd50/_tmp_space.db
2024-04-30T20:37:24,454  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): drop table if exists junitTypeTest1
2024-04-30T20:37:24,454  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:24,455  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:24,455  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:24,455  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:24,455  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-30T20:37:24,455  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.001 seconds
2024-04-30T20:37:24,455  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:24,456  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:24,456  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): drop table if exists junitTypeTest1
2024-04-30T20:37:24,456  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:24,456  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:24,457  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:24,457  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-30T20:37:24,457  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.001 seconds
2024-04-30T20:37:24,457  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): use default
2024-04-30T20:37:24,458  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:24,458  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:24,458  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:24,458  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:24,458  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=0, flushCache_()=0}
2024-04-30T20:37:24,458  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.001 seconds
2024-04-30T20:37:24,459  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:24,459  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:24,459  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): use default
2024-04-30T20:37:24,459  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:24,459  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:24,460  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:24,460  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:24,460  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-30T20:37:24,460  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.001 seconds
2024-04-30T20:37:24,461  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:24,461  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa
2024-04-30T20:37:24,461  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:24,462  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9e669ab1-b2c1-4743-9ff8-e2550097cd50, clientType=HIVECLI]
2024-04-30T20:37:24,462  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:24,463  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:24,463  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4b4efe16, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@54a9cb38 will be shutdown
2024-04-30T20:37:24,463  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:24,463  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -36
2024-04-30T20:37:24,464  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:24,464  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:24,464  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:24,465  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165, with PersistenceManager: null will be shutdown
2024-04-30T20:37:24,465  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d667774 created in the thread with id: 1
2024-04-30T20:37:24,466  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165 from thread id: 1
2024-04-30T20:37:24,466  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:24,466  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:24,466  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-30T20:37:24,468  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:24,468  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:24,469  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4d667774 will be shutdown
2024-04-30T20:37:24,469  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@633b98b6 created in the thread with id: 1
2024-04-30T20:37:24,470  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:24,470  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:24,471  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:24,471  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa
2024-04-30T20:37:24,471  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:24,471  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:24,471  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:24,471  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:24,472  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.01 seconds
2024-04-30T20:37:24,472  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:24,472  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:24,472  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:24,472  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:24,472  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:24,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:24,472  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6332d165, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@633b98b6 will be shutdown
2024-04-30T20:37:24,472  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:24,472  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -37
2024-04-30T20:37:24,473 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:24,473 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:24,474  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-30T20:37:24,474  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-30T20:37:24,474  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:24,474 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:24,474  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:24,474  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:24,474  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203724_593edc29-3bae-4ad9-80a9-d0808538a0aa); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testPartitionPublish" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.575"/>
  <testcase name="testWriteDecimal" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.072">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,087  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,090  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,090  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,090  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ff0f19e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@76a26976 will be shutdown
2024-04-30T20:37:25,090  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ff0f19e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@137c13bf created in the thread with id: 1
2024-04-30T20:37:25,092  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 44807330-fb69-4877-ae5a-eeafb4c5be41
2024-04-30T20:37:25,092  INFO [main] SessionState: Hive Session ID = 44807330-fb69-4877-ae5a-eeafb4c5be41
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:25,092  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:25,098  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/44807330-fb69-4877-ae5a-eeafb4c5be41
2024-04-30T20:37:25,100  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/44807330-fb69-4877-ae5a-eeafb4c5be41
2024-04-30T20:37:25,103  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/44807330-fb69-4877-ae5a-eeafb4c5be41/_tmp_space.db
2024-04-30T20:37:25,104  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): drop table if exists junitTypeTest1
2024-04-30T20:37:25,104  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,105  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,105  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.001 seconds
2024-04-30T20:37:25,105  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): drop table if exists junitTypeTest1
2024-04-30T20:37:25,105  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,106  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-04-30T20:37:25,106  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,106  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-04-30T20:37:25,107  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.001 seconds
2024-04-30T20:37:25,107  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): use default
2024-04-30T20:37:25,107  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,108  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,108  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.001 seconds
2024-04-30T20:37:25,108  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): use default
2024-04-30T20:37:25,108  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,109  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,109  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,110  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,110  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-30T20:37:25,110  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.001 seconds
2024-04-30T20:37:25,110  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:25,111  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4
2024-04-30T20:37:25,111  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:25,111  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=44807330-fb69-4877-ae5a-eeafb4c5be41, clientType=HIVECLI]
2024-04-30T20:37:25,111  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:25,111  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,111  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@6ff0f19e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@137c13bf will be shutdown
2024-04-30T20:37:25,111  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,111  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -42
2024-04-30T20:37:25,112  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,112  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,113  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,113  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,113  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19380c83 created in the thread with id: 1
2024-04-30T20:37:25,114  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6 from thread id: 1
2024-04-30T20:37:25,114  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,114  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,114  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-04-30T20:37:25,115  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,115  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,115  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19380c83 will be shutdown
2024-04-30T20:37:25,115  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b37b35d created in the thread with id: 1
2024-04-30T20:37:25,117  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,117  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,117  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,118  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,118  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,118  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.008 seconds
2024-04-30T20:37:25,118  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:25,118  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,118  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:25,119  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,119  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@752f0fa6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2b37b35d will be shutdown
2024-04-30T20:37:25,119  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,119  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -43
2024-04-30T20:37:25,119 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:25,120 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:25,121  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-04-30T20:37:25,121  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-04-30T20:37:25,121  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:25,121 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-04-30T20:37:25,121  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,121  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:25,121  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_219f2bc2-3018-44b5-a5f4-85b0d680c7b4); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testMultiPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.544">
    <failure message="expected:&lt;1&gt; but was:&lt;5&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<5>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,159  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,160  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,161  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,161  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,161  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,161  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@658e67e1 created in the thread with id: 1
2024-04-30T20:37:25,162  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e from thread id: 1
2024-04-30T20:37:25,162  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = e3733952-472b-404e-802d-82909a6733d1
2024-04-30T20:37:25,163  INFO [main] SessionState: Hive Session ID = e3733952-472b-404e-802d-82909a6733d1
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:25,163  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-04-30T20:37:25,168  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e3733952-472b-404e-802d-82909a6733d1
2024-04-30T20:37:25,171  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/e3733952-472b-404e-802d-82909a6733d1
2024-04-30T20:37:25,174  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e3733952-472b-404e-802d-82909a6733d1/_tmp_space.db
2024-04-30T20:37:25,174  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): drop table if exists employee
2024-04-30T20:37:25,175  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,175  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,175  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@658e67e1 will be shutdown
2024-04-30T20:37:25,175  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@542c8575 created in the thread with id: 1
2024-04-30T20:37:25,177  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,177  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,177  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,178  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,178  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.004 seconds
2024-04-30T20:37:25,178  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): drop table if exists employee
2024-04-30T20:37:25,178  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,179  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,179  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,179  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-04-30T20:37:25,179  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.001 seconds
2024-04-30T20:37:25,180  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): use default
2024-04-30T20:37:25,180  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,181  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,181  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.001 seconds
2024-04-30T20:37:25,181  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): use default
2024-04-30T20:37:25,181  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,181  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,182  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,182  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,182  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-04-30T20:37:25,182  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.001 seconds
2024-04-30T20:37:25,182  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:25,183  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac
2024-04-30T20:37:25,183  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:25,183  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e3733952-472b-404e-802d-82909a6733d1, clientType=HIVECLI]
2024-04-30T20:37:25,183  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-04-30T20:37:25,183  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,183  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5ebaad5e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@542c8575 will be shutdown
2024-04-30T20:37:25,184  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,184  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-04-30T20:37:25,184  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,185  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,185  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,185  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,185  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@28a7ef5d created in the thread with id: 1
2024-04-30T20:37:25,187  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765 from thread id: 1
2024-04-30T20:37:25,187  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,187  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,187  INFO [main] parse.CalcitePlanner: Creating table default.employee position=13
2024-04-30T20:37:25,188  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,188  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,188  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@28a7ef5d will be shutdown
2024-04-30T20:37:25,188  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@145fe5ce created in the thread with id: 1
2024-04-30T20:37:25,190  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,190  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,190  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-04-30T20:37:25,191  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-04-30T20:37:25,191  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:25,191  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.009 seconds
2024-04-30T20:37:25,191  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-04-30T20:37:25,191  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-04-30T20:37:25,191  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-04-30T20:37:25,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,191  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7dfbc765, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@145fe5ce will be shutdown
2024-04-30T20:37:25,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,191  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-04-30T20:37:25,193  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,193  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,193  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,194  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b0ed197, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,194  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b0ed197, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7946db67 created in the thread with id: 1
2024-04-30T20:37:25,195  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5b0ed197 from thread id: 1
2024-04-30T20:37:25,196  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,196  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,196  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:employee, dbName:default, owner:alex, createTime:1714534645, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:emp_id, type:int, comment:null), FieldSchema(name:emp_name, type:string, comment:null), FieldSchema(name:emp_start_date, type:string, comment:null), FieldSchema(name:emp_gender, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:emp_country, type:string, comment:null), FieldSchema(name:emp_state, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-04-30T20:37:25,197  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/employee
2024-04-30T20:37:25,212  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:25,212  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=16}
2024-04-30T20:37:25,212  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.021 seconds
2024-04-30T20:37:25,234  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-04-30T20:37:25,235  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-88b97e84-47b0-4a8a-a579-34e1c6a52411
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,333  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,333  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:25,334  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:25,335  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,336  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,339  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:25,359  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,360  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,360  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,360  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,375  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,376  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:25,376  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:25,377  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,378  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,382  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:25,402  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,403  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,403  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,403  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,418  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,418  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:25,419  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:25,420  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,421  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,427  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:25,446  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,446  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,446  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,446  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,446  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,447  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,447  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,447  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,462  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,463  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-04-30T20:37:25,463  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-04-30T20:37:25,464  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,465  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:25,469  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:25,488  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,488  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,489  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,489  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,490  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,492  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-04-30T20:37:25,500  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-04-30T20:37:25,500  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-04-30T20:37:25,504  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-04-30T20:37:25,505  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 5
2024-04-30T20:37:25,506  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 map-only splittees.
2024-04-30T20:37:25,506  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 out of total 5 MR operators.
2024-04-30T20:37:25,506  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-04-30T20:37:25,511  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:25,512  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-04-30T20:37:25,512  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-04-30T20:37:25,513  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-04-30T20:37:25,513  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-04-30T20:37:25,513  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-04-30T20:37:25,513  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714534645513-0
2024-04-30T20:37:25,521  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-04-30T20:37:25,523  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,538  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,539  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,539  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,539  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,539  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,539  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,539  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,539  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,540  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,540  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,540  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cbe3de5, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,540  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cbe3de5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@29b75107 created in the thread with id: 1969
2024-04-30T20:37:25,541  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cbe3de5 from thread id: 1969
2024-04-30T20:37:25,541  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,542  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,542  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,542  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@cbe3de5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@29b75107 will be shutdown
2024-04-30T20:37:25,542  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,542  INFO [JobControl] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-04-30T20:37:25,542  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,543  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,543  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52f68b9a, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,543  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52f68b9a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@666320ee created in the thread with id: 1969
2024-04-30T20:37:25,544  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@52f68b9a from thread id: 1969
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,560  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,561  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,561  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,561  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,579  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,580  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,580  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,580  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,597  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,597  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,598  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,611  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-04-30T20:37:25,616  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-04-30T20:37:25,618  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-04-30T20:37:25,619  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:25,619  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-04-30T20:37:25,619  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-04-30T20:37:25,639  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-04-30T20:37:25,653  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1254714432_0037
2024-04-30T20:37:25,653  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-04-30T20:37:25,708  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-30T20:37:25,709  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-30T20:37:25,713  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,713  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,716  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,716  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,718  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,718  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,721  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,721  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,722  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-04-30T20:37:25,723  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,723  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,736  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,736  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,750  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,750  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,764  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,764  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,777  INFO [Thread-1843] mapred.LocalJobRunner: Waiting for map tasks
2024-04-30T20:37:25,778  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1254714432_0037_m_000000_0
2024-04-30T20:37:25,784  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,784  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,787  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,787  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,790  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,790  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,793  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,793  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,795  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,795  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,796  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,796  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,798  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,798  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,799  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,799  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,799  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-04-30T20:37:25,799  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 132
Input split[0]:
   Length = 132
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-04-30T20:37:25,801  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-04-30T20:37:25,801  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/input.data:0+132
2024-04-30T20:37:25,802  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-04-30T20:37:25,802  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-04-30T20:37:25,805  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-30T20:37:25,808  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,808  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,815  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,815  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,819  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,819  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,824  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,824  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,869  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:25,869  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1254714432_0037_m_000000_0 is done. And is in the process of committing
2024-04-30T20:37:25,870  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,870  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,873  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-04-30T20:37:25,873  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1254714432_0037_m_000000_0 is allowed to commit now
2024-04-30T20:37:25,874  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,874  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,874  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1254714432_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/employee/_SCRATCH0,9434742929452588/emp_country=IN/emp_state=TN
2024-04-30T20:37:25,875  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,875  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,876  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1254714432_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/employee/_SCRATCH0,45354900780413443/emp_country=IN/emp_state=KA
2024-04-30T20:37:25,877  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,877  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,878  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1254714432_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/employee/_SCRATCH0,6624899656922602/emp_country=IN/emp_state=KL
2024-04-30T20:37:25,879  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-04-30T20:37:25,879  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-04-30T20:37:25,880  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1254714432_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/warehouse/employee/_SCRATCH0,8875306011954771/emp_country=IN/emp_state=AP
2024-04-30T20:37:25,880  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-04-30T20:37:25,880  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1254714432_0037_m_000000_0' done.
2024-04-30T20:37:25,880  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1254714432_0037_m_000000_0: Counters: 19
	File System Counters
		FILE: Number of bytes read=107859
		FILE: Number of bytes written=21155071
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=0
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=958398464
	MultiStoreCounters
		Output records in _0_employee=1
		Output records in _1_employee=1
		Output records in _2_employee=1
		Output records in _3_employee=1
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-04-30T20:37:25,880  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1254714432_0037_m_000000_0
2024-04-30T20:37:25,880  INFO [Thread-1843] mapred.LocalJobRunner: map task executor complete.
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,924  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,925  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:25,925  INFO [Thread-1843] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-04-30T20:37:25,926  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,926  INFO [Thread-1843] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-04-30T20:37:25,926  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c0da9fe, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,926  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c0da9fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ee9717b created in the thread with id: 1989
2024-04-30T20:37:25,927  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c0da9fe from thread id: 1989
2024-04-30T20:37:25,927  INFO [Thread-1843] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-04-30T20:37:25,928  INFO [Thread-1843] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-04-30T20:37:25,928  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-04-30T20:37:25,928  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c0da9fe, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6ee9717b will be shutdown
2024-04-30T20:37:25,928  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:25,928  INFO [Thread-1843] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-04-30T20:37:25,928  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:25,929  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-04-30T20:37:25,929  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@673365f4, with PersistenceManager: null will be shutdown
2024-04-30T20:37:25,929  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@673365f4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@a127872 created in the thread with id: 1989
2024-04-30T20:37:25,930  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@673365f4 from thread id: 1989
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:25,977  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:25,978  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=TN}].
2024-04-30T20:37:25,979  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,TN]	
2024-04-30T20:37:25,984  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-30T20:37:25,984  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-30T20:37:25,988  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-30T20:37:25,988  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 458
2024-04-30T20:37:26,017  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-30T20:37:26,022  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1254714432_0037
2024-04-30T20:37:26,022  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,AP,KA,KL,TN
2024-04-30T20:37:26,022  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-04-30T20:37:26,023  INFO [main] mapReduceLayer.MapReduceLauncher: 50% complete
2024-04-30T20:37:26,023  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1254714432_0037]
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,054  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,055  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,055  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,055  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,055  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,056  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,100  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,100  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,101  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,110  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KA}].
2024-04-30T20:37:26,111  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KA]	
2024-04-30T20:37:26,113  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-30T20:37:26,113  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-30T20:37:26,116  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-30T20:37:26,117  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 458
2024-04-30T20:37:26,139  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-30T20:37:26,202  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:26,202  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-04-30T20:37:26,202  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:26,202  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-04-30T20:37:26,203  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-04-30T20:37:26,203  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,212  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,212  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,213  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,214  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,258  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,259  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,259  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,259  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,259  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,259  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,259  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,259  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,269  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KL}].
2024-04-30T20:37:26,270  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KL]	
2024-04-30T20:37:26,272  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-30T20:37:26,272  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-30T20:37:26,276  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-30T20:37:26,276  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 456
2024-04-30T20:37:26,299  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,336  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,336  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,337  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,338  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,381  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,382  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,382  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,391  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=AP}].
2024-04-30T20:37:26,392  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,AP]	
2024-04-30T20:37:26,394  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-04-30T20:37:26,394  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-04-30T20:37:26,398  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-04-30T20:37:26,398  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 456
2024-04-30T20:37:26,422  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-04-30T20:37:26,459  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-04-30T20:37:26,460  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-04-30T20:37:26,460  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-04-30T20:37:26,461  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-04-30T20:37:26,525  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:26,525  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:26,526  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:26,539  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-30T20:37:26,540  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:26,540  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-30T20:37:26,540  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:26,540  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-30T20:37:26,541  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:26,541  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-04-30T20:37:26,541  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-04-30T20:37:26,541  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-04-30T20:37:26,542  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-04-30 20:37:25	2024-04-30 20:37:26	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1254714432_0037	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,AP,KA,KL,TN	MULTI_QUERY,MAP_ONLY	employee,employee,employee,employee,

Input(s):
Successfully read 4 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714534607152/input.data"

Output(s):
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"

Counters:
Total records written : 4
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1254714432_0037


2024-04-30T20:37:26,543  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:26,543  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-04-30T20:37:26,544  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-04-30T20:37:26,544  INFO [main] ql.Driver: Compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): select * from employee
2024-04-30T20:37:26,545  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac
2024-04-30T20:37:26,545  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-04-30T20:37:26,545  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-04-30T20:37:26,545  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-30T20:37:26,545  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:26,549  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-04-30T20:37:26,549  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-30T20:37:26,549  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-30T20:37:26,550  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-04-30T20:37:26,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.employee	
2024-04-30T20:37:26,557  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.employee, projIndxSet: [0, 1, 2, 3, 4, 5], allowMissingStats: true
2024-04-30T20:37:26,558  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.employee	
2024-04-30T20:37:26,581  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.employee	
2024-04-30T20:37:26,590  WARN [main] calcite.RelOptHiveTable: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-04-30T20:37:26,590  INFO [main] SessionState: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-04-30T20:37:26,620  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-04-30T20:37:26,621  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-04-30T20:37:26,621  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-04-30T20:37:26,622  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/e3733952-472b-404e-802d-82909a6733d1/hive_2024-04-30_20-37-26_544_4653135083184428557-1/-mr-10001/.hive-staging_hive_2024-04-30_20-37-26_544_4653135083184428557-1
2024-04-30T20:37:26,631  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-04-30T20:37:26,632  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-04-30T20:37:26,633  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-04-30T20:37:26,633  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac
2024-04-30T20:37:26,633  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-04-30T20:37:26,633  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:employee.emp_id, type:int, comment:null), FieldSchema(name:employee.emp_name, type:string, comment:null), FieldSchema(name:employee.emp_start_date, type:string, comment:null), FieldSchema(name:employee.emp_gender, type:string, comment:null), FieldSchema(name:employee.emp_country, type:string, comment:null), FieldSchema(name:employee.emp_state, type:string, comment:null)], properties:null)
2024-04-30T20:37:26,634  INFO [main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-04-30T20:37:26,634  INFO [main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-04-30T20:37:26,634  INFO [main] exec.SelectOperator: SELECT struct<emp_id:int,emp_name:string,emp_start_date:string,emp_gender:string,emp_country:string,emp_state:string>
2024-04-30T20:37:26,634  INFO [main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-04-30T20:37:26,634  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-04-30T20:37:26,634  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, listPartitions_(String, String, short)=23, getAllTableConstraints_(AllTableConstraintsRequest)=1, getTable_(GetTableRequest)=4, flushCache_()=0, getAggrColStatsFor_(String, String, List, List, String, String)=9}
2024-04-30T20:37:26,634  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.09 seconds
2024-04-30T20:37:26,634  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-04-30T20:37:26,634  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-04-30T20:37:26,635  INFO [main] ql.Driver: Executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac): select * from employee
2024-04-30T20:37:26,635  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-04-30T20:37:26,635  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-04-30T20:37:26,635  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240430203725_9890d263-c233-4989-af92-707f852b63ac); Time taken: 0.001 seconds
2024-04-30T20:37:26,639  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:26,639  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-30T20:37:26,644  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:26,645  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-30T20:37:26,645  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-30T20:37:26,651  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:26,651  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-30T20:37:26,651  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-30T20:37:26,657  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-04-30T20:37:26,658  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-04-30T20:37:26,658  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-04-30T20:37:26,660  INFO [main] exec.TableScanOperator: Closing Operator: TS[0]
2024-04-30T20:37:26,660  INFO [main] exec.TableScanOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_TS_0:4, 
2024-04-30T20:37:26,660  INFO [main] exec.SelectOperator: Closing Operator: SEL[1]
2024-04-30T20:37:26,660  INFO [main] exec.SelectOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_SEL_1:4, 
2024-04-30T20:37:26,660  INFO [main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-04-30T20:37:26,660  INFO [main] exec.ListSinkOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_LIST_SINK_3:4, 
2024-04-30T20:37:26,660  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-04-30T20:37:26,665  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testStaticPartitioningMultiPartColsNoData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.163"/>
  <testcase name="testStoreMultiTables" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.275"/>
  <testcase name="testStoreWithNoSchema" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.767"/>
  <testcase name="testNoAlias" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.254"/>
  <testcase name="testStoreFuncSimple" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.728"/>
</testsuite>