<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="49.656" tests="29" errors="4" skipped="5" failures="2">
  <properties>
    <property name="sun.desktop" value="gnome"/>
    <property name="build.test.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="test.tmp.dir.uri" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="qfile" value=""/>
    <property name="log4j.configurationFile" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="test.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/warehouse"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="run_disabled" value=""/>
    <property name="java.vm.vendor" value="Private Build"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="hadoop.version" value="3.1.0"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="user.timezone" value=""/>
    <property name="user.country.format" value="PT"/>
    <property name="test.local.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/localfs/warehouse"/>
    <property name="os.name" value="Linux"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="test.tmp.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="test.build.data" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="clustermode" value=""/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="user.country" value="US"/>
    <property name="test.data.files" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter3568223618768778235.jar /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire 2024-05-01T18-25-52_277-jvmRun1 surefire4146867810564264253tmp surefire_9454292110286307763181tmp"/>
    <property name="surefire.test.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/alex"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/java-8-openjdk-amd64/jre"/>
    <property name="java.security.krb5.conf" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/krb5.conf"/>
    <property name="file.separator" value="/"/>
    <property name="basedir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="test.dfs.mkdir" value=""/>
    <property name="derby.stream.error.file" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/derby.log"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="surefire.real.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter3568223618768778235.jar"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes"/>
    <property name="hadoop.log.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="log4j.debug" value="true"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="spark.home" value=""/>
    <property name="java.runtime.version" value="1.8.0_402-8u402-ga-2ubuntu1~22.04-b06"/>
    <property name="user.name" value="alex"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="test.src.tables" value=""/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="6.5.0-28-generic"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="hive.test.console.log.level" value="INFO"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="qfile_regex" value=""/>
    <property name="hadoop.bin.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../testutils/hadoop"/>
    <property name="localRepository" value="/home/alex/.m2/repository"/>
    <property name="build.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target"/>
    <property name="mapred.job.tracker" value="local"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="java.io.tmpdir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="maven.local.repository" value="/home/alex/.m2/repository"/>
    <property name="java.version" value="1.8.0_402"/>
    <property name="user.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="os.arch" value="amd64"/>
    <property name="hive.version" value="4.0.0-SNAPSHOT"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="user.language.format" value="pt"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="antlr.version" value="3.5.2"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="derby.version" value="10.14.1.0"/>
    <property name="test.data.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib"/>
    <property name="java.vendor" value="Private Build"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.402-b06"/>
    <property name="java.specification.maintenance.version" value="5"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="initScript" value=""/>
    <property name="java.class.version" value="52.0"/>
    <property name="test.output.overwrite" value=""/>
    <property name="hive.root" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../"/>
  </properties>
  <testcase name="testWriteTinyint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
    <system-err><![CDATA[SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,135367 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@7f31245a
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916]...
DEBUG StatusLogger Reconfiguration started for context[name=7f31245a] at URI null (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@233795b6
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,028355 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", modulate="true", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(max="30", min="null", fileIndex="null", compressionLevel="null", ={}, stopCustomActionsOnError="null", tempCompressedFilePattern="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileName="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log", filePattern="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertise="null", advertiseURI="null", filePermissions="null", fileOwner="null", fileGroup="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log seek to 63305575
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-05-01T14:31:35.031-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/02-00:00:00.000, nextFileTime=2024/05/01-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/05/01-14:31:36.846, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/02-00:00:00.000, nextFileTime=2024/05/01-00:00:00.000, prevFileTime=2024/05/01-00:00:00.000, current=2024/05/01-14:31:36.847, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@609db546
TRACE StatusLogger Reregistering context (1/1): '7f31245a' org.apache.logging.log4j.core.LoggerContext@2b71e916
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=7f31245a] at URI /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916] started OK.
2024-05-01T14:31:37,042  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
]]></system-err>
  </testcase>
  <testcase name="testWriteSmallint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDateCharTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteTimestamp" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteDecimalXY" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDynamicPartitioningMultiPartColsInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="16.049"/>
  <testcase name="testEmptyStore" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.195"/>
  <testcase name="testStoreFuncAllSimpleTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.632"/>
  <testcase name="testDynamicPartitioningMultiPartColsInDataPartialSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.743"/>
  <testcase name="testWriteDate2" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.278"/>
  <testcase name="testWriteDate3" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="2.479"/>
  <testcase name="testStoreInPartiitonedTbl" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.295">
    <failure message="expected:&lt;1&gt; but was:&lt;2&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testStoreInPartiitonedTbl(AbstractHCatStorerTest.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,469  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,470  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,471  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,471  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67f3a068, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@674502f9 will be shutdown
2024-05-01T14:32:05,471  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67f3a068, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@30eb4739 created in the thread with id: 1
2024-05-01T14:32:05,473  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = e3c99b95-4b00-44d1-8337-e794ed67abe5
2024-05-01T14:32:05,473  INFO [main] SessionState: Hive Session ID = e3c99b95-4b00-44d1-8337-e794ed67abe5
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:05,474  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:05,479  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e3c99b95-4b00-44d1-8337-e794ed67abe5
2024-05-01T14:32:05,482  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/e3c99b95-4b00-44d1-8337-e794ed67abe5
2024-05-01T14:32:05,485  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e3c99b95-4b00-44d1-8337-e794ed67abe5/_tmp_space.db
2024-05-01T14:32:05,485  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): drop table if exists junit_unparted
2024-05-01T14:32:05,486  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:05,487  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:05,487  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:05,487  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:05,487  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T14:32:05,487  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.002 seconds
2024-05-01T14:32:05,487  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:05,487  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:05,487  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): drop table if exists junit_unparted
2024-05-01T14:32:05,488  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:05,488  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:05,489  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:05,489  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T14:32:05,489  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.002 seconds
2024-05-01T14:32:05,489  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): use default
2024-05-01T14:32:05,490  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:05,491  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:05,491  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:05,491  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:05,491  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:05,491  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.002 seconds
2024-05-01T14:32:05,491  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:05,491  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:05,491  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): use default
2024-05-01T14:32:05,492  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:05,492  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:05,494  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:05,495  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:05,495  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-05-01T14:32:05,495  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.003 seconds
2024-05-01T14:32:05,495  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:05,496  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228
2024-05-01T14:32:05,496  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:05,496  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e3c99b95-4b00-44d1-8337-e794ed67abe5, clientType=HIVECLI]
2024-05-01T14:32:05,496  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:05,496  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:05,496  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@67f3a068, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@30eb4739 will be shutdown
2024-05-01T14:32:05,497  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:05,497  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-05-01T14:32:05,497  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,498  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:05,498  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a, with PersistenceManager: null will be shutdown
2024-05-01T14:32:05,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66709528 created in the thread with id: 1
2024-05-01T14:32:05,500  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a from thread id: 1
2024-05-01T14:32:05,500  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:05,500  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:05,500  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted position=13
2024-05-01T14:32:05,501  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,501  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,501  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@66709528 will be shutdown
2024-05-01T14:32:05,501  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3035b9e6 created in the thread with id: 1
2024-05-01T14:32:05,503  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:05,503  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:05,503  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:05,504  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:05,504  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:05,504  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.009 seconds
2024-05-01T14:32:05,504  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:05,504  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:05,504  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:05,505  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:05,505  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4effe36a, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3035b9e6 will be shutdown
2024-05-01T14:32:05,505  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:05,505  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-05-01T14:32:05,506  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,507  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:05,507  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,507  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619, with PersistenceManager: null will be shutdown
2024-05-01T14:32:05,507  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b3f2fe1 created in the thread with id: 1
2024-05-01T14:32:05,509  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619 from thread id: 1
2024-05-01T14:32:05,509  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:05,509  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:05,509  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted, dbName:default, owner:alex, createTime:1714599125, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:b, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-05-01T14:32:05,511  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/junit_unparted
2024-05-01T14:32:05,524  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:05,524  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=15}
2024-05-01T14:32:05,524  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143205_8e33191a-e9d2-4842-8a81-713995b5a228); Time taken: 0.02 seconds
2024-05-01T14:32:05,548  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-05-01T14:32:05,549  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-33d1e6ae-96cd-4962-8218-9836914f159b
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,587  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,588  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,588  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,588  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,588  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,588  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:05,589  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:05,590  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:05,591  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:05,596  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:05,617  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,618  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,618  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:05,619  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:05,620  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-05-01T14:32:05,629  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T14:32:05,630  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T14:32:05,630  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T14:32:05,631  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-05-01T14:32:05,631  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T14:32:05,636  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:05,637  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T14:32:05,638  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T14:32:05,638  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-05-01T14:32:05,639  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-05-01T14:32:05,639  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-05-01T14:32:05,639  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714599125638-0
2024-05-01T14:32:05,643  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T14:32:05,644  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,662  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,662  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:05,663  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,664  INFO [JobControl] metastore.HMSHandler: 26: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:05,664  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,664  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5adbcd08, with PersistenceManager: null will be shutdown
2024-05-01T14:32:05,664  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5adbcd08, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1826219c created in the thread with id: 1060
2024-05-01T14:32:05,667  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5adbcd08 from thread id: 1060
2024-05-01T14:32:05,667  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:05,667  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:05,667  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:05,682  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T14:32:05,687  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T14:32:05,688  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T14:32:05,689  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:05,689  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-05-01T14:32:05,689  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T14:32:05,712  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T14:32:05,728  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local499899475_0018
2024-05-01T14:32:05,728  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T14:32:05,792  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T14:32:05,792  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T14:32:05,796  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,796  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,797  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T14:32:05,799  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,799  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,810  INFO [Thread-994] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T14:32:05,810  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local499899475_0018_m_000000_0
2024-05-01T14:32:05,815  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,815  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,816  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,816  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,817  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T14:32:05,817  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 23
Input split[0]:
   Length = 23
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-05-01T14:32:05,818  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T14:32:05,818  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/input.data:0+23
2024-05-01T14:32:05,820  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,820  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,823  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T14:32:05,823  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-05-01T14:32:05,824  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1] C:  R: 
2024-05-01T14:32:05,837  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:05,837  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local499899475_0018_m_000000_0 is done. And is in the process of committing
2024-05-01T14:32:05,838  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,838  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,839  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:05,839  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local499899475_0018_m_000000_0 is allowed to commit now
2024-05-01T14:32:05,841  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:05,841  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:05,842  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local499899475_0018_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/junit_unparted/_SCRATCH0,8296446120270932/b=1
2024-05-01T14:32:05,843  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T14:32:05,843  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local499899475_0018_m_000000_0' done.
2024-05-01T14:32:05,843  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local499899475_0018_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=61554
		FILE: Number of bytes written=10275960
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=990380032
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T14:32:05,843  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local499899475_0018_m_000000_0
2024-05-01T14:32:05,843  INFO [Thread-994] mapred.LocalJobRunner: map task executor complete.
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,890  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,891  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:05,891  INFO [Thread-994] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:05,892  INFO [Thread-994] metastore.HMSHandler: 27: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:05,892  INFO [Thread-994] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:05,893  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4cf4931d, with PersistenceManager: null will be shutdown
2024-05-01T14:32:05,893  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4cf4931d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@441985cd created in the thread with id: 1080
2024-05-01T14:32:05,895  INFO [Thread-994] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4cf4931d from thread id: 1080
2024-05-01T14:32:05,895  INFO [Thread-994] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:05,895  INFO [Thread-994] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:05,895  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:05,939  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:05,940  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_unparted has new partitions [{b=1}].
2024-05-01T14:32:05,941  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_unparted[1]	
2024-05-01T14:32:05,947  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T14:32:05,948  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T14:32:05,954  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_unparted
2024-05-01T14:32:05,954  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 265
2024-05-01T14:32:05,991  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:06,034  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:06,035  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:06,035  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:06,035  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:06,035  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:06,035  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:06,035  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:06,036  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T14:32:06,144  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local499899475_0018
2024-05-01T14:32:06,144  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A
2024-05-01T14:32:06,144  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1] C:  R: 
2024-05-01T14:32:06,145  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,146  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,147  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,162  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T14:32:06,163  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 14:32:05	2024-05-01 14:32:06	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local499899475_0018	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A	MAP_ONLY	default.junit_unparted,

Input(s):
Successfully read 11 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/input.data"

Output(s):
Successfully stored 11 records in: "default.junit_unparted"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local499899475_0018


2024-05-01T14:32:06,163  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,164  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,164  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:06,190  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:06,191  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:06,191  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:06,191  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:06,191  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:06,191  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:06,191  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:06,192  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:06,197  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:06,215  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:06,215  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:06,217  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:06,223  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,225  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-05-01T14:32:06,236  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T14:32:06,236  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T14:32:06,236  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T14:32:06,237  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-05-01T14:32:06,237  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T14:32:06,244  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,245  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T14:32:06,245  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:06,262  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:06,263  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:06,263  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:06,263  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:06,263  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:06,263  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:06,263  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:06,263  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:06,265  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:06,266  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:06,274  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,274  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_by_filter : tbl=hive.default.junit_unparted	
2024-05-01T14:32:06,322  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-05-01T14:32:06,328  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T14:32:06,329  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,337  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T14:32:06,343  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T14:32:06,347  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, test.data.files=${hive.root}/data/files, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-05-01T14:32:06,352  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:06,352  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T14:32:06,379  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T14:32:06,424  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1389654987_0019
2024-05-01T14:32:06,424  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T14:32:06,487  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T14:32:06,487  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T14:32:06,492  INFO [Thread-1036] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:06,492  INFO [Thread-1036] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:06,493  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T14:32:06,503  INFO [Thread-1036] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T14:32:06,503  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1389654987_0019_m_000000_0
2024-05-01T14:32:06,510  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:06,510  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:06,511  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T14:32:06,511  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 265
Input split[0]:
   Length = 265
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-05-01T14:32:06,515  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@441d60c5
2024-05-01T14:32:06,516  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:06,516  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:06,532  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.avro.AvroSerDe with properties {name=default.junit_unparted, columns.types=int, serialization.format=1, columns=a, columns.comments=, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.avro.AvroSerDe, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1714599125}
2024-05-01T14:32:06,532  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T14:32:06,533  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T14:32:06,533  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: B[3,4] C:  R: 
2024-05-01T14:32:06,533  WARN [LocalJobRunner Map Task Executor #0] avro.AvroDeserializer: Received different schemas.  Have to re-encode: {"type":"record","name":"junit_unparted","fields":[{"name":"a","type":["null","int"],"default":null}]}
SIZE{-4895259a:18f36122fa7:-7fed=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@34595843} ID -4895259a:18f36122fa7:-7fed
2024-05-01T14:32:06,535  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:06,535  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1389654987_0019_m_000000_0 is done. And is in the process of committing
2024-05-01T14:32:06,536  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:06,537  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1389654987_0019_m_000000_0 is allowed to commit now
2024-05-01T14:32:06,539  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1389654987_0019_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714599114441/pig/temp/temp1219361764/tmp-311888770
2024-05-01T14:32:06,540  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T14:32:06,540  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1389654987_0019_m_000000_0' done.
2024-05-01T14:32:06,540  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1389654987_0019_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=65145
		FILE: Number of bytes written=10840080
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=2102
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=988807168
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T14:32:06,540  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1389654987_0019_m_000000_0
2024-05-01T14:32:06,540  INFO [Thread-1036] mapred.LocalJobRunner: map task executor complete.
2024-05-01T14:32:06,687  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1389654987_0019
2024-05-01T14:32:06,687  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases B
2024-05-01T14:32:06,687  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: B[3,4] C:  R: 
2024-05-01T14:32:06,690  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,691  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,693  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,694  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T14:32:06,694  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 14:32:06	2024-05-01 14:32:06	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1389654987_0019	1	0	n/a	n/a	n/a	n/a	0	0	0	0	B	MAP_ONLY	file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714599114441/pig/temp/temp1219361764/tmp-311888770,

Input(s):
Successfully read 11 records from: "default.junit_unparted"

Output(s):
Successfully stored 11 records in: "file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714599114441/pig/temp/temp1219361764/tmp-311888770"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1389654987_0019


2024-05-01T14:32:06,695  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,697  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:06,697  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T14:32:06,698  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T14:32:06,706  INFO [main] input.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:06,706  INFO [main] util.MapRedUtil: Total input paths to process : 1
2024-05-01T14:32:06,707  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T14:32:06,712  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testWriteChar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.153">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:06,769  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:06,773  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:06,773  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:06,773  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5b3f2fe1 will be shutdown
2024-05-01T14:32:06,774  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4a3ebba6 created in the thread with id: 1
2024-05-01T14:32:06,777  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = f957c48a-a08b-4d81-a68b-44a06f6736c6
2024-05-01T14:32:06,777  INFO [main] SessionState: Hive Session ID = f957c48a-a08b-4d81-a68b-44a06f6736c6
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:06,778  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:06,784  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/f957c48a-a08b-4d81-a68b-44a06f6736c6
2024-05-01T14:32:06,787  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/f957c48a-a08b-4d81-a68b-44a06f6736c6
2024-05-01T14:32:06,790  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/f957c48a-a08b-4d81-a68b-44a06f6736c6/_tmp_space.db
2024-05-01T14:32:06,790  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): drop table if exists junitTypeTest1
2024-05-01T14:32:06,791  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:06,798  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:06,798  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:06,798  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=7, flushCache_()=0}
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.008 seconds
2024-05-01T14:32:06,798  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): drop table if exists junitTypeTest1
2024-05-01T14:32:06,798  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:06,799  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:06,807  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:06,815  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:06,816  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:06,840  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:06,840  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=32, getTable_(GetTableRequest)=9}
2024-05-01T14:32:06,840  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.042 seconds
2024-05-01T14:32:06,840  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): use default
2024-05-01T14:32:06,841  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:06,842  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:06,842  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.002 seconds
2024-05-01T14:32:06,842  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): use default
2024-05-01T14:32:06,842  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:06,842  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:06,843  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:06,844  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:06,844  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-05-01T14:32:06,844  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.002 seconds
2024-05-01T14:32:06,845  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:06,846  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541
2024-05-01T14:32:06,846  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:06,846  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f957c48a-a08b-4d81-a68b-44a06f6736c6, clientType=HIVECLI]
2024-05-01T14:32:06,846  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:06,846  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:06,846  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@9be2619, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4a3ebba6 will be shutdown
2024-05-01T14:32:06,847  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:06,847  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-05-01T14:32:06,847  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:06,848  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:06,848  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:06,848  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80, with PersistenceManager: null will be shutdown
2024-05-01T14:32:06,848  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@172194b9 created in the thread with id: 1
2024-05-01T14:32:06,850  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80 from thread id: 1
2024-05-01T14:32:06,850  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:06,851  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:06,851  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T14:32:06,851  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:06,852  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:06,852  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@172194b9 will be shutdown
2024-05-01T14:32:06,852  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3cf852f9 created in the thread with id: 1
2024-05-01T14:32:06,854  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:06,854  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:06,855  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:06,856  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:06,856  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:06,856  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.011 seconds
2024-05-01T14:32:06,856  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:06,856  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:06,856  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:06,856  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:06,857  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@b97bc80, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3cf852f9 will be shutdown
2024-05-01T14:32:06,857  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:06,857  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-05-01T14:32:06,868 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:06,869 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:06,872  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T14:32:06,872  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T14:32:06,872  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:06,872 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:06,872  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:06,872  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:06,872  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143206_6c9feb0c-9019-4118-bab6-010c7291f541); Time taken: 0.016 seconds
]]></system-err>
  </testcase>
  <testcase name="testWriteDate" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.897"/>
  <testcase name="testDynamicPartitioningMultiPartColsNoDataInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.826"/>
  <testcase name="testWriteVarchar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.149">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T14:32:13,644  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:13,644  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:13,644  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:13,645  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:13,647  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:13,647  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:13,648  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@270706e3 will be shutdown
2024-05-01T14:32:13,648  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f9c22ac created in the thread with id: 1
2024-05-01T14:32:13,650  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = fe6fc7e7-708d-4d26-9334-91767b660ce8
2024-05-01T14:32:13,650  INFO [main] SessionState: Hive Session ID = fe6fc7e7-708d-4d26-9334-91767b660ce8
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:13,651  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:13,657  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/fe6fc7e7-708d-4d26-9334-91767b660ce8
2024-05-01T14:32:13,661  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/fe6fc7e7-708d-4d26-9334-91767b660ce8
2024-05-01T14:32:13,663  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/fe6fc7e7-708d-4d26-9334-91767b660ce8/_tmp_space.db
2024-05-01T14:32:13,664  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): drop table if exists junitTypeTest1
2024-05-01T14:32:13,664  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:13,670  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:13,670  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:13,670  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:13,671  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:13,671  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=6, flushCache_()=0}
2024-05-01T14:32:13,671  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.006 seconds
2024-05-01T14:32:13,671  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:13,671  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:13,671  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): drop table if exists junitTypeTest1
2024-05-01T14:32:13,671  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:13,671  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:13,712  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:13,712  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:13,717  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:13,717  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:13,731  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:13,731  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=19, getTable_(GetTableRequest)=41}
2024-05-01T14:32:13,731  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.06 seconds
2024-05-01T14:32:13,731  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): use default
2024-05-01T14:32:13,732  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:13,733  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:13,733  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.002 seconds
2024-05-01T14:32:13,733  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): use default
2024-05-01T14:32:13,733  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:13,733  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:13,734  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:13,735  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:13,735  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-05-01T14:32:13,735  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.002 seconds
2024-05-01T14:32:13,735  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:13,736  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418
2024-05-01T14:32:13,736  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:13,736  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=fe6fc7e7-708d-4d26-9334-91767b660ce8, clientType=HIVECLI]
2024-05-01T14:32:13,736  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:13,736  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:13,736  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eeee97f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7f9c22ac will be shutdown
2024-05-01T14:32:13,736  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:13,737  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-05-01T14:32:13,737  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:13,738  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:13,738  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:13,738  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f, with PersistenceManager: null will be shutdown
2024-05-01T14:32:13,738  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23b31c6c created in the thread with id: 1
2024-05-01T14:32:13,740  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f from thread id: 1
2024-05-01T14:32:13,740  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:13,740  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:13,740  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T14:32:13,740  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:13,740  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:13,741  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23b31c6c will be shutdown
2024-05-01T14:32:13,741  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@389732e5 created in the thread with id: 1
2024-05-01T14:32:13,743  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:13,743  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:13,743  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:13,743  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418
2024-05-01T14:32:13,743  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:13,744  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:13,744  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:13,744  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:13,744  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.009 seconds
2024-05-01T14:32:13,744  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:13,744  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:13,744  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:13,744  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:13,744  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:13,744  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:13,744  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@70573f5f, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@389732e5 will be shutdown
2024-05-01T14:32:13,744  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:13,744  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-05-01T14:32:13,745 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:13,746 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:13,746  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T14:32:13,746  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T14:32:13,746  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:13,746 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:13,746  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:13,746  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:13,746  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143213_470a4ce6-a897-47ff-9119-55f558325418); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testBagNStruct" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.14"/>
  <testcase name="testStaticPartitioningMultiPartCols" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.165"/>
  <testcase name="testPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.327"/>
  <testcase name="testStoreWithNoCtorArgs" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.071"/>
  <testcase name="testWriteDecimalX" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.088">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:18,492  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:18,493  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:18,493  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:18,493  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:18,493  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:18,493  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:18,495  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:18,495  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:18,496  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@422c8d72, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@33b0b7d8 will be shutdown
2024-05-01T14:32:18,496  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@422c8d72, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@255d94b0 created in the thread with id: 1
2024-05-01T14:32:18,498  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = e97c10ad-1440-41e0-8d10-fc133c46dbbd
2024-05-01T14:32:18,498  INFO [main] SessionState: Hive Session ID = e97c10ad-1440-41e0-8d10-fc133c46dbbd
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:18,498  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:18,505  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e97c10ad-1440-41e0-8d10-fc133c46dbbd
2024-05-01T14:32:18,508  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/e97c10ad-1440-41e0-8d10-fc133c46dbbd
2024-05-01T14:32:18,511  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/e97c10ad-1440-41e0-8d10-fc133c46dbbd/_tmp_space.db
2024-05-01T14:32:18,512  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): drop table if exists junitTypeTest1
2024-05-01T14:32:18,513  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:18,514  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:18,514  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.002 seconds
2024-05-01T14:32:18,514  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): drop table if exists junitTypeTest1
2024-05-01T14:32:18,514  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:18,515  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:18,516  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:18,516  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1}
2024-05-01T14:32:18,516  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.002 seconds
2024-05-01T14:32:18,517  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): use default
2024-05-01T14:32:18,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:18,518  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:18,518  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.001 seconds
2024-05-01T14:32:18,518  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): use default
2024-05-01T14:32:18,518  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:18,518  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:18,519  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:18,520  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:18,520  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-05-01T14:32:18,520  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.002 seconds
2024-05-01T14:32:18,521  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:18,522  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b
2024-05-01T14:32:18,522  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:18,523  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e97c10ad-1440-41e0-8d10-fc133c46dbbd, clientType=HIVECLI]
2024-05-01T14:32:18,523  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:18,523  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:18,523  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@422c8d72, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@255d94b0 will be shutdown
2024-05-01T14:32:18,523  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:18,523  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -35
2024-05-01T14:32:18,524  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:18,525  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:18,525  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:18,525  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60, with PersistenceManager: null will be shutdown
2024-05-01T14:32:18,526  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4868445f created in the thread with id: 1
2024-05-01T14:32:18,527  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60 from thread id: 1
2024-05-01T14:32:18,527  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:18,528  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:18,528  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T14:32:18,530  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:18,530  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:18,530  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4868445f will be shutdown
2024-05-01T14:32:18,531  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a82cd17 created in the thread with id: 1
2024-05-01T14:32:18,532  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:18,532  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:18,533  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:18,533  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b
2024-05-01T14:32:18,533  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:18,533  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:18,533  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:18,533  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:18,533  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.013 seconds
2024-05-01T14:32:18,534  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:18,534  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:18,534  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:18,534  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:18,534  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:18,534  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:18,534  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@36fb9a60, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a82cd17 will be shutdown
2024-05-01T14:32:18,534  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:18,534  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -36
2024-05-01T14:32:18,535 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:18,536 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:18,537  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T14:32:18,537  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T14:32:18,537  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:18,537 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:18,537  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:18,537  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:18,537  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143218_4b7a37f4-ee93-4146-aaf6-4f272845c28b); Time taken: 0.003 seconds
]]></system-err>
  </testcase>
  <testcase name="testPartitionPublish" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.747"/>
  <testcase name="testWriteDecimal" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.074">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,325  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,326  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,326  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,327  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,327  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,327  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@647fb4af, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7d9bf2d will be shutdown
2024-05-01T14:32:19,328  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@647fb4af, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@58e391f3 created in the thread with id: 1
2024-05-01T14:32:19,329  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 7402f683-13bc-4ade-bba8-e90b9ab66beb
2024-05-01T14:32:19,329  INFO [main] SessionState: Hive Session ID = 7402f683-13bc-4ade-bba8-e90b9ab66beb
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:19,329  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:19,335  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/7402f683-13bc-4ade-bba8-e90b9ab66beb
2024-05-01T14:32:19,338  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/7402f683-13bc-4ade-bba8-e90b9ab66beb
2024-05-01T14:32:19,341  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/7402f683-13bc-4ade-bba8-e90b9ab66beb/_tmp_space.db
2024-05-01T14:32:19,342  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): drop table if exists junitTypeTest1
2024-05-01T14:32:19,342  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,343  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,343  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.002 seconds
2024-05-01T14:32:19,343  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): drop table if exists junitTypeTest1
2024-05-01T14:32:19,343  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,343  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T14:32:19,344  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,344  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T14:32:19,344  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.001 seconds
2024-05-01T14:32:19,345  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): use default
2024-05-01T14:32:19,345  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,346  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,346  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.001 seconds
2024-05-01T14:32:19,346  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): use default
2024-05-01T14:32:19,346  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,346  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,347  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,347  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,347  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=0}
2024-05-01T14:32:19,347  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.001 seconds
2024-05-01T14:32:19,347  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:19,348  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b
2024-05-01T14:32:19,348  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:19,348  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7402f683-13bc-4ade-bba8-e90b9ab66beb, clientType=HIVECLI]
2024-05-01T14:32:19,348  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:19,348  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:19,348  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@647fb4af, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@58e391f3 will be shutdown
2024-05-01T14:32:19,349  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:19,349  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -43
2024-05-01T14:32:19,349  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,350  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,350  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,350  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,350  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23353f43 created in the thread with id: 1
2024-05-01T14:32:19,351  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83 from thread id: 1
2024-05-01T14:32:19,351  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,351  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,351  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T14:32:19,352  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,352  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,352  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@23353f43 will be shutdown
2024-05-01T14:32:19,352  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f870095 created in the thread with id: 1
2024-05-01T14:32:19,354  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,354  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,354  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,354  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,355  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,355  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.008 seconds
2024-05-01T14:32:19,355  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:19,355  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,355  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:19,355  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:19,355  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@19380c83, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f870095 will be shutdown
2024-05-01T14:32:19,355  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:19,355  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-05-01T14:32:19,356 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:19,356 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:19,357  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T14:32:19,357  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T14:32:19,357  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:19,357 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T14:32:19,357  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,357  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:19,357  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_5b3a90e1-1dce-438d-808d-c1456ebccd6b); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testMultiPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.632">
    <failure message="expected:&lt;1&gt; but was:&lt;5&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<5>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,397  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,399  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,400  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,400  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,400  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,400  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@61675dd5 created in the thread with id: 1
2024-05-01T14:32:19,401  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1 from thread id: 1
2024-05-01T14:32:19,401  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = ac1d3e81-aed7-4930-b249-b87aefd9ab22
2024-05-01T14:32:19,402  INFO [main] SessionState: Hive Session ID = ac1d3e81-aed7-4930-b249-b87aefd9ab22
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:19,402  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T14:32:19,408  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/ac1d3e81-aed7-4930-b249-b87aefd9ab22
2024-05-01T14:32:19,410  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/ac1d3e81-aed7-4930-b249-b87aefd9ab22
2024-05-01T14:32:19,413  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/ac1d3e81-aed7-4930-b249-b87aefd9ab22/_tmp_space.db
2024-05-01T14:32:19,414  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): drop table if exists employee
2024-05-01T14:32:19,415  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,415  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,415  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@61675dd5 will be shutdown
2024-05-01T14:32:19,415  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f52d24b created in the thread with id: 1
2024-05-01T14:32:19,417  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,418  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,418  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,419  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,419  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,419  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,419  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T14:32:19,419  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.005 seconds
2024-05-01T14:32:19,419  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,419  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,419  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): drop table if exists employee
2024-05-01T14:32:19,420  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,420  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,421  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,421  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T14:32:19,421  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.002 seconds
2024-05-01T14:32:19,421  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): use default
2024-05-01T14:32:19,421  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,422  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,422  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.001 seconds
2024-05-01T14:32:19,422  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): use default
2024-05-01T14:32:19,422  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,422  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,423  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,423  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,423  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-05-01T14:32:19,423  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.001 seconds
2024-05-01T14:32:19,424  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:19,424  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343
2024-05-01T14:32:19,424  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:19,424  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ac1d3e81-aed7-4930-b249-b87aefd9ab22, clientType=HIVECLI]
2024-05-01T14:32:19,425  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T14:32:19,425  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:19,425  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@658e67e1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f52d24b will be shutdown
2024-05-01T14:32:19,425  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:19,425  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-05-01T14:32:19,425  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,426  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,426  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,426  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,426  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65891190 created in the thread with id: 1
2024-05-01T14:32:19,428  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d from thread id: 1
2024-05-01T14:32:19,428  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,428  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,428  INFO [main] parse.CalcitePlanner: Creating table default.employee position=13
2024-05-01T14:32:19,429  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,429  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,429  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@65891190 will be shutdown
2024-05-01T14:32:19,429  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@45d39b9 created in the thread with id: 1
2024-05-01T14:32:19,431  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,431  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,431  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T14:32:19,431  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343
2024-05-01T14:32:19,431  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:19,431  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T14:32:19,431  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:19,431  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:19,431  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.007 seconds
2024-05-01T14:32:19,431  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:19,432  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:19,432  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T14:32:19,432  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T14:32:19,432  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T14:32:19,432  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:19,432  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@28a7ef5d, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@45d39b9 will be shutdown
2024-05-01T14:32:19,432  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:19,432  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-05-01T14:32:19,433  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,434  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,434  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,434  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7946db67, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,434  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7946db67, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@466e8523 created in the thread with id: 1
2024-05-01T14:32:19,435  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7946db67 from thread id: 1
2024-05-01T14:32:19,435  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,435  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,436  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:employee, dbName:default, owner:alex, createTime:1714599139, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:emp_id, type:int, comment:null), FieldSchema(name:emp_name, type:string, comment:null), FieldSchema(name:emp_start_date, type:string, comment:null), FieldSchema(name:emp_gender, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:emp_country, type:string, comment:null), FieldSchema(name:emp_state, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-05-01T14:32:19,437  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/employee
2024-05-01T14:32:19,452  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:19,452  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=17}
2024-05-01T14:32:19,453  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.02 seconds
2024-05-01T14:32:19,474  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-05-01T14:32:19,475  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-33d1e6ae-96cd-4962-8218-9836914f159b
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,572  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,573  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,573  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,573  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,573  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,573  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,573  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:19,573  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:19,574  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,576  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,579  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,601  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,601  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,602  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,618  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,619  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:19,619  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:19,620  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,621  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,625  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,648  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,648  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,649  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,664  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,664  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,664  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,664  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,664  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,665  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,665  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:19,665  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:19,666  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,667  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,673  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,695  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,695  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,696  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,710  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,711  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,711  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T14:32:19,712  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T14:32:19,712  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,714  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:19,717  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,738  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,739  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,739  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,739  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,739  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,739  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,739  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,739  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,741  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-05-01T14:32:19,751  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T14:32:19,751  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T14:32:19,754  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T14:32:19,755  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 5
2024-05-01T14:32:19,755  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 map-only splittees.
2024-05-01T14:32:19,755  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 out of total 5 MR operators.
2024-05-01T14:32:19,755  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T14:32:19,762  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:19,763  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T14:32:19,763  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T14:32:19,764  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-05-01T14:32:19,765  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-05-01T14:32:19,765  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-05-01T14:32:19,765  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714599139764-0
2024-05-01T14:32:19,773  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T14:32:19,775  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,795  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,796  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,796  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,796  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,796  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,796  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,796  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,796  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:19,797  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,797  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:19,797  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a0a1470, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,797  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a0a1470, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5300100f created in the thread with id: 1969
2024-05-01T14:32:19,798  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a0a1470 from thread id: 1969
2024-05-01T14:32:19,798  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:19,799  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:19,799  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:19,799  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a0a1470, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5300100f will be shutdown
2024-05-01T14:32:19,799  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:19,799  INFO [JobControl] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-05-01T14:32:19,799  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,800  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:19,800  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94f5223, with PersistenceManager: null will be shutdown
2024-05-01T14:32:19,800  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94f5223, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@7a25cb17 created in the thread with id: 1969
2024-05-01T14:32:19,801  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94f5223 from thread id: 1969
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,818  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,818  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,818  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,836  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,836  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,837  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:19,855  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:19,855  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:19,856  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:19,870  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T14:32:19,875  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T14:32:19,876  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T14:32:19,877  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:19,877  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-05-01T14:32:19,877  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T14:32:19,899  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T14:32:19,913  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1185763742_0037
2024-05-01T14:32:19,913  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T14:32:19,973  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T14:32:19,974  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T14:32:19,980  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:19,980  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:19,982  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:19,982  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:19,986  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:19,986  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:19,988  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:19,988  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:19,989  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T14:32:19,990  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:19,990  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,004  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,004  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,018  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,018  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,032  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,032  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,046  INFO [Thread-1843] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T14:32:20,046  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1185763742_0037_m_000000_0
2024-05-01T14:32:20,053  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,053  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,057  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,057  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,060  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,060  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,063  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,063  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,065  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,066  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,067  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,067  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,068  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,068  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,069  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,069  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,069  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T14:32:20,070  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 132
Input split[0]:
   Length = 132
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-05-01T14:32:20,072  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T14:32:20,072  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/input.data:0+132
2024-05-01T14:32:20,073  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T14:32:20,073  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-05-01T14:32:20,075  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-05-01T14:32:20,079  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,079  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,089  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,089  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,094  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,094  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,099  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,099  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,146  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:20,146  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1185763742_0037_m_000000_0 is done. And is in the process of committing
2024-05-01T14:32:20,147  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,147  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,149  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T14:32:20,149  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1185763742_0037_m_000000_0 is allowed to commit now
2024-05-01T14:32:20,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,150  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,151  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1185763742_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/employee/_SCRATCH0,5553919330195844/emp_country=IN/emp_state=TN
2024-05-01T14:32:20,152  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,152  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,153  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1185763742_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/employee/_SCRATCH0,5393683114748624/emp_country=IN/emp_state=KA
2024-05-01T14:32:20,153  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,153  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,154  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1185763742_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/employee/_SCRATCH0,5092259466627207/emp_country=IN/emp_state=KL
2024-05-01T14:32:20,155  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T14:32:20,155  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T14:32:20,156  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1185763742_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/warehouse/employee/_SCRATCH0,9044568955468111/emp_country=IN/emp_state=AP
2024-05-01T14:32:20,156  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T14:32:20,156  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1185763742_0037_m_000000_0' done.
2024-05-01T14:32:20,156  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1185763742_0037_m_000000_0: Counters: 19
	File System Counters
		FILE: Number of bytes read=107859
		FILE: Number of bytes written=21146012
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=0
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=946864128
	MultiStoreCounters
		Output records in _0_employee=1
		Output records in _1_employee=1
		Output records in _2_employee=1
		Output records in _3_employee=1
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T14:32:20,156  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1185763742_0037_m_000000_0
2024-05-01T14:32:20,157  INFO [Thread-1843] mapred.LocalJobRunner: map task executor complete.
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,204  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,205  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,205  INFO [Thread-1843] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T14:32:20,206  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:20,206  INFO [Thread-1843] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T14:32:20,206  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3afb9522, with PersistenceManager: null will be shutdown
2024-05-01T14:32:20,206  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3afb9522, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19938048 created in the thread with id: 1989
2024-05-01T14:32:20,207  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3afb9522 from thread id: 1989
2024-05-01T14:32:20,208  INFO [Thread-1843] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T14:32:20,208  INFO [Thread-1843] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T14:32:20,208  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T14:32:20,208  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3afb9522, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@19938048 will be shutdown
2024-05-01T14:32:20,208  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:20,208  INFO [Thread-1843] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-05-01T14:32:20,208  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,209  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T14:32:20,210  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7883c9a0, with PersistenceManager: null will be shutdown
2024-05-01T14:32:20,210  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7883c9a0, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@319fb93f created in the thread with id: 1989
2024-05-01T14:32:20,212  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7883c9a0 from thread id: 1989
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,263  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,264  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,264  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,264  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=TN}].
2024-05-01T14:32:20,265  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,TN]	
2024-05-01T14:32:20,272  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T14:32:20,272  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T14:32:20,274  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1185763742_0037
2024-05-01T14:32:20,274  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,AP,KA,KL,TN
2024-05-01T14:32:20,274  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-05-01T14:32:20,275  INFO [main] mapReduceLayer.MapReduceLauncher: 50% complete
2024-05-01T14:32:20,275  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1185763742_0037]
2024-05-01T14:32:20,278  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T14:32:20,278  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 458
2024-05-01T14:32:20,307  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,346  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,346  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,346  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,347  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,394  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,394  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,395  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,404  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KA}].
2024-05-01T14:32:20,405  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KA]	
2024-05-01T14:32:20,407  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T14:32:20,407  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T14:32:20,411  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T14:32:20,411  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 458
2024-05-01T14:32:20,435  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,475  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,475  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,476  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,477  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,528  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,528  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,529  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,539  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KL}].
2024-05-01T14:32:20,540  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KL]	
2024-05-01T14:32:20,543  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T14:32:20,543  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T14:32:20,548  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T14:32:20,548  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 456
2024-05-01T14:32:20,573  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T14:32:20,613  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,613  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,613  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,613  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,614  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,614  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,614  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,615  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,664  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,664  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,665  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,674  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=AP}].
2024-05-01T14:32:20,675  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,AP]	
2024-05-01T14:32:20,677  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T14:32:20,678  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T14:32:20,683  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T14:32:20,683  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 456
2024-05-01T14:32:20,707  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T14:32:20,746  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T14:32:20,746  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T14:32:20,747  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T14:32:20,748  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T14:32:20,777  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:20,778  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:20,778  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:20,793  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T14:32:20,793  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:20,794  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T14:32:20,794  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:20,794  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T14:32:20,794  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:20,795  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T14:32:20,795  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T14:32:20,796  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T14:32:20,796  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 14:32:19	2024-05-01 14:32:20	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1185763742_0037	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,AP,KA,KL,TN	MULTI_QUERY,MAP_ONLY	employee,employee,employee,employee,

Input(s):
Successfully read 4 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714599096926/input.data"

Output(s):
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"

Counters:
Total records written : 4
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1185763742_0037


2024-05-01T14:32:20,798  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:20,799  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T14:32:20,799  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T14:32:20,800  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): select * from employee
2024-05-01T14:32:20,800  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343
2024-05-01T14:32:20,800  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T14:32:20,801  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-05-01T14:32:20,801  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-05-01T14:32:20,801  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:20,805  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T14:32:20,805  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-05-01T14:32:20,805  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-05-01T14:32:20,805  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-05-01T14:32:20,807  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.employee	
2024-05-01T14:32:20,814  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.employee, projIndxSet: [0, 1, 2, 3, 4, 5], allowMissingStats: true
2024-05-01T14:32:20,815  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.employee	
2024-05-01T14:32:20,844  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.employee	
2024-05-01T14:32:20,856  WARN [main] calcite.RelOptHiveTable: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-05-01T14:32:20,856  INFO [main] SessionState: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-05-01T14:32:20,905  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:20,905  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-05-01T14:32:20,906  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T14:32:20,906  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-05-01T14:32:20,931  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-05-01T14:32:20,931  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-05-01T14:32:20,931  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-05-01T14:32:20,933  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/ac1d3e81-aed7-4930-b249-b87aefd9ab22/hive_2024-05-01_14-32-20_800_1374435126744634835-1/-mr-10001/.hive-staging_hive_2024-05-01_14-32-20_800_1374435126744634835-1
2024-05-01T14:32:20,944  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-05-01T14:32:20,945  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-05-01T14:32:20,946  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-05-01T14:32:20,946  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343
2024-05-01T14:32:20,946  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T14:32:20,947  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:employee.emp_id, type:int, comment:null), FieldSchema(name:employee.emp_name, type:string, comment:null), FieldSchema(name:employee.emp_start_date, type:string, comment:null), FieldSchema(name:employee.emp_gender, type:string, comment:null), FieldSchema(name:employee.emp_country, type:string, comment:null), FieldSchema(name:employee.emp_state, type:string, comment:null)], properties:null)
2024-05-01T14:32:20,948  INFO [main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-05-01T14:32:20,948  INFO [main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-05-01T14:32:20,948  INFO [main] exec.SelectOperator: SELECT struct<emp_id:int,emp_name:string,emp_start_date:string,emp_gender:string,emp_country:string,emp_state:string>
2024-05-01T14:32:20,948  INFO [main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-05-01T14:32:20,949  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T14:32:20,949  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, listPartitions_(String, String, short)=28, getAllTableConstraints_(AllTableConstraintsRequest)=2, getTable_(GetTableRequest)=4, flushCache_()=0, getAggrColStatsFor_(String, String, List, List, String, String)=12}
2024-05-01T14:32:20,949  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.149 seconds
2024-05-01T14:32:20,949  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T14:32:20,949  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T14:32:20,949  INFO [main] ql.Driver: Executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343): select * from employee
2024-05-01T14:32:20,949  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T14:32:20,949  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T14:32:20,949  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501143219_b0fb42af-d339-4d2d-8c07-c9d983783343); Time taken: 0.0 seconds
2024-05-01T14:32:20,953  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:20,954  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T14:32:20,961  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:20,961  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T14:32:20,961  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T14:32:20,968  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:20,968  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T14:32:20,968  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T14:32:20,979  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T14:32:20,980  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T14:32:20,981  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T14:32:20,984  INFO [main] exec.TableScanOperator: Closing Operator: TS[0]
2024-05-01T14:32:20,984  INFO [main] exec.TableScanOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_TS_0:4, 
2024-05-01T14:32:20,984  INFO [main] exec.SelectOperator: Closing Operator: SEL[1]
2024-05-01T14:32:20,984  INFO [main] exec.SelectOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_SEL_1:4, 
2024-05-01T14:32:20,984  INFO [main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-05-01T14:32:20,984  INFO [main] exec.ListSinkOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_LIST_SINK_3:4, 
2024-05-01T14:32:20,984  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T14:32:20,990  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testStaticPartitioningMultiPartColsNoData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.267"/>
  <testcase name="testStoreMultiTables" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.378"/>
  <testcase name="testStoreWithNoSchema" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.813"/>
  <testcase name="testNoAlias" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.266"/>
  <testcase name="testStoreFuncSimple" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.771"/>
</testsuite>