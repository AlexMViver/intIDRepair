<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report-3.0.xsd" version="3.0" name="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="45.359" tests="29" errors="4" skipped="5" failures="2">
  <properties>
    <property name="sun.desktop" value="gnome"/>
    <property name="build.test.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="test.tmp.dir.uri" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="qfile" value=""/>
    <property name="log4j.configurationFile" value="file:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties"/>
    <property name="sun.cpu.isalist" value=""/>
    <property name="test.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/warehouse"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="run_disabled" value=""/>
    <property name="java.vm.vendor" value="Private Build"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="hadoop.version" value="3.1.0"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="user.timezone" value=""/>
    <property name="user.country.format" value="PT"/>
    <property name="test.local.warehouse.dir" value="pfile:///home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/localfs/warehouse"/>
    <property name="os.name" value="Linux"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="test.tmp.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="test.build.data" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="clustermode" value=""/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="user.country" value="US"/>
    <property name="test.data.files" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="sun.boot.library.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64"/>
    <property name="sun.java.command" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter8896934073253633372.jar /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire 2024-05-01T05-01-21_120-jvmRun1 surefire7740194391023508718tmp surefire_9455115590936526047138tmp"/>
    <property name="surefire.test.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/test-classes:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/classes:/home/alex/Repositories/hive/hcatalog/core/target/classes:/home/alex/Repositories/hive/cli/target/classes:/home/alex/Repositories/hive/service/target/classes:/home/alex/Repositories/hive/llap-server/target/classes:/home/alex/.m2/repository/com/lmax/disruptor/3.3.7/disruptor-3.3.7.jar:/home/alex/Repositories/hive/llap-common/target/test-classes:/home/alex/Repositories/hive/hplsql/target/classes:/home/alex/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/home/alex/.m2/repository/org/abego/treelayout/org.abego.treelayout.core/1.0.1/org.abego.treelayout.core-1.0.1.jar:/home/alex/.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-runner/9.3.27.v20190418/jetty-runner-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.27.v20190418/jetty-plus-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-annotations/9.3.27.v20190418/jetty-annotations-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/ow2/asm/asm-commons/6.0/asm-commons-6.0.jar:/home/alex/.m2/repository/org/ow2/asm/asm-tree/6.0/asm-tree-6.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jaas/9.3.27.v20190418/jetty-jaas-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-server/9.3.27.v20190418/websocket-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-common/9.3.27.v20190418/websocket-common-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-api/9.3.27.v20190418/websocket-api-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-client/9.3.27.v20190418/websocket-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/websocket/websocket-servlet/9.3.27.v20190418/websocket-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.27.v20190418/jetty-jndi-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jsp/9.3.27.v20190418/apache-jsp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/toolchain/jetty-schemas/3.1/jetty-schemas-3.1.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/compiler/ecj/4.4.2/ecj-4.4.2.jar:/home/alex/.m2/repository/org/eclipse/jetty/apache-jstl/9.3.27.v20190418/apache-jstl-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-spec/1.2.5/taglibs-standard-spec-1.2.5.jar:/home/alex/.m2/repository/org/apache/taglibs/taglibs-standard-impl/1.2.5/taglibs-standard-impl-1.2.5.jar:/home/alex/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.64/bcprov-jdk15on-1.64.jar:/home/alex/.m2/repository/org/apache/santuario/xmlsec/2.2.1/xmlsec-2.2.1.jar:/home/alex/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/alex/Repositories/hive/common/target/classes:/home/alex/Repositories/hive/classification/target/classes:/home/alex/.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-rewrite/9.3.27.v20190418/jetty-rewrite-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-client/9.3.27.v20190418/jetty-client-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-web/2.13.2/log4j-web-2.13.2.jar:/home/alex/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/home/alex/Repositories/hive/metastore/target/classes:/home/alex/Repositories/hive/ql/target/classes:/home/alex/.m2/repository/com/google/guava/guava/19.0/guava-19.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.0/jackson-annotations-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.0/jackson-core-2.12.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.0/jackson-databind-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.0/hadoop-annotations-3.1.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-archives/3.1.0/hadoop-archives-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0.jar:/home/alex/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/alex/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/alex/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/alex/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/alex/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar:/home/alex/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/alex/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar:/home/alex/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar:/home/alex/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar:/home/alex/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/alex/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar:/home/alex/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/alex/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/alex/.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/alex/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/alex/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/alex/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/alex/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/alex/.m2/repository/com/google/re2j/re2j/1.2/re2j-1.2.jar:/home/alex/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/alex/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-auth/3.1.0/hadoop-auth-3.1.0.jar:/home/alex/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/alex/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/alex/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/alex/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/alex/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/alex/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/alex/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/alex/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/home/alex/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/home/alex/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/alex/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.1.0/hadoop-mapreduce-client-jobclient-3.1.0.jar:/home/alex/.m2/repository/com/google/inject/extensions/guice-servlet/4.0/guice-servlet-4.0.jar:/home/alex/.m2/repository/com/google/inject/guice/4.0/guice-4.0.jar:/home/alex/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/3.1.0/hadoop-mapreduce-client-core-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-client/3.1.0/hadoop-yarn-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-api/3.1.0/hadoop-yarn-api-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.1.0/hadoop-yarn-common-3.1.0.jar:/home/alex/.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.19/jersey-guice-1.19.jar:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.12.0/jackson-module-jaxb-annotations-2.12.0.jar:/home/alex/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar:/home/alex/.m2/repository/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.12.0/jackson-jaxrs-json-provider-2.12.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.0/hadoop-hdfs-client-3.1.0.jar:/home/alex/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/alex/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar:/home/alex/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/alex/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar:/home/alex/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/alex/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/alex/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/home/alex/.m2/repository/ant/ant/1.6.5/ant-1.6.5.jar:/home/alex/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/alex/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/alex/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/alex/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/home/alex/.m2/repository/junit/junit/4.13/junit-4.13.jar:/home/alex/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/alex/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/alex/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/alex/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/alex/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/alex/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/alex/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/alex/.m2/repository/org/apache/ivy/ivy/2.2.0/ivy-2.2.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0.jar:/home/alex/.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar:/home/alex/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/alex/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/alex/.m2/repository/io/netty/netty-all/4.1.65.Final/netty-all-4.1.65.Final.jar:/home/alex/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/alex/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/home/alex/Repositories/hive/hcatalog/core/target/test-classes:/home/alex/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/alex/Repositories/hive/ql/target/test-classes:/home/alex/.m2/repository/org/apache/atlas/atlas-client-v2/2.1.0/atlas-client-v2-2.1.0.jar:/home/alex/.m2/repository/cglib/cglib/2.2.2/cglib-2.2.2.jar:/home/alex/.m2/repository/asm/asm/3.3.1/asm-3.3.1.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-client-common/2.1.0/atlas-client-common-2.1.0.jar:/home/alex/.m2/repository/org/apache/atlas/atlas-intg/2.1.0/atlas-intg-2.1.0.jar:/home/alex/.m2/repository/commons-validator/commons-validator/1.6/commons-validator-1.6.jar:/home/alex/.m2/repository/commons-digester/commons-digester/1.8.1/commons-digester-1.8.1.jar:/home/alex/.m2/repository/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.12.0/jackson-jaxrs-base-2.12.0.jar:/home/alex/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/alex/.m2/repository/org/springframework/spring-context/4.3.20.RELEASE/spring-context-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-aop/4.3.20.RELEASE/spring-aop-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-beans/4.3.20.RELEASE/spring-beans-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-core/4.3.20.RELEASE/spring-core-4.3.20.RELEASE.jar:/home/alex/.m2/repository/org/springframework/spring-expression/4.3.20.RELEASE/spring-expression-4.3.20.RELEASE.jar:/home/alex/.m2/repository/commons-configuration/commons-configuration/1.10/commons-configuration-1.10.jar:/home/alex/Repositories/hive/vector-code-gen/target/classes:/home/alex/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/home/alex/Repositories/hive/serde/target/classes:/home/alex/.m2/repository/org/apache/arrow/arrow-vector/0.15.1/arrow-vector-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-format/0.15.1/arrow-format-0.15.1.jar:/home/alex/.m2/repository/org/apache/arrow/arrow-memory/0.15.1/arrow-memory-0.15.1.jar:/home/alex/.m2/repository/com/google/flatbuffers/flatbuffers-java/1.9.0/flatbuffers-java-1.9.0.jar:/home/alex/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/alex/.m2/repository/com/github/davidmoten/flatbuffers-java/1.6.0.1/flatbuffers-java-1.6.0.1.jar:/home/alex/Repositories/hive/parser/target/classes:/home/alex/Repositories/hive/udf/target/classes:/home/alex/Repositories/hive/service-rpc/target/classes:/home/alex/Repositories/hive/llap-client/target/classes:/home/alex/Repositories/hive/llap-common/target/classes:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-api/0.10.5/jjwt-api-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-impl/0.10.5/jjwt-impl-0.10.5.jar:/home/alex/.m2/repository/io/jsonwebtoken/jjwt-jackson/0.10.5/jjwt-jackson-0.10.5.jar:/home/alex/Repositories/hive/llap-tez/target/classes:/home/alex/Repositories/hive/shims/aggregator/target/classes:/home/alex/Repositories/hive/shims/common/target/classes:/home/alex/Repositories/hive/shims/0.23/target/classes:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/3.1.0/hadoop-yarn-server-resourcemanager-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/3.1.0/hadoop-yarn-server-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/alex/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/3.1.0/hadoop-yarn-server-applicationhistoryservice-3.1.0.jar:/home/alex/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/home/alex/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/home/alex/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/home/alex/Repositories/hive/shims/scheduler/target/classes:/home/alex/Repositories/hive/spark-client/target/classes:/home/alex/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.12.0/jackson-module-scala_2.12-2.12.0.jar:/home/alex/.m2/repository/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar:/home/alex/.m2/repository/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar:/home/alex/.m2/repository/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.jar:/home/alex/.m2/repository/com/twitter/chill-java/0.9.3/chill-java-0.9.3.jar:/home/alex/.m2/repository/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-client/3.1.0/hadoop-client-3.1.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-launcher_2.12/2.4.5/spark-launcher_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-kvstore_2.12/2.4.5/spark-kvstore_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-common_2.12/2.4.5/spark-network-common_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-network-shuffle_2.12/2.4.5/spark-network-shuffle_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/spark/spark-unsafe_2.12/2.4.5/spark-unsafe_2.12-2.4.5.jar:/home/alex/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/alex/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/alex/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/alex/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/alex/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/alex/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/alex/.m2/repository/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-core_2.12/3.5.3/json4s-core_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-ast_2.12/3.5.3/json4s-ast_2.12-3.5.3.jar:/home/alex/.m2/repository/org/json4s/json4s-scalap_2.12/3.5.3/json4s-scalap_2.12-3.5.3.jar:/home/alex/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/alex/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/alex/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/alex/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/alex/.m2/repository/org/apache/spark/spark-tags_2.12/2.4.5/spark-tags_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/alex/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/alex/.m2/repository/org/apache/spark/spark-yarn_2.12/2.4.5/spark-yarn_2.12-2.4.5.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/3.1.0/hadoop-yarn-server-web-proxy-3.1.0.jar:/home/alex/Repositories/hive/storage-api/target/classes:/home/alex/.m2/repository/com/amazonaws/secretsmanager/aws-secretsmanager-caching-java/1.0.1/aws-secretsmanager-caching-java-1.0.1.jar:/home/alex/.m2/repository/com/esotericsoftware/kryo/5.0.3/kryo-5.0.3.jar:/home/alex/.m2/repository/com/esotericsoftware/minlog/1.3.1/minlog-1.3.1.jar:/home/alex/.m2/repository/com/esotericsoftware/reflectasm/1.11.9/reflectasm-1.11.9.jar:/home/alex/.m2/repository/org/apache/parquet/parquet-hadoop-bundle/1.11.1/parquet-hadoop-bundle-1.11.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-text/1.8/commons-text-1.8.jar:/home/alex/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.13.2/log4j-1.2-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.2/log4j-api-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.2/log4j-core-2.13.2.jar:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar:/home/alex/.m2/repository/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar:/home/alex/.m2/repository/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar:/home/alex/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/home/alex/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/home/alex/.m2/repository/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/3.1.0/hadoop-yarn-registry-3.1.0.jar:/home/alex/.m2/repository/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar:/home/alex/.m2/repository/org/apache/orc/orc-tools/1.6.9/orc-tools-1.6.9.jar:/home/alex/.m2/repository/com/opencsv/opencsv/3.9/opencsv-3.9.jar:/home/alex/.m2/repository/org/threeten/threetenbp/1.3.5/threetenbp-1.3.5.jar:/home/alex/.m2/repository/org/apache/thrift/libthrift/0.14.1/libthrift-0.14.1.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar:/home/alex/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.46/tomcat-embed-core-8.5.46.jar:/home/alex/.m2/repository/org/apache/tomcat/tomcat-annotations-api/8.5.46/tomcat-annotations-api-8.5.46.jar:/home/alex/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar:/home/alex/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar:/home/alex/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-hive/1.1.0-incubating/datasketches-hive-1.1.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-java/1.3.0-incubating/datasketches-java-1.3.0-incubating.jar:/home/alex/.m2/repository/org/apache/datasketches/datasketches-memory/1.2.0-incubating/datasketches-memory-1.2.0-incubating.jar:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar:/home/alex/.m2/repository/org/codehaus/groovy/groovy-all/2.4.11/groovy-all-2.4.11.jar:/home/alex/.m2/repository/org/jodd/jodd-util/6.0.0/jodd-util-6.0.0.jar:/home/alex/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.12.0/jackson-dataformat-yaml-2.12.0.jar:/home/alex/.m2/repository/org/yaml/snakeyaml/1.27/snakeyaml-1.27.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-core/5.2.4/datanucleus-core-5.2.4.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-core/1.25.0/calcite-core-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-linq4j/1.25.0/calcite-linq4j-1.25.0.jar:/home/alex/.m2/repository/org/apiguardian/apiguardian-api/1.1.0/apiguardian-api-1.1.0.jar:/home/alex/.m2/repository/com/esri/geometry/esri-geometry-api/2.2.0/esri-geometry-api-2.2.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/sketches-core/0.9.0/sketches-core-0.9.0.jar:/home/alex/.m2/repository/com/yahoo/datasketches/memory/0.9.0/memory-0.9.0.jar:/home/alex/.m2/repository/org/apache/calcite/calcite-druid/1.25.0/calcite-druid-1.25.0.jar:/home/alex/.m2/repository/org/apache/calcite/avatica/avatica/1.12.0/avatica-1.12.0.jar:/home/alex/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/home/alex/.m2/repository/com/tdunning/json/1.8/json-1.8.jar:/home/alex/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/alex/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/classes:/home/alex/.m2/repository/org/reflections/reflections/0.9.10/reflections-0.9.10.jar:/home/alex/.m2/repository/org/javassist/javassist/3.19.0-GA/javassist-3.19.0-GA.jar:/home/alex/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/alex/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/home/alex/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/home/alex/.m2/repository/com/sun/jersey/contribs/jersey-multipart/1.19/jersey-multipart-1.19.jar:/home/alex/.m2/repository/org/jvnet/mimepull/mimepull/1.9.3/mimepull-1.9.3.jar:/home/alex/.m2/repository/org/apache/kafka/kafka-clients/2.5.0/kafka-clients-2.5.0.jar:/home/alex/.m2/repository/com/github/luben/zstd-jni/1.4.4-7/zstd-jni-1.4.4-7.jar:/home/alex/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.1.0/hadoop-hdfs-3.1.0-tests.jar:/home/alex/Repositories/hive/standalone-metastore/metastore-server/target/test-classes:/home/alex/Repositories/hive/standalone-metastore/metastore-common/target/classes:/home/alex/.m2/repository/com/github/ben-manes/caffeine/caffeine/2.8.4/caffeine-2.8.4.jar:/home/alex/.m2/repository/org/checkerframework/checker-qual/3.4.0/checker-qual-3.4.0.jar:/home/alex/.m2/repository/com/google/errorprone/error_prone_annotations/2.3.4/error_prone_annotations-2.3.4.jar:/home/alex/.m2/repository/org/apache/orc/orc-core/1.6.9/orc-core-1.6.9.jar:/home/alex/.m2/repository/org/apache/orc/orc-shims/1.6.9/orc-shims-1.6.9.jar:/home/alex/.m2/repository/io/airlift/aircompressor/0.19/aircompressor-0.19.jar:/home/alex/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/alex/.m2/repository/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/alex/.m2/repository/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/home/alex/.m2/repository/com/zaxxer/HikariCP/2.6.1/HikariCP-2.6.1.jar:/home/alex/.m2/repository/org/apache/commons/commons-dbcp2/2.7.0/commons-dbcp2-2.7.0.jar:/home/alex/.m2/repository/org/apache/commons/commons-pool2/2.7.0/commons-pool2-2.7.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.0/metrics-core-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.0/metrics-jvm-3.1.0.jar:/home/alex/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.0/metrics-json-3.1.0.jar:/home/alex/.m2/repository/org/apache/derby/derby/10.14.1.0/derby-10.14.1.0.jar:/home/alex/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-api-jdo/5.2.4/datanucleus-api-jdo-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/datanucleus-rdbms/5.2.4/datanucleus-rdbms-5.2.4.jar:/home/alex/.m2/repository/org/datanucleus/javax.jdo/3.2.0-release/javax.jdo-3.2.0-release.jar:/home/alex/.m2/repository/javax/transaction/javax.transaction-api/1.3/javax.transaction-api-1.3.jar:/home/alex/.m2/repository/org/glassfish/corba/glassfish-corba-omgapi/4.2.2/glassfish-corba-omgapi-4.2.2.jar:/home/alex/.m2/repository/sqlline/sqlline/1.9.0/sqlline-1.9.0.jar:/home/alex/.m2/repository/org/jline/jline-terminal/3.12.1/jline-terminal-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-reader/3.12.1/jline-reader-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jansi/3.12.1/jline-terminal-jansi-3.12.1.jar:/home/alex/.m2/repository/org/fusesource/jansi/jansi/1.18/jansi-1.18.jar:/home/alex/.m2/repository/org/jline/jline-terminal-jna/3.12.1/jline-terminal-jna-3.12.1.jar:/home/alex/.m2/repository/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar:/home/alex/.m2/repository/org/jline/jline-builtins/3.12.1/jline-builtins-3.12.1.jar:/home/alex/.m2/repository/org/jline/jline-style/3.12.1/jline-style-3.12.1.jar:/home/alex/.m2/repository/com/cronutils/cron-utils/9.1.3/cron-utils-9.1.3.jar:/home/alex/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/home/alex/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar:/home/alex/.m2/repository/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar:/home/alex/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre8/mssql-jdbc-6.2.1.jre8.jar:/home/alex/.m2/repository/org/postgresql/postgresql/42.2.14/postgresql-42.2.14.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/3.1.0/hadoop-mapreduce-client-common-3.1.0.jar:/home/alex/.m2/repository/org/apache/hadoop/hadoop-common/3.1.0/hadoop-common-3.1.0-tests.jar:/home/alex/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/home/alex/.m2/repository/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar:/home/alex/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar:/home/alex/.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar:/home/alex/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/alex/.m2/repository/org/mockito/mockito-core/3.4.4/mockito-core-3.4.4.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy/1.10.13/byte-buddy-1.10.13.jar:/home/alex/.m2/repository/net/bytebuddy/byte-buddy-agent/1.10.13/byte-buddy-agent-1.10.13.jar:/home/alex/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../conf:"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="user.home" value="/home/alex"/>
    <property name="user.language" value="en"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="java.home" value="/usr/lib/jvm/java-8-openjdk-amd64/jre"/>
    <property name="java.security.krb5.conf" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/krb5.conf"/>
    <property name="file.separator" value="/"/>
    <property name="basedir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="test.dfs.mkdir" value=""/>
    <property name="derby.stream.error.file" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/derby.log"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="surefire.real.class.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/surefire/surefirebooter8896934073253633372.jar"/>
    <property name="sun.boot.class.path" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes"/>
    <property name="hadoop.log.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="log4j.debug" value="true"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="spark.home" value=""/>
    <property name="java.runtime.version" value="1.8.0_402-8u402-ga-2ubuntu1~22.04-b06"/>
    <property name="user.name" value="alex"/>
    <property name="java.net.preferIPv4Stack" value="true"/>
    <property name="test.src.tables" value=""/>
    <property name="path.separator" value=":"/>
    <property name="os.version" value="6.5.0-28-generic"/>
    <property name="java.endorsed.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed"/>
    <property name="java.runtime.name" value="OpenJDK Runtime Environment"/>
    <property name="hive.test.console.log.level" value="INFO"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/>
    <property name="qfile_regex" value=""/>
    <property name="hadoop.bin.path" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../testutils/hadoop"/>
    <property name="localRepository" value="/home/alex/.m2/repository"/>
    <property name="build.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target"/>
    <property name="mapred.job.tracker" value="local"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="java.io.tmpdir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp"/>
    <property name="maven.local.repository" value="/home/alex/.m2/repository"/>
    <property name="java.version" value="1.8.0_402"/>
    <property name="user.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter"/>
    <property name="os.arch" value="amd64"/>
    <property name="hive.version" value="4.0.0-SNAPSHOT"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="user.language.format" value="pt"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="antlr.version" value="3.5.2"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="derby.version" value="10.14.1.0"/>
    <property name="test.data.dir" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../data/files"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib"/>
    <property name="java.vendor" value="Private Build"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.vm.version" value="25.402-b06"/>
    <property name="java.specification.maintenance.version" value="5"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="java.ext.dirs" value="/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="initScript" value=""/>
    <property name="java.class.version" value="52.0"/>
    <property name="test.output.overwrite" value=""/>
    <property name="hive.root" value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/../../"/>
  </properties>
  <testcase name="testWriteTinyint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
    <system-err><![CDATA[SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.2/log4j-slf4j-impl-2.13.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/alex/.m2/repository/org/slf4j/slf4j-simple/1.7.27/slf4j-simple-1.7.27.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
DEBUG StatusLogger Using ShutdownCallbackRegistry class org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry
DEBUG StatusLogger Took 0,110317 seconds to load 251 plugins from sun.misc.Launcher$AppClassLoader@7f31245a
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger Starting LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916]...
DEBUG StatusLogger Reconfiguration started for context[name=7f31245a] at URI null (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger PluginManager 'ConfigurationFactory' found 6 plugins
DEBUG StatusLogger Using configurationFactory org.apache.logging.log4j.core.config.ConfigurationFactory$Factory@233795b6
DEBUG StatusLogger Apache Log4j Core 2.13.2 initializing configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Installed 2 script engines
DEBUG StatusLogger Groovy Scripting Engine version: 2.0, language: Groovy, threading: MULTITHREADED, compile: true, names: [groovy, Groovy], factory class: org.codehaus.groovy.jsr223.GroovyScriptEngineFactory
DEBUG StatusLogger Oracle Nashorn version: 1.8.0_402, language: ECMAScript, threading: Not Thread Safe, compile: true, names: [nashorn, Nashorn, js, JS, JavaScript, javascript, ECMAScript, ecmascript], factory class: jdk.nashorn.api.scripting.NashornScriptEngineFactory
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/common/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PerfLogger matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.PidFilePatternConverter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest$TestFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender$NameFilter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$EventCounts matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.HiveEventCounter$1 matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.LogDivertAppender matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogSerDe matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogStorageHandler matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat$Location matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogParser matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.syslog.SyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.NullAppender matches criteria annotated with @Plugin
INFO StatusLogger Scanning for classes in '/home/alex/Repositories/hive/ql/target/test-classes/org/apache/hadoop/hive/ql/log' matching criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestSyslogInputFormat matches criteria annotated with @Plugin
DEBUG StatusLogger Checking to see if class org.apache.hadoop.hive.ql.log.TestLog4j2Appenders matches criteria annotated with @Plugin
DEBUG StatusLogger Took 0,019044 seconds to load 7 plugins from package org.apache.hadoop.hive.ql.log
DEBUG StatusLogger PluginManager 'Core' found 133 plugins
DEBUG StatusLogger PluginManager 'Level' found 0 plugins
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
TRACE StatusLogger TypeConverterRegistry initializing.
DEBUG StatusLogger PluginManager 'TypeConverter' found 26 plugins
DEBUG StatusLogger createProperty(name="hive.log.file", value="hive.log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.dir", value="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.root.logger", value="DRFA")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.log.level", value="DEBUG")
DEBUG StatusLogger Building Plugin[name=property, class=org.apache.logging.log4j.core.config.Property].
DEBUG StatusLogger createProperty(name="hive.test.console.log.level", value="INFO")
DEBUG StatusLogger Building Plugin[name=properties, class=org.apache.logging.log4j.core.config.PropertiesPlugin].
DEBUG StatusLogger configureSubstitutor(={hive.log.file=hive.log, hive.log.dir=/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log, hive.root.logger=DRFA, hive.log.level=DEBUG, hive.test.console.log.level=INFO}, Configuration(HiveLog4j2Test))
DEBUG StatusLogger PluginManager 'Lookup' found 17 plugins
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.ipc", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.security", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hdfs", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.hdfs.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.metrics2", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.mortbay", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.yarn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.hadoop.yarn.server", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.tez", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="org.apache.hadoop.conf.Configuration", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.zookeeper", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.ServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.server.NIOServerCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxn", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocket", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.zookeeper.ClientCnxnSocketNIO", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="DataNucleus", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="Datastore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="ERROR", name="JPOX", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.ql.exec.Operator", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.serde2.lazy", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.hadoop.hive.metastore.ObjectStore", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.calcite.plan.RelOptPlanner", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="com.amazonaws", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="INFO", name="org.apache.http", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.apache.thrift", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="org.eclipse.jetty", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="WARN", name="BlockStateChange", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=logger, class=org.apache.logging.log4j.core.config.LoggerConfig].
DEBUG StatusLogger createLogger(additivity="true", level="DEBUG", name="org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer", includeLocation="null", ={}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="console", level="INFO", Filter=null)
DEBUG StatusLogger Building Plugin[name=AppenderRef, class=org.apache.logging.log4j.core.config.AppenderRef].
DEBUG StatusLogger createAppenderRef(ref="DRFA", level="null", Filter=null)
DEBUG StatusLogger Building Plugin[name=root, class=org.apache.logging.log4j.core.config.LoggerConfig$RootLogger].
DEBUG StatusLogger createLogger(additivity="null", level="DEBUG", includeLocation="null", ={console, DRFA}, ={}, Configuration(HiveLog4j2Test), Filter=null)
DEBUG StatusLogger Building Plugin[name=loggers, class=org.apache.logging.log4j.core.config.LoggersPlugin].
DEBUG StatusLogger createLoggers(={org.apache.hadoop.ipc, org.apache.hadoop.security, org.apache.hadoop.hdfs, org.apache.hadoop.hdfs.server, org.apache.hadoop.metrics2, org.mortbay, org.apache.hadoop.yarn, org.apache.hadoop.yarn.server, org.apache.tez, org.apache.hadoop.conf.Configuration, org.apache.zookeeper, org.apache.zookeeper.server.ServerCnxn, org.apache.zookeeper.server.NIOServerCnxn, org.apache.zookeeper.ClientCnxn, org.apache.zookeeper.ClientCnxnSocket, org.apache.zookeeper.ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, org.apache.hadoop.hive.ql.exec.Operator, org.apache.hadoop.hive.serde2.lazy, org.apache.hadoop.hive.metastore.ObjectStore, org.apache.calcite.plan.RelOptPlanner, com.amazonaws, org.apache.http, org.apache.thrift, org.eclipse.jetty, BlockStateChange, org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer, root})
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger PluginManager 'Converter' found 46 plugins
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.ConsoleAppender].
DEBUG StatusLogger ConsoleAppender$Builder(target="SYSTEM_ERR", follow="null", direct="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="console", Configuration(HiveLog4j2Test), Filter=null, ={})
DEBUG StatusLogger Starting OutputStreamManager SYSTEM_ERR.false.false
DEBUG StatusLogger Building Plugin[name=layout, class=org.apache.logging.log4j.core.layout.PatternLayout].
DEBUG StatusLogger PatternLayout$Builder(pattern="%d{ISO8601} %5p [%t] %c{2}: %m%n", PatternSelector=null, Configuration(HiveLog4j2Test), Replace=null, charset="null", alwaysWriteExceptions="null", disableAnsi="null", noConsoleNoAnsi="null", header="null", footer="null")
DEBUG StatusLogger Building Plugin[name=TimeBasedTriggeringPolicy, class=org.apache.logging.log4j.core.appender.rolling.TimeBasedTriggeringPolicy].
DEBUG StatusLogger TimeBasedTriggeringPolicy$Builder(interval="1", modulate="true", maxRandomDelay="null")
DEBUG StatusLogger Building Plugin[name=Policies, class=org.apache.logging.log4j.core.appender.rolling.CompositeTriggeringPolicy].
DEBUG StatusLogger createPolicy(={TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)})
DEBUG StatusLogger Building Plugin[name=DefaultRolloverStrategy, class=org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy].
DEBUG StatusLogger DefaultRolloverStrategy$Builder(max="30", min="null", fileIndex="null", compressionLevel="null", ={}, stopCustomActionsOnError="null", tempCompressedFilePattern="null", Configuration(HiveLog4j2Test))
DEBUG StatusLogger Building Plugin[name=appender, class=org.apache.logging.log4j.core.appender.RollingRandomAccessFileAppender].
DEBUG StatusLogger RollingRandomAccessFileAppender$Builder(fileName="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log", filePattern="/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log.%d{yyyy-MM-dd}", append="null", Policies(CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])), DefaultRolloverStrategy(DefaultRolloverStrategy(min=1, max=30, useMax=true)), advertise="null", advertiseURI="null", filePermissions="null", fileOwner="null", fileGroup="null", bufferedIo="null", bufferSize="null", immediateFlush="null", ignoreExceptions="null", PatternLayout(%d{ISO8601} %5p [%t] %c{2}: %m%n), name="DRFA", Configuration(HiveLog4j2Test), Filter=null, ={})
TRACE StatusLogger RandomAccessFile /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log seek to 63245461
DEBUG StatusLogger Starting RollingRandomAccessFileManager /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/log/hive.log
DEBUG StatusLogger PluginManager 'FileConverter' found 3 plugins
DEBUG StatusLogger Setting prev file time to 2024-05-01T01:00:40.422-0700
DEBUG StatusLogger Initializing triggering policy CompositeTriggeringPolicy(policies=[TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)])
DEBUG StatusLogger Initializing triggering policy TimeBasedTriggeringPolicy(nextRolloverMillis=0, interval=1, modulate=true)
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/02-00:00:00.000, nextFileTime=2024/05/01-00:00:00.000, prevFileTime=1969/12/31-16:00:00.000, current=2024/05/01-01:00:42.094, freq=DAILY
TRACE StatusLogger PatternProcessor.getNextTime returning 2024/05/02-00:00:00.000, nextFileTime=2024/05/01-00:00:00.000, prevFileTime=2024/05/01-00:00:00.000, current=2024/05/01-01:00:42.094, freq=DAILY
DEBUG StatusLogger Building Plugin[name=appenders, class=org.apache.logging.log4j.core.config.AppendersPlugin].
DEBUG StatusLogger createAppenders(={console, DRFA})
DEBUG StatusLogger Configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 initialized
DEBUG StatusLogger Starting configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14
DEBUG StatusLogger Started configuration org.apache.logging.log4j.core.config.properties.PropertiesConfiguration@70325e14 OK.
TRACE StatusLogger Stopping org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29...
TRACE StatusLogger DefaultConfiguration notified 1 ReliabilityStrategies that config will be stopped.
TRACE StatusLogger DefaultConfiguration stopping root LoggerConfig.
TRACE StatusLogger DefaultConfiguration notifying ReliabilityStrategies that appenders will be stopped.
TRACE StatusLogger DefaultConfiguration stopping remaining Appenders.
DEBUG StatusLogger Shutting down OutputStreamManager SYSTEM_OUT.false.false-1
DEBUG StatusLogger OutputStream closed
DEBUG StatusLogger Shut down OutputStreamManager SYSTEM_OUT.false.false-1, all resources released: true
DEBUG StatusLogger Appender DefaultConsole-1 stopped with status true
TRACE StatusLogger DefaultConfiguration stopped 1 remaining Appenders.
TRACE StatusLogger DefaultConfiguration cleaning Appenders from 1 LoggerConfigs.
DEBUG StatusLogger Stopped org.apache.logging.log4j.core.config.DefaultConfiguration@145eaa29 OK
TRACE StatusLogger Reregistering MBeans after reconfigure. Selector=org.apache.logging.log4j.core.selector.ClassLoaderContextSelector@609db546
TRACE StatusLogger Reregistering context (1/1): '7f31245a' org.apache.logging.log4j.core.LoggerContext@2b71e916
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=StatusLogger'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=ContextSelector'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncAppenders,name=*'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=AsyncLoggerRingBuffer'
TRACE StatusLogger Unregistering but no MBeans found matching 'org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=*,subtype=RingBuffer'
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=StatusLogger
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=ContextSelector
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.tez
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=com.amazonaws
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.thrift
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.eclipse.jetty
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=BlockStateChange
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.metrics2
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.security
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=JPOX
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.conf.Configuration
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.yarn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocketNIO
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.ServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hdfs.server
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.serde2.lazy
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.ClientCnxnSocket
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.mortbay
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.http
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.ql.exec.Operator
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=DataNucleus
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=Datastore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.hive.metastore.ObjectStore
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.hadoop.ipc
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.zookeeper.server.NIOServerCnxn
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Loggers,name=org.apache.calcite.plan.RelOptPlanner
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=console
DEBUG StatusLogger Registering MBean org.apache.logging.log4j2:type=7f31245a,component=Appenders,name=DRFA
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
TRACE StatusLogger Using DummyNanoClock for nanosecond timestamps.
DEBUG StatusLogger Reconfiguration complete for context[name=7f31245a] at URI /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/testconf/hive-log4j2.properties (org.apache.logging.log4j.core.LoggerContext@2b71e916) with optional ClassLoader: null
DEBUG StatusLogger Shutdown hook enabled. Registering a new one.
DEBUG StatusLogger LoggerContext[name=7f31245a, org.apache.logging.log4j.core.LoggerContext@2b71e916] started OK.
2024-05-01T01:00:42,286  INFO [main] mapreduce.HCatBaseTest: Using warehouse directory /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse
DEBUG StatusLogger AsyncLogger.ThreadNameStrategy=UNCACHED (user specified null, default is UNCACHED)
TRACE StatusLogger Using default SystemClock for timestamps.
DEBUG StatusLogger org.apache.logging.log4j.core.util.SystemClock does not support precise timestamps.
]]></system-err>
  </testcase>
  <testcase name="testWriteSmallint" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDateCharTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteTimestamp" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testWriteDecimalXY" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0">
    <skipped message="Temporarily disable until fixed"/>
  </testcase>
  <testcase name="testDynamicPartitioningMultiPartColsInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="14.358"/>
  <testcase name="testEmptyStore" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.182"/>
  <testcase name="testStoreFuncAllSimpleTypes" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.485"/>
  <testcase name="testDynamicPartitioningMultiPartColsInDataPartialSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.592"/>
  <testcase name="testWriteDate2" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="4.758"/>
  <testcase name="testWriteDate3" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="2.15"/>
  <testcase name="testStoreInPartiitonedTbl" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.183">
    <failure message="expected:&lt;1&gt; but was:&lt;2&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testStoreInPartiitonedTbl(AbstractHCatStorerTest.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:07,856  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:07,857  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:07,857  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:07,858  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1e33dda will be shutdown
2024-05-01T01:01:07,858  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@63af8885 created in the thread with id: 1
2024-05-01T01:01:07,860  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 9df70ec9-53b7-471f-a8e6-eb17171022bf
2024-05-01T01:01:07,860  INFO [main] SessionState: Hive Session ID = 9df70ec9-53b7-471f-a8e6-eb17171022bf
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:07,860  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:07,866  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/9df70ec9-53b7-471f-a8e6-eb17171022bf
2024-05-01T01:01:07,869  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/9df70ec9-53b7-471f-a8e6-eb17171022bf
2024-05-01T01:01:07,871  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/9df70ec9-53b7-471f-a8e6-eb17171022bf/_tmp_space.db
2024-05-01T01:01:07,872  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): drop table if exists junit_unparted
2024-05-01T01:01:07,873  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:07,874  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:07,874  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.002 seconds
2024-05-01T01:01:07,874  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): drop table if exists junit_unparted
2024-05-01T01:01:07,874  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:07,875  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:07,876  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:07,876  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T01:01:07,876  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.002 seconds
2024-05-01T01:01:07,876  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): use default
2024-05-01T01:01:07,877  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:07,877  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:07,877  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:07,877  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:07,877  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=0, flushCache_()=0}
2024-05-01T01:01:07,877  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.001 seconds
2024-05-01T01:01:07,877  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:07,877  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:07,877  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): use default
2024-05-01T01:01:07,878  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:07,878  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:07,879  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:07,879  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:07,879  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-05-01T01:01:07,879  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.002 seconds
2024-05-01T01:01:07,880  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:07,880  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833
2024-05-01T01:01:07,880  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:07,881  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9df70ec9-53b7-471f-a8e6-eb17171022bf, clientType=HIVECLI]
2024-05-01T01:01:07,881  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:07,881  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:07,881  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7803a877, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@63af8885 will be shutdown
2024-05-01T01:01:07,881  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:07,881  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -14
2024-05-01T01:01:07,882  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:07,882  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:07,882  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:07,882  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb, with PersistenceManager: null will be shutdown
2024-05-01T01:01:07,883  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12326ccb created in the thread with id: 1
2024-05-01T01:01:07,884  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb from thread id: 1
2024-05-01T01:01:07,884  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:07,884  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:07,884  INFO [main] parse.CalcitePlanner: Creating table default.junit_unparted position=13
2024-05-01T01:01:07,885  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:07,885  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:07,885  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@12326ccb will be shutdown
2024-05-01T01:01:07,885  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@f75843e created in the thread with id: 1
2024-05-01T01:01:07,888  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:07,888  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:07,888  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:07,889  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:07,889  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:07,889  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.009 seconds
2024-05-01T01:01:07,889  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833): create table junit_unparted(a int) partitioned by (b string) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:07,889  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:07,890  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:07,890  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:07,890  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@328b60fb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@f75843e will be shutdown
2024-05-01T01:01:07,890  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:07,890  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -15
2024-05-01T01:01:07,891  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:07,892  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:07,892  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:07,892  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031, with PersistenceManager: null will be shutdown
2024-05-01T01:01:07,892  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@799c1c8 created in the thread with id: 1
2024-05-01T01:01:07,894  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031 from thread id: 1
2024-05-01T01:01:07,894  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:07,894  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:07,894  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:junit_unparted, dbName:default, owner:alex, createTime:1714550467, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:b, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-05-01T01:01:07,896  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/junit_unparted
2024-05-01T01:01:07,908  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:07,908  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=14}
2024-05-01T01:01:07,908  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010107_f3e3f96c-8b66-413e-869d-1f4b562ea833); Time taken: 0.019 seconds
2024-05-01T01:01:07,930  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-05-01T01:01:07,931  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-019b99ca-06f0-4ae6-a7b9-d21f90c9d774
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:07,966  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:07,966  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:07,967  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:07,968  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:07,969  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:07,974  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,003  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,004  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:08,004  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,006  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-05-01T01:01:08,017  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T01:01:08,017  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T01:01:08,018  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T01:01:08,018  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-05-01T01:01:08,018  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T01:01:08,024  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,025  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T01:01:08,025  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T01:01:08,026  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-05-01T01:01:08,026  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-05-01T01:01:08,026  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-05-01T01:01:08,026  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714550468026-0
2024-05-01T01:01:08,030  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T01:01:08,032  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,091  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,091  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,092  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,092  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:08,093  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:08,093  INFO [JobControl] metastore.HMSHandler: 26: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:08,093  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:08,094  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@22104315, with PersistenceManager: null will be shutdown
2024-05-01T01:01:08,094  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@22104315, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35ec0d74 created in the thread with id: 1060
2024-05-01T01:01:08,096  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@22104315 from thread id: 1060
2024-05-01T01:01:08,096  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:08,096  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:08,096  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,110  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T01:01:08,115  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T01:01:08,116  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T01:01:08,116  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:08,116  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-05-01T01:01:08,116  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T01:01:08,136  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T01:01:08,150  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1696567249_0018
2024-05-01T01:01:08,150  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T01:01:08,206  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T01:01:08,206  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T01:01:08,211  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,211  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,211  INFO [Thread-994] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T01:01:08,213  INFO [Thread-994] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,213  INFO [Thread-994] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,223  INFO [Thread-994] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T01:01:08,223  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1696567249_0018_m_000000_0
2024-05-01T01:01:08,228  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,228  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,230  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,230  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,230  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T01:01:08,231  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 23
Input split[0]:
   Length = 23
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-05-01T01:01:08,232  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T01:01:08,232  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/input.data:0+23
2024-05-01T01:01:08,234  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,234  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,237  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T01:01:08,237  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-05-01T01:01:08,238  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1] C:  R: 
2024-05-01T01:01:08,249  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:08,249  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1696567249_0018_m_000000_0 is done. And is in the process of committing
2024-05-01T01:01:08,250  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,250  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,251  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:08,251  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1696567249_0018_m_000000_0 is allowed to commit now
2024-05-01T01:01:08,251  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,252  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,252  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1696567249_0018_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/junit_unparted/_SCRATCH0,6493587403361678/b=1
2024-05-01T01:01:08,253  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T01:01:08,253  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1696567249_0018_m_000000_0' done.
2024-05-01T01:01:08,253  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1696567249_0018_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=61554
		FILE: Number of bytes written=10281298
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1027604480
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T01:01:08,253  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1696567249_0018_m_000000_0
2024-05-01T01:01:08,253  INFO [Thread-994] mapred.LocalJobRunner: map task executor complete.
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,297  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,297  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:08,298  INFO [Thread-994] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:08,298  INFO [Thread-994] metastore.HMSHandler: 27: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:08,298  INFO [Thread-994] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:08,299  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@aca9089, with PersistenceManager: null will be shutdown
2024-05-01T01:01:08,299  INFO [Thread-994] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@aca9089, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6958a8a4 created in the thread with id: 1080
2024-05-01T01:01:08,300  INFO [Thread-994] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@aca9089 from thread id: 1080
2024-05-01T01:01:08,300  INFO [Thread-994] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:08,301  INFO [Thread-994] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:08,301  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,340  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,341  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table junit_unparted has new partitions [{b=1}].
2024-05-01T01:01:08,342  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.junit_unparted[1]	
2024-05-01T01:01:08,349  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T01:01:08,349  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T01:01:08,355  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updating partition stats fast for: junit_unparted
2024-05-01T01:01:08,355  INFO [HMSHandler #8] utils.MetaStoreServerUtils: Updated size to 265
2024-05-01T01:01:08,391  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,432  WARN [Thread-994] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,432  INFO [Thread-994] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:08,433  INFO [Thread-994] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,434  INFO [Thread-994] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T01:01:08,531  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1696567249_0018
2024-05-01T01:01:08,531  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A
2024-05-01T01:01:08,531  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1] C:  R: 
2024-05-01T01:01:08,533  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,534  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,534  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,547  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T01:01:08,547  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 01:01:08	2024-05-01 01:01:08	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1696567249_0018	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A	MAP_ONLY	default.junit_unparted,

Input(s):
Successfully read 11 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/input.data"

Output(s):
Successfully stored 11 records in: "default.junit_unparted"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1696567249_0018


2024-05-01T01:01:08,548  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,548  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,549  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T01:01:08,573  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,573  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,573  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,573  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,574  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,574  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,575  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:08,579  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,595  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,596  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,596  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,597  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:08,602  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:08,603  INFO [main] pigstats.ScriptState: Pig features used in the script: UNKNOWN
2024-05-01T01:01:08,611  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T01:01:08,611  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T01:01:08,612  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T01:01:08,612  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 1
2024-05-01T01:01:08,612  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T01:01:08,618  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,619  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T01:01:08,619  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:08,632  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:08,632  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:08,633  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:08,634  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:08,635  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:08,639  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:08,640  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions_by_filter : tbl=hive.default.junit_unparted	
2024-05-01T01:01:08,671  INFO [main] mapReduceLayer.JobControlCompiler: Setting up single store job
2024-05-01T01:01:08,675  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T01:01:08,676  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,683  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T01:01:08,688  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T01:01:08,690  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Using differences={hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.llap.io.cache.orc.size=8388608, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, hive.mapjoin.max.gc.time.percentage=0.99, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.stats.key.prefix.reserve.length=0, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.test.dummystats.aggregator=value2, hive.users.in.admin.role=hive_admin_user, iceberg.hive.keep.stats=true, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.query.reexecution.stats.persist.scope=query, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, test.log.dir=${test.tmp.dir}/log/, hive.cbo.fallback.strategy=TEST, hive.exec.mode.local.auto=false, hive.support.concurrency=true, javax.jdo.option.ConnectionUserName=APP, hive.ignore.mapjoin.hint=false, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, datanucleus.connectionPool.maxPoolSize=4, hive.strict.timestamp.conversion=false, datanucleus.schema.autoCreateAll=true, test.data.files=${hive.root}/data/files, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.stats.fetch.bitvector=true, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, hive.conf.restricted.list=dummy.config.value, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.property1=value1, hive.llap.io.cache.orc.alloc.max=2097152, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.fetch.task.conversion=minimal, test.data.scripts=${hive.root}/data/scripts, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.schema.verification=false, hive.query.results.cache.enabled=false, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.querylog.location=${test.tmp.dir}/tmp, hive.llap.io.cache.orc.alloc.min=32768, hive.llap.io.allocator.direct=false, hive.stats.column.autogather=true, hive.llap.cache.allow.synthetic.fileid=true, javax.jdo.option.ConnectionPassword=mine, hive.metastore.client.cache.recordStats=true, hive.llap.io.cache.orc.arena.size=8388608, hive.scheduled.queries.executor.enabled=false, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.in.test=true, hive.metastore.client.cache.enabled=true, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml}
2024-05-01T01:01:08,694  INFO [JobControl] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:08,694  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T01:01:08,714  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T01:01:08,728  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1610180408_0019
2024-05-01T01:01:08,728  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T01:01:08,776  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T01:01:08,776  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T01:01:08,780  INFO [Thread-1036] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,780  INFO [Thread-1036] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,780  INFO [Thread-1036] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T01:01:08,789  INFO [Thread-1036] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T01:01:08,789  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1610180408_0019_m_000000_0
2024-05-01T01:01:08,793  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,793  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,793  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T01:01:08,794  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 265
Input split[0]:
   Length = 265
   ClassName: org.apache.hive.hcatalog.mapreduce.HCatSplit
   Locations:

-----------------------

2024-05-01T01:01:08,796  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed org.apache.hive.hcatalog.mapreduce.HCatSplit@3653b38
2024-05-01T01:01:08,797  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:08,797  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:08,810  INFO [LocalJobRunner Map Task Executor #0] mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.avro.AvroSerDe with properties {name=default.junit_unparted, columns.types=int, serialization.format=1, columns=a, columns.comments=, bucketing_version=2, serialization.lib=org.apache.hadoop.hive.serde2.avro.AvroSerDe, column.name.delimiter=,, serialization.null.format=\N, transient_lastDdlTime=1714550467}
2024-05-01T01:01:08,810  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T01:01:08,810  WARN [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T01:01:08,810  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: B[3,4] C:  R: 
2024-05-01T01:01:08,811  WARN [LocalJobRunner Map Task Executor #0] avro.AvroDeserializer: Received different schemas.  Have to re-encode: {"type":"record","name":"junit_unparted","fields":[{"name":"a","type":["null","int"],"default":null}]}
SIZE{52c015aa:18f332bbfe4:-7fed=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@39360c9} ID 52c015aa:18f332bbfe4:-7fed
2024-05-01T01:01:08,812  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:08,812  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1610180408_0019_m_000000_0 is done. And is in the process of committing
2024-05-01T01:01:08,813  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:08,813  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1610180408_0019_m_000000_0 is allowed to commit now
2024-05-01T01:01:08,814  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1610180408_0019_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714550457992/pig/temp/temp2015593063/tmp-464773418
2024-05-01T01:01:08,815  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T01:01:08,815  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1610180408_0019_m_000000_0' done.
2024-05-01T01:01:08,815  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1610180408_0019_m_000000_0: Counters: 15
	File System Counters
		FILE: Number of bytes read=65145
		FILE: Number of bytes written=10845402
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Input split bytes=2102
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1027604480
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T01:01:08,815  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1610180408_0019_m_000000_0
2024-05-01T01:01:08,815  INFO [Thread-1036] mapred.LocalJobRunner: map task executor complete.
2024-05-01T01:01:08,977  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1610180408_0019
2024-05-01T01:01:08,977  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases B
2024-05-01T01:01:08,977  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: B[3,4] C:  R: 
2024-05-01T01:01:08,978  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,979  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,979  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,981  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T01:01:08,981  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 01:01:08	2024-05-01 01:01:08	UNKNOWN

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1610180408_0019	1	0	n/a	n/a	n/a	n/a	0	0	0	0	B	MAP_ONLY	file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714550457992/pig/temp/temp2015593063/tmp-464773418,

Input(s):
Successfully read 11 records from: "default.junit_unparted"

Output(s):
Successfully stored 11 records in: "file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/HCatBaseTest_1714550457992/pig/temp/temp2015593063/tmp-464773418"

Counters:
Total records written : 11
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1610180408_0019


2024-05-01T01:01:08,982  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,982  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:08,983  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T01:01:08,983  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T01:01:08,990  INFO [main] input.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:08,990  INFO [main] util.MapRedUtil: Total input paths to process : 1
2024-05-01T01:01:08,991  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junit_unparted	
2024-05-01T01:01:08,995  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testWriteChar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.139">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:09,038  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:09,039  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:09,039  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:09,039  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:09,039  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:09,039  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:09,040  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:09,040  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:09,041  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@799c1c8 will be shutdown
2024-05-01T01:01:09,041  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35069061 created in the thread with id: 1
2024-05-01T01:01:09,043  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 3d0227ed-f121-46c3-b379-e2f32593ccdc
2024-05-01T01:01:09,043  INFO [main] SessionState: Hive Session ID = 3d0227ed-f121-46c3-b379-e2f32593ccdc
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:09,043  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:09,049  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3d0227ed-f121-46c3-b379-e2f32593ccdc
2024-05-01T01:01:09,052  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/3d0227ed-f121-46c3-b379-e2f32593ccdc
2024-05-01T01:01:09,054  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/3d0227ed-f121-46c3-b379-e2f32593ccdc/_tmp_space.db
2024-05-01T01:01:09,055  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): drop table if exists junitTypeTest1
2024-05-01T01:01:09,055  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:09,061  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:09,061  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:09,061  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:09,061  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:09,061  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=6, flushCache_()=0}
2024-05-01T01:01:09,061  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.006 seconds
2024-05-01T01:01:09,061  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:09,061  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:09,061  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): drop table if exists junitTypeTest1
2024-05-01T01:01:09,062  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:09,062  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:09,067  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:09,068  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:09,074  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:09,075  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:09,099  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:09,099  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=31, getTable_(GetTableRequest)=6}
2024-05-01T01:01:09,099  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.038 seconds
2024-05-01T01:01:09,100  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): use default
2024-05-01T01:01:09,100  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:09,101  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:09,101  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:09,101  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:09,101  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T01:01:09,102  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.001 seconds
2024-05-01T01:01:09,102  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:09,102  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:09,102  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): use default
2024-05-01T01:01:09,102  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:09,102  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:09,103  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:09,104  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:09,104  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=2}
2024-05-01T01:01:09,104  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.002 seconds
2024-05-01T01:01:09,104  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:09,106  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21
2024-05-01T01:01:09,106  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:09,106  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3d0227ed-f121-46c3-b379-e2f32593ccdc, clientType=HIVECLI]
2024-05-01T01:01:09,106  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:09,106  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:09,107  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@5bbfd031, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@35069061 will be shutdown
2024-05-01T01:01:09,107  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:09,107  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -16
2024-05-01T01:01:09,107  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:09,108  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:09,108  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:09,109  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1, with PersistenceManager: null will be shutdown
2024-05-01T01:01:09,109  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@774d286 created in the thread with id: 1
2024-05-01T01:01:09,111  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1 from thread id: 1
2024-05-01T01:01:09,111  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:09,111  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:09,112  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T01:01:09,112  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:09,112  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:09,113  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@774d286 will be shutdown
2024-05-01T01:01:09,113  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@270741c2 created in the thread with id: 1
2024-05-01T01:01:09,115  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:09,115  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:09,116  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:09,116  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21
2024-05-01T01:01:09,116  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:09,116  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:09,117  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:09,117  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:09,117  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.013 seconds
2024-05-01T01:01:09,117  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:09,117  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:09,117  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21): create table junitTypeTest1(f1 char(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:09,117  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:09,117  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:09,117  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:09,118  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@2090d3d1, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@270741c2 will be shutdown
2024-05-01T01:01:09,118  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:09,118  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -17
2024-05-01T01:01:09,133 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:09,134 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:159) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteChar(AbstractHCatStorerTest.java:87) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:09,138  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T01:01:09,138  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T01:01:09,138  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:09,138 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:09,138  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:09,138  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:09,138  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010109_45286221-af69-45c2-95b8-62dcb2565c21); Time taken: 0.021 seconds
]]></system-err>
  </testcase>
  <testcase name="testWriteDate" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="5.548"/>
  <testcase name="testDynamicPartitioningMultiPartColsNoDataInDataNoSpec" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.771"/>
  <testcase name="testWriteVarchar" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.095">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115)
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109)
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88)
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:15,496  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:15,497  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:15,497  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:15,497  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:15,498  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:15,498  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:15,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@55ddc5a5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5dadf7d9 will be shutdown
2024-05-01T01:01:15,498  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@55ddc5a5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@22e9122a created in the thread with id: 1
2024-05-01T01:01:15,500  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = d91a856e-6eac-4642-b7ad-9f3a982e97df
2024-05-01T01:01:15,500  INFO [main] SessionState: Hive Session ID = d91a856e-6eac-4642-b7ad-9f3a982e97df
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:15,500  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:15,506  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/d91a856e-6eac-4642-b7ad-9f3a982e97df
2024-05-01T01:01:15,509  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/d91a856e-6eac-4642-b7ad-9f3a982e97df
2024-05-01T01:01:15,511  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/d91a856e-6eac-4642-b7ad-9f3a982e97df/_tmp_space.db
2024-05-01T01:01:15,512  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): drop table if exists junitTypeTest1
2024-05-01T01:01:15,512  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:15,517  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:15,517  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:15,517  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getTable_(GetTableRequest)=5, flushCache_()=0}
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.005 seconds
2024-05-01T01:01:15,517  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): drop table if exists junitTypeTest1
2024-05-01T01:01:15,517  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:15,517  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:15,521  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:15,522  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:15,525  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:15,525  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:15,539  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:15,539  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, dropTable_(String, String, boolean, boolean, boolean)=17, getTable_(GetTableRequest)=5}
2024-05-01T01:01:15,539  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.022 seconds
2024-05-01T01:01:15,540  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): use default
2024-05-01T01:01:15,540  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:15,541  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:15,541  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.001 seconds
2024-05-01T01:01:15,541  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): use default
2024-05-01T01:01:15,541  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:15,541  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:15,542  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:15,542  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:15,542  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-05-01T01:01:15,542  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.001 seconds
2024-05-01T01:01:15,543  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:15,543  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427
2024-05-01T01:01:15,543  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:15,543  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d91a856e-6eac-4642-b7ad-9f3a982e97df, clientType=HIVECLI]
2024-05-01T01:01:15,543  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:15,544  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:15,544  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@55ddc5a5, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@22e9122a will be shutdown
2024-05-01T01:01:15,544  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:15,544  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -24
2024-05-01T01:01:15,544  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:15,545  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:15,545  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:15,545  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6, with PersistenceManager: null will be shutdown
2024-05-01T01:01:15,545  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72f855b9 created in the thread with id: 1
2024-05-01T01:01:15,547  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6 from thread id: 1
2024-05-01T01:01:15,547  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:15,547  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:15,547  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T01:01:15,547  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:15,547  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:15,548  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@72f855b9 will be shutdown
2024-05-01T01:01:15,548  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@63689254 created in the thread with id: 1
2024-05-01T01:01:15,550  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:15,550  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:15,550  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:15,551  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427
2024-05-01T01:01:15,551  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:15,551  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:15,551  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:15,551  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:15,551  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.008 seconds
2024-05-01T01:01:15,551  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:15,551  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:15,551  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427): create table junitTypeTest1(f1 varchar(5)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:15,552  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:15,552  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:15,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:15,552  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3c2e5cb6, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@63689254 will be shutdown
2024-05-01T01:01:15,552  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:15,552  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -25
2024-05-01T01:01:15,553 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:15,554 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getIntFromSchema(AvroSerdeUtils.java:326) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:170) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(SchemaToTypeInfo.java:205) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.access$000(SchemaToTypeInfo.java:47) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:120) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo$1.makeInstance(SchemaToTypeInfo.java:115) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(InstanceCache.java:63) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(SchemaToTypeInfo.java:187) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:109) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(SchemaToTypeInfo.java:88) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.<init>(AvroObjectInspectorGenerator.java:53) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:139) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteVarchar(AbstractHCatStorerTest.java:97) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:15,554  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T01:01:15,555  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T01:01:15,555  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:15,555 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:15,555  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:15,555  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:15,555  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010115_ed66de4a-bf8f-4f12-bfda-c9c7997bf427); Time taken: 0.004 seconds
]]></system-err>
  </testcase>
  <testcase name="testBagNStruct" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.98"/>
  <testcase name="testStaticPartitioningMultiPartCols" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.033"/>
  <testcase name="testPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.269"/>
  <testcase name="testStoreWithNoCtorArgs" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.077"/>
  <testcase name="testWriteDecimalX" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.073">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:19,953  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:19,955  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:19,956  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:19,956  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4aa0ee7c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@422c8d72 will be shutdown
2024-05-01T01:01:19,956  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4aa0ee7c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@37310e63 created in the thread with id: 1
2024-05-01T01:01:19,957  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 92ac04ac-65cc-4261-b04f-583ed9bad68b
2024-05-01T01:01:19,958  INFO [main] SessionState: Hive Session ID = 92ac04ac-65cc-4261-b04f-583ed9bad68b
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:19,958  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:19,964  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/92ac04ac-65cc-4261-b04f-583ed9bad68b
2024-05-01T01:01:19,967  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/92ac04ac-65cc-4261-b04f-583ed9bad68b
2024-05-01T01:01:19,969  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/92ac04ac-65cc-4261-b04f-583ed9bad68b/_tmp_space.db
2024-05-01T01:01:19,970  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): drop table if exists junitTypeTest1
2024-05-01T01:01:19,971  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:19,971  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:19,971  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:19,971  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:19,971  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T01:01:19,972  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.001 seconds
2024-05-01T01:01:19,972  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:19,972  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:19,972  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): drop table if exists junitTypeTest1
2024-05-01T01:01:19,972  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:19,972  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:19,973  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:19,973  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T01:01:19,973  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.001 seconds
2024-05-01T01:01:19,973  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): use default
2024-05-01T01:01:19,974  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:19,974  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:19,974  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.001 seconds
2024-05-01T01:01:19,974  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): use default
2024-05-01T01:01:19,974  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:19,975  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:19,975  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:19,976  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:19,976  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=1, getDatabase_(String)=0}
2024-05-01T01:01:19,976  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.002 seconds
2024-05-01T01:01:19,976  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:19,977  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30
2024-05-01T01:01:19,977  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:19,978  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=92ac04ac-65cc-4261-b04f-583ed9bad68b, clientType=HIVECLI]
2024-05-01T01:01:19,978  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:19,978  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:19,978  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@4aa0ee7c, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@37310e63 will be shutdown
2024-05-01T01:01:19,978  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:19,978  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -36
2024-05-01T01:01:19,979  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:19,979  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:19,979  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:19,980  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de, with PersistenceManager: null will be shutdown
2024-05-01T01:01:19,980  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@36fb9a60 created in the thread with id: 1
2024-05-01T01:01:19,981  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de from thread id: 1
2024-05-01T01:01:19,981  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:19,982  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:19,982  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T01:01:19,983  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:19,983  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:19,983  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@36fb9a60 will be shutdown
2024-05-01T01:01:19,984  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@74626534 created in the thread with id: 1
2024-05-01T01:01:19,985  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:19,985  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:19,985  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:19,986  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:19,986  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:19,986  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.01 seconds
2024-05-01T01:01:19,986  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30): create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:19,986  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:19,986  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:19,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:19,987  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1a7224de, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@74626534 will be shutdown
2024-05-01T01:01:19,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:19,987  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -37
2024-05-01T01:01:19,987 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:19,988 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimalX(AbstractHCatStorerTest.java:120) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:19,988  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T01:01:19,988  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T01:01:19,988  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:19,988 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:19,988  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:19,988  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:19,988  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010119_94f72f07-c78e-4702-9a1a-4b5f50b42c30); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testPartitionPublish" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.728"/>
  <testcase name="testWriteDecimal" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.067">
    <error message="FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I" type="(responseCode = 40000, errorMessage = FAILED"><![CDATA[(responseCode = 40000, errorMessage = FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I, SQLState = 08S01, exception = org.codehaus.jackson.JsonNode.asInt()I)
	at org.apache.hadoop.hive.ql.DriverUtils.createProcessorException(DriverUtils.java:156)
	at org.apache.hadoop.hive.ql.Executor.handleTaskFailure(Executor.java:403)
	at org.apache.hadoop.hive.ql.Executor.handleFinished(Executor.java:285)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134)
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107)
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92)
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149)
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175)
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160)
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69)
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56)
	at org.apache.avro.Schema.parse(Schema.java:1327)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75)
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933)
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59)
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	... 43 more
]]></error>
    <system-err><![CDATA[2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:20,753  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:20,754  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:20,754  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:20,755  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,755  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,755  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72ef2add, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@647fb4af will be shutdown
2024-05-01T01:01:20,755  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72ef2add, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4e1c2a55 created in the thread with id: 1
2024-05-01T01:01:20,757  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = c831bd4a-4b2b-4f65-81b6-e217c44cb44c
2024-05-01T01:01:20,757  INFO [main] SessionState: Hive Session ID = c831bd4a-4b2b-4f65-81b6-e217c44cb44c
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:20,757  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:20,763  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c831bd4a-4b2b-4f65-81b6-e217c44cb44c
2024-05-01T01:01:20,766  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/c831bd4a-4b2b-4f65-81b6-e217c44cb44c
2024-05-01T01:01:20,768  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/c831bd4a-4b2b-4f65-81b6-e217c44cb44c/_tmp_space.db
2024-05-01T01:01:20,769  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): drop table if exists junitTypeTest1
2024-05-01T01:01:20,769  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:20,770  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,770  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,770  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,770  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T01:01:20,770  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.001 seconds
2024-05-01T01:01:20,770  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,770  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,771  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): drop table if exists junitTypeTest1
2024-05-01T01:01:20,771  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,771  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.junitTypeTest1	
2024-05-01T01:01:20,772  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,772  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T01:01:20,772  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.002 seconds
2024-05-01T01:01:20,772  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): use default
2024-05-01T01:01:20,772  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,773  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,773  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.001 seconds
2024-05-01T01:01:20,773  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): use default
2024-05-01T01:01:20,773  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,773  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,774  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,774  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,774  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-05-01T01:01:20,774  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.001 seconds
2024-05-01T01:01:20,775  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:20,775  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1
2024-05-01T01:01:20,775  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:20,775  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c831bd4a-4b2b-4f65-81b6-e217c44cb44c, clientType=HIVECLI]
2024-05-01T01:01:20,775  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:20,776  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:20,776  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@72ef2add, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@4e1c2a55 will be shutdown
2024-05-01T01:01:20,776  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:20,776  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -42
2024-05-01T01:01:20,776  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,777  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:20,777  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,777  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb, with PersistenceManager: null will be shutdown
2024-05-01T01:01:20,777  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eea5707 created in the thread with id: 1
2024-05-01T01:01:20,778  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb from thread id: 1
2024-05-01T01:01:20,778  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,778  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,778  INFO [main] parse.CalcitePlanner: Creating table default.junitTypeTest1 position=13
2024-05-01T01:01:20,779  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,779  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,779  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@2eea5707 will be shutdown
2024-05-01T01:01:20,779  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@127f9c8 created in the thread with id: 1
2024-05-01T01:01:20,781  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,781  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,781  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,782  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,782  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,782  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.007 seconds
2024-05-01T01:01:20,782  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1): create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:20,782  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,782  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:20,782  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:20,782  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@1c8960cb, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@127f9c8 will be shutdown
2024-05-01T01:01:20,782  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:20,782  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -43
2024-05-01T01:01:20,782 ERROR [main] exec.Task: Failed
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:20,783 ERROR [main] exec.Task: DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation
java.lang.NoSuchMethodError: org.codehaus.jackson.JsonNode.asInt()I
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:107) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.util.internal.JacksonUtils.toObject(JacksonUtils.java:92) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.JsonProperties.getObjectProp(JsonProperties.java:149) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.hasProperty(LogicalTypes.java:248) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:175) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes$Decimal.<init>(LogicalTypes.java:160) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaImpl(LogicalTypes.java:69) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.LogicalTypes.fromSchemaIgnoreInvalid(LogicalTypes.java:56) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema.parse(Schema.java:1327) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1032) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.avro.Schema$Parser.parse(Schema.java:1020) ~[avro-1.8.2.jar:1.8.2]
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:283) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfoToSchema.java:148) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfoToSchema.java:82) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(TypeInfoToSchema.java:75) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(TypeInfoToSchema.java:60) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSchemaFromCols(AvroSerDe.java:190) ~[classes/:?]
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:123) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:95) ~[classes/:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:78) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:349) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:331) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:741) ~[classes/:?]
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:724) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableDesc.toTable(CreateTableDesc.java:933) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.execute(CreateTableOperation.java:59) ~[classes/:?]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) [classes/:?]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245) [classes/:?]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153) [classes/:?]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164) [classes/:?]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:230) [classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.executeStatementOnDriver(AbstractHCatLoaderTest.java:151) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTable(AbstractHCatLoaderTest.java:134) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.createTableDefaultDB(AbstractHCatLoaderTest.java:114) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:267) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.pigValueRangeTest(AbstractHCatStorerTest.java:243) [test-classes/:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testWriteDecimal(AbstractHCatStorerTest.java:131) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:20,783  INFO [main] reexec.ReOptimizePlugin: ReOptimization: retryPossible: false
2024-05-01T01:01:20,784  INFO [main] reexec.ReExecuteLostAMQueryPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
2024-05-01T01:01:20,784  INFO [main] reexec.ReExecutionDagSubmitPlugin: Got exception message: org.codehaus.jackson.JsonNode.asInt()I retryPossible: false
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:20,784 ERROR [main] ql.Driver: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. org.codehaus.jackson.JsonNode.asInt()I
2024-05-01T01:01:20,784  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,784  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:20,784  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_627c438b-c051-43cf-814b-02f28e8a98e1); Time taken: 0.002 seconds
]]></system-err>
  </testcase>
  <testcase name="testMultiPartColsInData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.732">
    <failure message="expected:&lt;1&gt; but was:&lt;5&gt;" type="java.lang.AssertionError"><![CDATA[java.lang.AssertionError: expected:<1> but was:<5>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:491)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
]]></failure>
    <system-out><![CDATA[log4j: Could not find root logger information. Is this OK?
log4j: Finished configuring.
]]></system-out>
    <system-err><![CDATA[2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:20,820  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:20,821  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,822  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:20,822  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,822  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3, with PersistenceManager: null will be shutdown
2024-05-01T01:01:20,822  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a3ccd91 created in the thread with id: 1
2024-05-01T01:01:20,823  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3 from thread id: 1
2024-05-01T01:01:20,823  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
Hive Session ID = 569b95aa-b83a-4aa5-9399-2fa66efab1dd
2024-05-01T01:01:20,823  INFO [main] SessionState: Hive Session ID = 569b95aa-b83a-4aa5-9399-2fa66efab1dd
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:20,824  INFO [main] DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/home/alex/.m2/repository/org/apache/pig/pig/0.16.0/pig-0.16.0-h2.jar!/ivysettings.xml will be used
2024-05-01T01:01:20,829  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/569b95aa-b83a-4aa5-9399-2fa66efab1dd
2024-05-01T01:01:20,832  INFO [main] session.SessionState: Created local directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/569b95aa-b83a-4aa5-9399-2fa66efab1dd
2024-05-01T01:01:20,835  INFO [main] session.SessionState: Created HDFS directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/scratchdir/alex/569b95aa-b83a-4aa5-9399-2fa66efab1dd/_tmp_space.db
2024-05-01T01:01:20,835  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): drop table if exists employee
2024-05-01T01:01:20,836  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,836  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,836  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6a3ccd91 will be shutdown
2024-05-01T01:01:20,837  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46c0e65b created in the thread with id: 1
2024-05-01T01:01:20,838  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,838  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,838  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:20,839  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,839  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,839  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,839  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, flushCache_()=0}
2024-05-01T01:01:20,839  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.004 seconds
2024-05-01T01:01:20,839  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,839  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,839  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): drop table if exists employee
2024-05-01T01:01:20,840  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,840  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:20,840  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,840  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0}
2024-05-01T01:01:20,841  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.001 seconds
2024-05-01T01:01:20,841  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): use default
2024-05-01T01:01:20,841  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,842  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,842  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1, flushCache_()=0}
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.001 seconds
2024-05-01T01:01:20,842  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): use default
2024-05-01T01:01:20,842  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,842  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,843  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,843  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,843  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, getDatabase_(String)=1}
2024-05-01T01:01:20,843  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.001 seconds
2024-05-01T01:01:20,843  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:20,844  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad
2024-05-01T01:01:20,844  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:20,845  INFO [main] sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=569b95aa-b83a-4aa5-9399-2fa66efab1dd, clientType=HIVECLI]
2024-05-01T01:01:20,845  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2024-05-01T01:01:20,845  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:20,845  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@35729bc3, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@46c0e65b will be shutdown
2024-05-01T01:01:20,845  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:20,845  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -44
2024-05-01T01:01:20,846  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,846  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:20,846  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,846  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e, with PersistenceManager: null will be shutdown
2024-05-01T01:01:20,846  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@466a36d7 created in the thread with id: 1
2024-05-01T01:01:20,848  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e from thread id: 1
2024-05-01T01:01:20,848  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,848  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,848  INFO [main] parse.CalcitePlanner: Creating table default.employee position=13
2024-05-01T01:01:20,849  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,849  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,849  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@466a36d7 will be shutdown
2024-05-01T01:01:20,849  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f59b2ef created in the thread with id: 1
2024-05-01T01:01:20,851  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,851  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,851  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_database: default	
2024-05-01T01:01:20,851  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad
2024-05-01T01:01:20,851  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:20,851  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:null, properties:null)
2024-05-01T01:01:20,852  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:20,852  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:20,852  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.008 seconds
2024-05-01T01:01:20,852  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:20,852  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:20,852  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): create table employee(emp_id INT, emp_name STRING, emp_start_date STRING , emp_gender STRING) partitioned by (emp_country STRING , emp_state STRING) stored as AVRO TBLPROPERTIES ('transactional'='false')
2024-05-01T01:01:20,852  INFO [main] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2024-05-01T01:01:20,852  INFO [main] metastore.HiveMetaStoreClient: Mestastore configuration metastore.filter.hook changed from org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook to org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
2024-05-01T01:01:20,852  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:20,852  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@94ae80e, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@1f59b2ef will be shutdown
2024-05-01T01:01:20,852  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:20,852  INFO [main] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -45
2024-05-01T01:01:20,853  INFO [main] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:20,854  INFO [main] metastore.HMSHandler: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:20,854  INFO [main] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:20,854  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7668bf01, with PersistenceManager: null will be shutdown
2024-05-01T01:01:20,854  INFO [main] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7668bf01, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6b4163b5 created in the thread with id: 1
2024-05-01T01:01:20,855  INFO [main] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7668bf01 from thread id: 1
2024-05-01T01:01:20,855  INFO [main] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:20,855  INFO [main] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:20,855  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=create_table_req: Table(tableName:employee, dbName:default, owner:alex, createTime:1714550480, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:emp_id, type:int, comment:null), FieldSchema(name:emp_name, type:string, comment:null), FieldSchema(name:emp_start_date, type:string, comment:null), FieldSchema(name:emp_gender, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.avro.AvroSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:emp_country, type:string, comment:null), FieldSchema(name:emp_state, type:string, comment:null)], parameters:{bucketing_version=2, transactional=false}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{alex=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:alex, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null), temporary:false, catName:hive, ownerType:USER)	
2024-05-01T01:01:20,857  INFO [main] utils.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/employee
2024-05-01T01:01:20,869  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:20,870  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {createTable_(Table)=14}
2024-05-01T01:01:20,870  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.017 seconds
2024-05-01T01:01:20,891  INFO [main] executionengine.HExecutionEngine: Connecting to hadoop file system at: file:///
2024-05-01T01:01:20,892  INFO [main] pig.PigServer: Pig Script ID for the session: PIG-default-019b99ca-06f0-4ae6-a7b9-d21f90c9d774
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:20,984  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:20,984  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:20,985  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:20,986  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:20,987  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:20,990  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,010  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,011  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,011  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,025  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,026  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,026  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:21,026  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:21,027  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,028  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:21,032  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,052  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,053  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,053  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,067  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,068  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,068  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:21,068  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:21,069  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,070  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:21,076  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,096  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,096  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,096  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,111  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,111  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf not set. Generating configuration differences.
2024-05-01T01:01:21,112  INFO [main] common.HCatUtil: Configuration differences={datanucleus.connectionPool.maxPoolSize=4, hive.llap.io.cache.orc.alloc.max=2097152, javax.jdo.option.ConnectionUserName=APP, test.property1=value1, hive.stats.column.autogather=true, hive.query.results.cache.enabled=false, fs.pfile.impl=org.apache.hadoop.fs.ProxyLocalFileSystem, hive.querylog.location=${test.tmp.dir}/tmp, hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore, hive.llap.io.use.lrfu=true, javax.jdo.option.ConnectionPassword=mine, hive.jar.path=${maven.local.repository}/org/apache/hive/hive-exec/${hive.version}/hive-exec-${hive.version}.jar, hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, datanucleus.schema.autoCreateAll=true, test.data.scripts=${hive.root}/data/scripts, hive.users.in.admin.role=hive_admin_user, hive.llap.io.cache.orc.size=8388608, test.var.hiveconf.property=${hive.exec.default.partition.name}, hive.dummyparam.test.server.specific.config.hivesite=from.hive-site.xml, hive.exec.mode.local.auto=false, hive.materializedview.rewriting=true, hive.dummyparam.test.server.specific.config.override=from.hive-site.xml, hive.fetch.task.conversion=minimal, hive.metastore.metadb.dir=file://${test.tmp.dir}/metadb/, hive.conf.restricted.list=dummy.config.value, hive.server2.operation.log.purgePolicy.timeToLive=5s, hive.query.reexecution.stats.persist.scope=query, hive.cbo.fallback.strategy=TEST, hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat, hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest, test.data.files=${hive.root}/data/files, hive.metastore.client.cache.enabled=true, hive.stats.key.prefix.reserve.length=0, hive.exec.local.scratchdir=${test.tmp.dir}/localscratchdir/, hive.exec.submit.local.task.via.child=false, hive.auto.convert.join=false, hive.llap.io.cache.orc.alloc.min=32768, hive.stats.fetch.bitvector=true, hive.llap.io.allocator.direct=false, hive.metastore.schema.verification=false, javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver, hive.support.concurrency=true, hive.in.test=true, hive.metastore.warehouse.dir=${test.warehouse.dir}, hive.llap.io.cache.orc.arena.size=8388608, hive.exec.scratchdir=${test.tmp.dir}/scratchdir, test.log.dir=${test.tmp.dir}/log/, hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, javax.jdo.option.ConnectionURL=jdbc:derby:memory:${test.tmp.dir}/junit_metastore_db;create=true, hive.llap.cache.allow.synthetic.fileid=true, hive.scheduled.queries.executor.enabled=false, hive.ignore.mapjoin.hint=false, hive.metastore.client.cache.maxSize=10Mb, hive.metastore.client.cache.recordStats=true, hive.mapjoin.max.gc.time.percentage=0.99, hive.test.dummystats.aggregator=value2, hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.PreExecutePrinter, org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables, org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook, iceberg.hive.keep.stats=true, hive.strict.timestamp.conversion=false}
2024-05-01T01:01:21,113  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,114  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:21,117  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,137  WARN [main] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,138  INFO [main] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,138  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,140  INFO [main] pigstats.ScriptState: Pig features used in the script: FILTER
2024-05-01T01:01:21,148  WARN [main] data.SchemaTupleBackend: SchemaTupleBackend has already been initialized
2024-05-01T01:01:21,148  INFO [main] optimizer.LogicalPlanOptimizer: {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}
2024-05-01T01:01:21,151  INFO [main] mapReduceLayer.MRCompiler: File concatenation threshold: 100 optimistic? false
2024-05-01T01:01:21,152  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size before optimization: 5
2024-05-01T01:01:21,153  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 map-only splittees.
2024-05-01T01:01:21,153  INFO [main] mapReduceLayer.MultiQueryOptimizer: Merged 4 out of total 5 MR operators.
2024-05-01T01:01:21,153  INFO [main] mapReduceLayer.MultiQueryOptimizer: MR plan size after optimization: 1
2024-05-01T01:01:21,159  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:21,159  INFO [main] mapreduce.MRScriptState: Pig script settings are added to the job
2024-05-01T01:01:21,160  INFO [main] mapReduceLayer.JobControlCompiler: mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2024-05-01T01:01:21,160  INFO [main] mapReduceLayer.JobControlCompiler: Setting up multi store job
2024-05-01T01:01:21,161  INFO [main] data.SchemaTupleFrontend: Key [pig.schematuple] is false, will not generate code.
2024-05-01T01:01:21,161  INFO [main] data.SchemaTupleFrontend: Starting process to move generated code to distributed cacche
2024-05-01T01:01:21,161  INFO [main] data.SchemaTupleFrontend: Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/1714550481160-0
2024-05-01T01:01:21,169  INFO [main] mapReduceLayer.MapReduceLauncher: 1 map-reduce job(s) waiting for submission.
2024-05-01T01:01:21,170  WARN [JobControl] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,185  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,186  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,186  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,186  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,186  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,187  INFO [JobControl] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:21,187  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:21,187  INFO [JobControl] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:21,187  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7e6bc5af, with PersistenceManager: null will be shutdown
2024-05-01T01:01:21,187  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7e6bc5af, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@14210372 created in the thread with id: 1969
2024-05-01T01:01:21,189  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7e6bc5af from thread id: 1969
2024-05-01T01:01:21,189  INFO [JobControl] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:21,189  INFO [JobControl] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:21,189  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:21,190  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7e6bc5af, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@14210372 will be shutdown
2024-05-01T01:01:21,190  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:21,190  INFO [JobControl] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -46
2024-05-01T01:01:21,190  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,190  INFO [JobControl] metastore.HMSHandler: 52: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:21,191  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@65cf7913, with PersistenceManager: null will be shutdown
2024-05-01T01:01:21,191  INFO [JobControl] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@65cf7913, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@570136da created in the thread with id: 1969
2024-05-01T01:01:21,192  INFO [JobControl] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@65cf7913 from thread id: 1969
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,208  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,208  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,209  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,225  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,226  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,226  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,246  WARN [JobControl] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,246  INFO [JobControl] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,247  INFO [JobControl] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,261  WARN [JobControl] mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2024-05-01T01:01:21,265  WARN [JobControl] mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2024-05-01T01:01:21,267  INFO [JobControl] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T01:01:21,267  INFO [JobControl] input.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:21,267  INFO [JobControl] util.MapRedUtil: Total input paths to process : 1
2024-05-01T01:01:21,268  INFO [JobControl] util.MapRedUtil: Total input paths (combined) to process : 1
2024-05-01T01:01:21,287  INFO [JobControl] mapreduce.JobSubmitter: number of splits:1
2024-05-01T01:01:21,301  INFO [JobControl] mapreduce.JobSubmitter: Submitting tokens for job: job_local1589957077_0037
2024-05-01T01:01:21,301  INFO [JobControl] mapreduce.JobSubmitter: Executing with tokens: []
2024-05-01T01:01:21,360  INFO [JobControl] mapreduce.Job: The url to track the job: http://localhost:8080/
2024-05-01T01:01:21,360  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter set in config null
2024-05-01T01:01:21,365  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,365  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,367  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,368  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,370  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,370  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,373  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,373  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,374  INFO [Thread-1843] mapred.LocalJobRunner: OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2024-05-01T01:01:21,375  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,375  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,390  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,390  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,403  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,403  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,416  INFO [Thread-1843] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,416  INFO [Thread-1843] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,430  INFO [Thread-1843] mapred.LocalJobRunner: Waiting for map tasks
2024-05-01T01:01:21,430  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Starting task: attempt_local1589957077_0037_m_000000_0
2024-05-01T01:01:21,439  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,439  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,443  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,443  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,448  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,448  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,452  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,452  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,455  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,455  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,457  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,457  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,458  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,458  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,460  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,460  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,460  INFO [LocalJobRunner Map Task Executor #0] mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2024-05-01T01:01:21,461  INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Number of splits :1
Total Length = 132
Input split[0]:
   Length = 132
   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit
   Locations:

-----------------------

2024-05-01T01:01:21,463  INFO [LocalJobRunner Map Task Executor #0] builtin.PigStorage: Using PigTextInputFormat
2024-05-01T01:01:21,463  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigRecordReader: Current split being processed file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/input.data:0+132
2024-05-01T01:01:21,464  INFO [LocalJobRunner Map Task Executor #0] util.SpillableMemoryManager: Selected heap (PS Old Gen) of size 1431830528 to monitor. collectionUsageThreshold = 1064828928, usageThreshold = 1064828928
2024-05-01T01:01:21,465  INFO [LocalJobRunner Map Task Executor #0] data.SchemaTupleBackend: Key [pig.schematuple] was not set... will not generate code.
2024-05-01T01:01:21,468  INFO [LocalJobRunner Map Task Executor #0] mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-05-01T01:01:21,473  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,473  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,482  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,482  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,488  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,488  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,527  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:21,527  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -47
2024-05-01T01:01:21,527  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:21,527  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -48
2024-05-01T01:01:21,527  INFO [Finalizer] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:21,527  INFO [Finalizer] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -49
2024-05-01T01:01:21,529  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,529  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,575  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:21,576  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task:attempt_local1589957077_0037_m_000000_0 is done. And is in the process of committing
2024-05-01T01:01:21,577  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,577  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,580  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: 
2024-05-01T01:01:21,580  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task attempt_local1589957077_0037_m_000000_0 is allowed to commit now
2024-05-01T01:01:21,581  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,581  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,582  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1589957077_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/employee/_SCRATCH0,29634100171350974/emp_country=IN/emp_state=TN
2024-05-01T01:01:21,583  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,583  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,584  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1589957077_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/employee/_SCRATCH0,5718995608246988/emp_country=IN/emp_state=KA
2024-05-01T01:01:21,585  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,585  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,586  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1589957077_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/employee/_SCRATCH0,07095155463618463/emp_country=IN/emp_state=KL
2024-05-01T01:01:21,587  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: File Output Committer Algorithm version is 2
2024-05-01T01:01:21,587  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-05-01T01:01:21,588  INFO [LocalJobRunner Map Task Executor #0] output.FileOutputCommitter: Saved output of task 'attempt_local1589957077_0037_m_000000_0' to file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/warehouse/employee/_SCRATCH0,2912842208162738/emp_country=IN/emp_state=AP
2024-05-01T01:01:21,588  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: map
2024-05-01T01:01:21,588  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Task 'attempt_local1589957077_0037_m_000000_0' done.
2024-05-01T01:01:21,588  INFO [LocalJobRunner Map Task Executor #0] mapred.Task: Final Counters for attempt_local1589957077_0037_m_000000_0: Counters: 19
	File System Counters
		FILE: Number of bytes read=107859
		FILE: Number of bytes written=21151980
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=0
		Input split bytes=480
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=35
		Total committed heap usage (bytes)=964689920
	MultiStoreCounters
		Output records in _0_employee=1
		Output records in _1_employee=1
		Output records in _2_employee=1
		Output records in _3_employee=1
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
2024-05-01T01:01:21,589  INFO [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner: Finishing task: attempt_local1589957077_0037_m_000000_0
2024-05-01T01:01:21,589  INFO [Thread-1843] mapred.LocalJobRunner: map task executor complete.
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,632  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,632  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,633  INFO [Thread-1843] metastore.HiveMetaStoreClient: HMS client filtering is enabled.
2024-05-01T01:01:21,633  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:21,633  INFO [Thread-1843] metastore.PersistenceManagerProvider: Configuration datanucleus.autoStartMechanismMode is not set. Defaulting to 'ignored'
2024-05-01T01:01:21,634  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64b139f4, with PersistenceManager: null will be shutdown
2024-05-01T01:01:21,634  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64b139f4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3f8228da created in the thread with id: 1989
2024-05-01T01:01:21,635  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64b139f4 from thread id: 1989
2024-05-01T01:01:21,635  INFO [Thread-1843] metastore.HMSHandler: HMS server filtering is disabled by configuration
2024-05-01T01:01:21,635  INFO [Thread-1843] metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient ugi=alex (auth:SIMPLE) retries=1 delay=1 lifetime=0
2024-05-01T01:01:21,636  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
2024-05-01T01:01:21,636  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@64b139f4, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3f8228da will be shutdown
2024-05-01T01:01:21,636  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2024-05-01T01:01:21,636  INFO [Thread-1843] metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: -50
2024-05-01T01:01:21,636  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,637  INFO [Thread-1843] metastore.HMSHandler: 53: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
2024-05-01T01:01:21,637  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eef1df8, with PersistenceManager: null will be shutdown
2024-05-01T01:01:21,637  INFO [Thread-1843] metastore.ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eef1df8, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@5df9a8e4 created in the thread with id: 1989
2024-05-01T01:01:21,638  INFO [Thread-1843] metastore.HMSHandler: Created RawStore: org.apache.hadoop.hive.metastore.ObjectStore@3eef1df8 from thread id: 1989
2024-05-01T01:01:21,670  INFO [main] mapReduceLayer.MapReduceLauncher: HadoopJobId: job_local1589957077_0037
2024-05-01T01:01:21,670  INFO [main] mapReduceLayer.MapReduceLauncher: Processing aliases A,AP,KA,KL,TN
2024-05-01T01:01:21,670  INFO [main] mapReduceLayer.MapReduceLauncher: detailed locations: M: A[1,4],A[-1,-1],TN[1,306],KA[1,341],KL[1,376],AP[1,411] C:  R: 
2024-05-01T01:01:21,671  INFO [main] mapReduceLayer.MapReduceLauncher: 50% complete
2024-05-01T01:01:21,671  INFO [main] mapReduceLayer.MapReduceLauncher: Running jobs are [job_local1589957077_0037]
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,691  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,692  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=TN}].
2024-05-01T01:01:21,693  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,TN]	
2024-05-01T01:01:21,698  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T01:01:21,698  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T01:01:21,702  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T01:01:21,702  INFO [HMSHandler #11] utils.MetaStoreServerUtils: Updated size to 458
2024-05-01T01:01:21,735  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,779  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,780  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,780  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,780  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,780  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,780  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,781  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,835  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,836  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,836  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,836  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,836  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,836  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,836  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,836  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,846  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KA}].
2024-05-01T01:01:21,847  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KA]	
2024-05-01T01:01:21,849  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T01:01:21,849  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T01:01:21,854  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T01:01:21,854  INFO [HMSHandler #12] utils.MetaStoreServerUtils: Updated size to 458
2024-05-01T01:01:21,877  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T01:01:21,917  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,917  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,917  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,918  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,918  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,919  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,919  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:21,967  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:21,968  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:21,968  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:21,977  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=KL}].
2024-05-01T01:01:21,978  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,KL]	
2024-05-01T01:01:21,980  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T01:01:21,980  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T01:01:21,984  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T01:01:21,984  INFO [HMSHandler #13] utils.MetaStoreServerUtils: Updated size to 456
2024-05-01T01:01:22,007  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:22,045  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:22,046  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:22,046  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:22,046  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:22,046  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:22,046  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:22,046  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:22,047  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:22,090  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:22,091  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:22,091  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:22,100  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: HAR not is not being used. The table employee has new partitions [{emp_country=IN, emp_state=AP}].
2024-05-01T01:01:22,101  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partition : tbl=hive.default.employee[IN,AP]	
2024-05-01T01:01:22,103  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partition	
2024-05-01T01:01:22,103  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=add_partitions	
2024-05-01T01:01:22,106  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updating partition stats fast for: employee
2024-05-01T01:01:22,107  INFO [HMSHandler #14] utils.MetaStoreServerUtils: Updated size to 456
2024-05-01T01:01:22,130  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: Cancelling delegation token for the job.
2024-05-01T01:01:22,166  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.size does not exist
2024-05-01T01:01:22,166  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.override does not exist
2024-05-01T01:01:22,166  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.metadb.dir does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.min does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.hivesite does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.alloc.max does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.maxSize does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.dummyparam.test.server.specific.config.metastoresite does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.metastore.client.cache.recordStats does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.llap.io.cache.orc.arena.size does not exist
2024-05-01T01:01:22,167  WARN [Thread-1843] conf.HiveConf: HiveConf of name hive.stats.key.prefix.reserve.length does not exist
2024-05-01T01:01:22,167  INFO [Thread-1843] common.HCatUtil: mapreduce.lib.hcatoutput.hive.conf is set. Applying configuration differences.
2024-05-01T01:01:22,167  INFO [Thread-1843] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_databases: @hive#NonExistentDatabaseUsedForHealthCheck	
2024-05-01T01:01:22,168  INFO [Thread-1843] mapreduce.FileOutputCommitterContainer: FileOutputCommitterContainer::cancelDelegationTokens(): Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation.
2024-05-01T01:01:22,363  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:22,364  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:22,364  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:22,380  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T01:01:22,381  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:22,381  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T01:01:22,381  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:22,382  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T01:01:22,382  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:22,383  INFO [main] pigstats.JobStats: using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
2024-05-01T01:01:22,383  WARN [main] pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File employee does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:464) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1919) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1961) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81) ~[pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOneOutputStats(MRJobStats.java:461) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addOutputStatistics(MRJobStats.java:443) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:237) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:308) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1474) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.execute(PigServer.java:1448) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:488) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.PigServer.executeBatch(PigServer.java:471) [pig-0.16.0-h2.jar:?]
	at org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.testMultiPartColsInData(AbstractHCatStorerTest.java:478) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_402]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_402]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_402]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.jar:4.13]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.jar:4.13]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.jar:4.13]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.jar:4.13]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.jar:4.13]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) [surefire-booter-3.0.0-M4.jar:3.0.0-M4]
2024-05-01T01:01:22,383  INFO [main] mapReduceLayer.MapReduceLauncher: 100% complete
2024-05-01T01:01:22,383  INFO [main] mapreduce.SimplePigStats: Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
3.1.0	0.16.0	alex	2024-05-01 01:01:21	2024-05-01 01:01:22	FILTER

Success!

Job Stats (time in seconds):
JobId	Maps	Reduces	MaxMapTime	MinMapTime	AvgMapTime	MedianMapTime	MaxReduceTime	MinReduceTime	AvgReduceTime	MedianReducetime	Alias	Feature	Outputs
job_local1589957077_0037	1	0	n/a	n/a	n/a	n/a	0	0	0	0	A,AP,KA,KL,TN	MULTI_QUERY,MAP_ONLY	employee,employee,employee,employee,

Input(s):
Successfully read 4 records from: "/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/build/test/data/org.apache.hive.hcatalog.mapreduce.HCatBaseTest-1714550442173/input.data"

Output(s):
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"
Successfully stored 1 records in: "employee"

Counters:
Total records written : 4
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local1589957077_0037


2024-05-01T01:01:22,385  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:22,386  WARN [main] impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2024-05-01T01:01:22,387  INFO [main] mapReduceLayer.MapReduceLauncher: Success!
2024-05-01T01:01:22,388  INFO [main] ql.Driver: Compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): select * from employee
2024-05-01T01:01:22,389  INFO [main] parse.CalcitePlanner: Starting caching scope for: alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad
2024-05-01T01:01:22,389  INFO [main] parse.CalcitePlanner: Starting Semantic Analysis
2024-05-01T01:01:22,389  INFO [main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2024-05-01T01:01:22,389  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-05-01T01:01:22,389  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:22,394  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
2024-05-01T01:01:22,394  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-05-01T01:01:22,394  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-05-01T01:01:22,394  INFO [main] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis
2024-05-01T01:01:22,397  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_all_table_constraints : tbl=hive.default.employee	
2024-05-01T01:01:22,403  INFO [main] calcite.RelOptHiveTable: Calculating column statistics for default.employee, projIndxSet: [0, 1, 2, 3, 4, 5], allowMissingStats: true
2024-05-01T01:01:22,403  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_partitions : tbl=hive.default.employee	
2024-05-01T01:01:22,432  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_aggr_stats_for: table=hive.default.employee	
2024-05-01T01:01:22,443  WARN [main] calcite.RelOptHiveTable: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-05-01T01:01:22,443  INFO [main] SessionState: No Stats for default@employee, Columns: emp_start_date, emp_name, emp_gender, emp_id
2024-05-01T01:01:22,473  INFO [main] parse.CalcitePlanner: Get metadata for source tables
2024-05-01T01:01:22,473  INFO [main] parse.CalcitePlanner: Get metadata for subqueries
2024-05-01T01:01:22,473  INFO [main] parse.CalcitePlanner: Get metadata for destination tables
2024-05-01T01:01:22,474  INFO [main] common.FileUtils: Creating directory if it doesn't exist: file:/home/alex/Repositories/hive/hcatalog/hcatalog-pig-adapter/target/tmp/localscratchdir/569b95aa-b83a-4aa5-9399-2fa66efab1dd/hive_2024-05-01_01-01-22_388_4854769462040849405-1/-mr-10001/.hive-staging_hive_2024-05-01_01-01-22_388_4854769462040849405-1
2024-05-01T01:01:22,482  INFO [main] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.
2024-05-01T01:01:22,483  INFO [main] optimizer.BucketVersionPopulator: not considering bucketingVersion for: TS[0] because it has -1<2 buckets 
2024-05-01T01:01:22,484  INFO [main] parse.CalcitePlanner: Completed plan generation
2024-05-01T01:01:22,484  INFO [main] parse.CalcitePlanner: Ending caching scope for: alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad
2024-05-01T01:01:22,484  INFO [main] ql.Driver: Semantic Analysis Completed (retrial = false)
2024-05-01T01:01:22,484  INFO [main] ql.Driver: Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:employee.emp_id, type:int, comment:null), FieldSchema(name:employee.emp_name, type:string, comment:null), FieldSchema(name:employee.emp_start_date, type:string, comment:null), FieldSchema(name:employee.emp_gender, type:string, comment:null), FieldSchema(name:employee.emp_country, type:string, comment:null), FieldSchema(name:employee.emp_state, type:string, comment:null)], properties:null)
2024-05-01T01:01:22,485  INFO [main] exec.TableScanOperator: Initializing Operator: TS[0]
2024-05-01T01:01:22,485  INFO [main] exec.SelectOperator: Initializing Operator: SEL[1]
2024-05-01T01:01:22,485  INFO [main] exec.SelectOperator: SELECT struct<emp_id:int,emp_name:string,emp_start_date:string,emp_gender:string,emp_country:string,emp_state:string>
2024-05-01T01:01:22,485  INFO [main] exec.ListSinkOperator: Initializing Operator: LIST_SINK[3]
2024-05-01T01:01:22,485  INFO [main] metadata.Hive: Dumping metastore api call timing information for : compilation phase
2024-05-01T01:01:22,486  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(Configuration)=0, listPartitions_(String, String, short)=28, getAllTableConstraints_(AllTableConstraintsRequest)=3, getTable_(GetTableRequest)=5, flushCache_()=0, getAggrColStatsFor_(String, String, List, List, String, String)=11}
2024-05-01T01:01:22,486  INFO [main] ql.Driver: Completed compiling command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.097 seconds
2024-05-01T01:01:22,486  INFO [main] reexec.ReExecDriver: Execution #1 of query
2024-05-01T01:01:22,486  INFO [main] ql.Driver: Concurrency mode is disabled, not creating a lock manager
2024-05-01T01:01:22,486  INFO [main] ql.Driver: Executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad): select * from employee
2024-05-01T01:01:22,486  INFO [main] metadata.Hive: Dumping metastore api call timing information for : execution phase
2024-05-01T01:01:22,486  INFO [main] metadata.Hive: Total time spent in each metastore function (ms): {}
2024-05-01T01:01:22,486  INFO [main] ql.Driver: Completed executing command(queryId=alex_20240501010120_0db3ed3e-3db2-4bad-a580-990b6c3924ad); Time taken: 0.0 seconds
2024-05-01T01:01:22,490  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:22,490  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T01:01:22,495  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:22,495  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T01:01:22,496  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T01:01:22,501  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:22,501  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T01:01:22,501  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T01:01:22,507  INFO [main] mapred.FileInputFormat: Total input files to process : 1
2024-05-01T01:01:22,507  INFO [main] avro.AvroSerDe: Updating column name/type properties based on current schema
2024-05-01T01:01:22,507  INFO [main] avro.AvroGenericRecordReader: Found the avro schema in the job
2024-05-01T01:01:22,510  INFO [main] exec.TableScanOperator: Closing Operator: TS[0]
2024-05-01T01:01:22,510  INFO [main] exec.TableScanOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_TS_0:4, 
2024-05-01T01:01:22,510  INFO [main] exec.SelectOperator: Closing Operator: SEL[1]
2024-05-01T01:01:22,510  INFO [main] exec.SelectOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_SEL_1:4, 
2024-05-01T01:01:22,510  INFO [main] exec.ListSinkOperator: Closing Operator: LIST_SINK[3]
2024-05-01T01:01:22,510  INFO [main] exec.ListSinkOperator: RECORDS_OUT_INTERMEDIATE:0, RECORDS_OUT_OPERATOR_LIST_SINK_3:4, 
2024-05-01T01:01:22,510  INFO [main] HiveMetaStore.audit: ugi=alex	ip=unknown-ip-addr	cmd=get_table : tbl=hive.default.employee	
2024-05-01T01:01:22,515  INFO [main] metastore.HMSHandler: Skipping translation for processor with null
]]></system-err>
  </testcase>
  <testcase name="testStaticPartitioningMultiPartColsNoData" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.161"/>
  <testcase name="testStoreMultiTables" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="1.101"/>
  <testcase name="testStoreWithNoSchema" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.741"/>
  <testcase name="testNoAlias" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.235"/>
  <testcase name="testStoreFuncSimple" classname="org.apache.hive.hcatalog.pig.TestAvroHCatStorer" time="0.762"/>
</testsuite>